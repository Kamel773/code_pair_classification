{"func": "def initZ(self, pmean, pvar, qmean, qvar, qE=None, qE2=None, covariates=None, scale_covariates=None):\n  if qmean is not None:\n  if isinstance(qmean,str):\n  if qmean == \"random\":\n  qmean = stats.norm.rvs(loc=0, scale=1, size=(self.N,self.K))\n  elif qmean == \"orthogonal\":\n  pca = sklearn.decomposition.PCA(n_components=self.K, copy=True, whiten=True)\n  pca.fit(stats.norm.rvs(loc=0, scale=1, size=(self.N,9999)).T)\n  qmean = pca.components_.T\n   elif qmean == \"pca\":\n  pca = sklearn.decomposition.PCA(n_components=self.K, copy=True, whiten=True)\n  pca.fit(s.concatenate(self.data,axis=1).T)\n  qmean = pca.components_.T\n   elif isinstance(qmean,s.ndarray):\n  assert qmean.shape == (self.N,self.K)\n  elif isinstance(qmean,(int,float)):\n  qmean = s.ones((self.N,self.K)) * qmean\n  else:\n  print(\"Wrong initialisation for Z\")\n  exit()\n  if covariates is not None:\n  assert scale_covariates != None, \"If you use covariates also define data_opts['scale_covariates']\"\n  idx_covariates = s.array(range(covariates.shape[1]))\n  scale_covariates = s.array(scale_covariates)\n  covariates[:,scale_covariates] = (covariates[:,scale_covariates] - s.nanmean(covariates[:,scale_covariates], axis=0)) / s.nanstd(covariates[:,scale_covariates], axis=0)\n  covariates[s.isnan(covariates)] = 0.\n  qmean[:,idx_covariates] = covariates\n  else:\n  idx_covariates = None\n  self.Z = Z_Node(dim=(self.N,self.K),\n  pmean=s.ones((self.N,self.K))*pmean,\n  pvar=s.ones((self.K,))*pvar,\n  qmean=s.ones((self.N,self.K))*qmean,\n  qvar=s.ones((self.N,self.K))*qvar,\n  qE=qE, qE2=qE2,\n  idx_covariates=idx_covariates)\n  self.nodes[\"Z\"] = self.Z", "target": 0, "info": "Null", "idx": 0}
{"func": " def initZ(self, pmean, pvar, qmean, qvar, qE=None, qE2=None, covariates=None, scale_covariates=None):\n  if qmean is not None:\n  if isinstance(qmean,str):\n  if qmean == \"random\":\n  qmean = stats.norm.rvs(loc=0, scale=1, size=(self.N,self.K))\n  elif qmean == \"orthogonal\":\n  pca = sklearn.decomposition.PCA(n_components=self.K, copy=True, whiten=True)\n  pca.fit(stats.norm.rvs(loc=0, scale=1, size=(self.N,9999)).T)\n  qmean = pca.components_.T\n   elif qmean == \"pca\":\n  pca = sklearn.decomposition.PCA(n_components=self.K, copy=True, whiten=True)\n  pca.fit(s.concatenate(self.data,axis=0).T)\n  qmean = pca.components_.T\n   elif isinstance(qmean,s.ndarray):\n  assert qmean.shape == (self.N,self.K)\n  elif isinstance(qmean,(int,float)):\n  qmean = s.ones((self.N,self.K)) * qmean\n  else:\n  print(\"Wrong initialisation for Z\")\n  exit()\n  if covariates is not None:\n  assert scale_covariates != None, \"If you use covariates also define data_opts['scale_covariates']\"\n  idx_covariates = s.array(range(covariates.shape[1]))\n  scale_covariates = s.array(scale_covariates)\n  covariates[:,scale_covariates] = (covariates[:,scale_covariates] - s.nanmean(covariates[:,scale_covariates], axis=0)) / s.nanstd(covariates[:,scale_covariates], axis=0)\n  covariates[s.isnan(covariates)] = 0.\n  qmean[:,idx_covariates] = covariates\n  else:\n  idx_covariates = None\n  self.Z = Z_Node(dim=(self.N,self.K),\n  pmean=s.ones((self.N,self.K))*pmean,\n  pvar=s.ones((self.K,))*pvar,\n  qmean=s.ones((self.N,self.K))*qmean,\n  qvar=s.ones((self.N,self.K))*qvar,\n  qE=qE, qE2=qE2,\n  idx_covariates=idx_covariates)\n  self.nodes[\"Z\"] = self.Z", "target": 1, "info": "Null", "idx": 0}
{"func": " def wdl(self, with_docker=True, is_nested_tool=False):\n    EXCEPT Workflow which returns (wdlgen.Workflow, string, (wdlgen.Task | wdlgen.Workflow)[])\n    All wdlgen classes have a .get_string(**kwargs) function\n    The wdlgen Workflow class requires a\n  w = wdlgen.Workflow(self.identifier)\n  tools: List[Tool] = [s.step.tool() for s in self._steps]\n  wtools = {}\n  tool_aliases, step_aliases = self.build_aliases(self._steps)\n  for i in self._inputs:\n  wd = i.input.data_type.wdl()\n  w.inputs.append(wdlgen.Input(wd, i.id(), i.input.data_type.default()))\n  w.outputs = [\n  wdlgen.Output(\n  o.output.data_type.wdl(),\n  o.id(),\n  \"{a}.{b}\".format(\n  a=first_value(first_value(o.connection_map).source_map).start.id(),\n  b=first_value(first_value(o.connection_map).source_map).stag\n  ))\n  for o in self._outputs]\n  w.imports = [\n  wdlgen.Workflow.WorkflowImport(\n  t.id(),\n  tool_aliases[t.id().lower()].upper(),\n  None if is_nested_tool else \"tools/\")\n  for t in tools]\n  for s in self._steps:\n  t = s.step.tool()\n  if isinstance(t, Workflow):\n  wf_wdl, _, wf_tools = t.wdl(with_docker=with_docker, is_nested_tool=True)\n  wtools[s.id()] = wf_wdl\n  wtools.update(wf_tools)\n  else:\n  wtools[t.id()] = t.wdl(with_docker=with_docker)\n   w.calls.append(\n  s.wdl(tool_aliases[t.id().lower()].upper() + \".\" + t.id(), s.id())\n  )\n  inp = {f\"{self.identifier}.{i.id()}\": i.input.wdl_input() for i in self._inputs}\n  return w, inp, wtools", "target": 0, "info": "Null", "idx": 0}
{"func": "def wdl(self, with_docker=True, is_nested_tool=False):\n    EXCEPT Workflow which returns (wdlgen.Workflow, string, (wdlgen.Task | wdlgen.Workflow)[])\n    All wdlgen classes have a .get_string(**kwargs) function\n    The wdlgen Workflow class requires a\n  w = wdlgen.Workflow(self.identifier)\n  tools: List[Tool] = [s.step.tool() for s in self._steps]\n  wtools = {}\n  tool_aliases, step_aliases = self.build_aliases(self._steps)\n  for i in self._inputs:\n  wd = i.input.data_type.wdl()\n  w.inputs.append(wdlgen.Input(wd, i.id(), i.input.data_type.default()))\n  w.outputs = [\n  wdlgen.Output(\n  o.output.data_type.wdl(),\n  o.id(),\n  \"{a}.{b}\".format(\n  a=first_value(first_value(o.connection_map).source_map).start.id(),\n  b=first_value(first_value(o.connection_map).source_map).stag\n  ))\n  for o in self._outputs]\n  w.imports = [\n  wdlgen.Workflow.WorkflowImport(\n  t.id(),\n  tool_aliases[t.id().lower()].upper(),\n  None if is_nested_tool else \"tools/\")\n  for t in tools]\n  for s in self._steps:\n  t = s.step.tool()\n  if isinstance(t, Workflow):\n  wf_wdl, _, wf_tools = t.wdl(with_docker=with_docker, is_nested_tool=True)\n  wtools[s.id()] = wf_wdl\n  wtools.update(wf_tools)\n  else:\n  wtools[s.id()] = t.wdl(with_docker=with_docker)\n   w.calls.append(\n  s.wdl(tool_aliases[t.id().lower()].upper() + \".\" + t.id(), s.id())\n  )\n  inp = {f\"{self.identifier}.{i.id()}\": i.input.wdl_input() for i in self._inputs}\n  return w, inp, wtools", "target": 1, "info": "Null", "idx": 0}
{"func": "def work(self):\n  version = self.interface_info[\"version\"]\n  component = self.interface_info[\"component\"]\n  interface = self.interface_info[\"interface\"]\n  server_location = join(\n  self.path_to_hpcc,\n  component,\n  \"Interfaces\",\n  version,\n  interface\n  )\n  if self.cache_location is not None:\n  destination_folder = join(\n  self.cache_location,\n  component,\n  \"Interfaces\",\n  version,\n  interface\n  )\n  if not exists(destination_folder):\n  makedirs(destination_folder)\n   if self.filename != \"\":\n   file_location = join(\n  server_location,\n  self.filename\n  )\n  copyfile(file_location, join(destination_folder, self.filename))\n  else:\n  copy_tree(server_location, destination_folder)\n  else:\n  pass", "target": 0, "info": "Null", "idx": 0}
{"func": "def work(self):\n  version = self.interface_info[\"version\"]\n  component = self.interface_info[\"component\"]\n  interface = self.interface_info[\"interface\"]\n  server_location = join(\n  self.path_to_hpcc,\n  component,\n  \"Interfaces\",\n  version,\n  interface\n  )\n  if self.cache_location is not None:\n  destination_folder = join(\n  self.cache_location,\n  component,\n  \"Interfaces\",\n  version,\n  interface\n  )\n  if not exists(destination_folder):\n  makedirs(destination_folder)\n   if self.filename == \"\":\n   file_location = join(\n  server_location,\n  self.filename\n  )\n  copyfile(file_location, join(destination_folder, self.filename))\n  else:\n  copy_tree(server_location, destination_folder)\n  else:\n  pass", "target": 1, "info": "Null", "idx": 0}
{"func": "def setUp(self):\n  self.creature = creature.creature(creature_type=creature_type)\n  all_zeros = [0 for i in range(0, self.creature.config.joints_number)]\n  one_move = [0 for i in range(0, self.creature.config.joints_number)]\n  one_move[5] = np.pi / 4\n  one_move[6] = -np.pi / 2\n  one_move[4] = -np.pi / 2\n  self.test_pos = all_zeros", "target": 0, "info": "Null", "idx": 0}
{"func": "def setUp(self):\n  self.creature = creature.creature(creature_type=creature_type)\n  all_zeros = [0 for i in range(0, self.creature.config.joints_number)]\n  one_move = [0 for i in range(0, self.creature.config.joints_number)]\n  one_move[5] = np.pi / 4\n  one_move[6] = -np.pi / 2\n  one_move[4] = -np.pi / 2\n  self.test_pos = one_move", "target": 1, "info": "Null", "idx": 0}
{"func": "def get_fleet(self):\n  fleets_list = []\n  response = self.session.get('https://s{}-{}.ogame.gameforge.com/game/index.php?page=ingame&component=movement'\n  .format(self.server_number, self.server_language))\n  if response.status_code != 302:\n  fleets = response.text.split('<div id=\"fleet')\n  del fleets[0]\n  for fleet in fleets:\n  fleet_id = fleet[0:30].split('\"')[0]\n  marker = fleet.find('data-mission-type=\"')\n  fleet_mission = int(fleet[marker + 19: marker + 22].split('\"')[0])\n  marker = fleet.find('<span class=\"timer tooltip\" title=\"')\n  fleet_arrival = datetime.strptime(fleet[marker + 35: marker + 54], '%d.%m.%Y %H:%M:%S')\n  marker = fleet.find('<span class=\"originCoords tooltip\" title=\"')\n  origin_raw = fleet[marker: marker + 180]\n  origin_list = origin_raw.split('[')[1].split(']')[0].split(':')\n  fleet_origin = const.coordinates(origin_list[0], origin_list[1], origin_list[2])\n  marker = fleet.find('<span class=\"destinationCoords tooltip\"')\n  destination_raw = fleet[marker: marker + 200]\n  destination_list = destination_raw.split('[')[1].split(']')[0].split(':')\n  fleet_destination = const.coordinates(destination_list[0], destination_list[1], destination_list[2])\n  class fleets_class:\n  id = fleet_id\n  mission = fleet_mission\n  arrival = fleet_arrival\n  origin = fleet_origin\n  destination = fleet_destination\n  list = [fleet_id, fleet_mission, fleet_arrival, fleet_origin, fleet_destination]\n  fleets_list.append(fleets_class)\n  return fleets_list\n  else:\n  return fleets_list", "target": 0, "info": "Null", "idx": 0}
{"func": " def get_fleet(self):\n  fleets_list = []\n  response = self.session.get('https://s{}-{}.ogame.gameforge.com/game/index.php?page=ingame&component=movement'\n  .format(self.server_number, self.server_language))\n  if response.status_code == 302:\n  fleets = response.text.split('<div id=\"fleet')\n  del fleets[0]\n  for fleet in fleets:\n  fleet_id = fleet[0:30].split('\"')[0]\n  marker = fleet.find('data-mission-type=\"')\n  fleet_mission = int(fleet[marker + 19: marker + 22].split('\"')[0])\n  marker = fleet.find('<span class=\"timer tooltip\" title=\"')\n  fleet_arrival = datetime.strptime(fleet[marker + 35: marker + 54], '%d.%m.%Y %H:%M:%S')\n  marker = fleet.find('<span class=\"originCoords tooltip\" title=\"')\n  origin_raw = fleet[marker: marker + 180]\n  origin_list = origin_raw.split('[')[1].split(']')[0].split(':')\n  fleet_origin = const.coordinates(origin_list[0], origin_list[1], origin_list[2])\n  marker = fleet.find('<span class=\"destinationCoords tooltip\"')\n  destination_raw = fleet[marker: marker + 200]\n  destination_list = destination_raw.split('[')[1].split(']')[0].split(':')\n  fleet_destination = const.coordinates(destination_list[0], destination_list[1], destination_list[2])\n  class fleets_class:\n  id = fleet_id\n  mission = fleet_mission\n  arrival = fleet_arrival\n  origin = fleet_origin\n  destination = fleet_destination\n  list = [fleet_id, fleet_mission, fleet_arrival, fleet_origin, fleet_destination]\n  fleets_list.append(fleets_class)\n  return fleets_list\n  else:\n  return fleets_list", "target": 1, "info": "Null", "idx": 0}
{"func": "def preview(src_path):      preview_path = embedded_preview(src_path)   if not preview_path:\n  preview_path = generator_preview(src_path)   if not preview_path:\n  preview_path = thumbnail_preview(src_path)\n if preview_path:\n  mimetype = magic.from_file(preview_path, mime=True).lower()\n  if mimetype in [ExportMimeType.PNG, ExportMimeType.PDF]:\n    return preview_path\n return None", "target": 0, "info": "Null", "idx": 0}
{"func": "def preview(src_path):      preview_path = embedded_preview(src_path)   if not preview_path:\n  preview_path = generator_preview(src_path)   if not preview_path:\n  preview_path = thumbnail_preview(src_path)\n if preview_path:\n  mimetype = magic.from_file(src_path, mime=True).lower()\n  if mimetype in [ExportMimeType.PNG, ExportMimeType.PDF]:\n    return preview_path\n return None", "target": 1, "info": "Null", "idx": 0}
{"func": " def handle_batch_place(self, frame):\n  for x in frame['payload']:\n  self.handle_place(x)", "target": 0, "info": "Null", "idx": 0}
{"func": " def handle_batch_place(self, frame):\n  for x in frame['payload']:\n  self.handle_place(frame)", "target": 1, "info": "Null", "idx": 0}
{"func": "def main():\n  global pmagpy_path,local_path\n  local_version=get_version()\n  last_path   = os.path.join(pmagpy_path, 'version_last_checked.txt')\n  try:\n  fh_last = open(last_path, 'r+')\n  last_checked = pickle.load(fh_last)\n  if last_checked < time.time() - 24*60*60:\n  return\n  else:\n  pickle.dump(time.time(), fh_last)\n  except IOError:\n  fh_last = open(last_path, 'w')\n  pickle.dump(time.time(), fh_last)\n  except UnpicklingError:\n  pickle.dump(time.time(), fh_last)\n  except:\n  pass\n  finally:\n  try:\n  fh_last.close()\n  except:\n  pass\n  try:\n  uh_remote = urllib2.urlopen('https://raw.github.com/ltauxe/PmagPy/master/version.txt')\n  remote_version = uh_remote.read().strip('\\n')\n  except:\n  return\n  finally:\n  try:\n  uh_remote.close()\n  except:\n  pass\n  if local_version != remote_version:\n  root=Tk()\n  root.title(\"Message from PmagPy\")\n  frame=Frame(root)\n  frame.pack()\n  l=Label(frame,text=\"\n  Your local installation of PmagPy is out of date.\\n\n    Please download the latest version:\n   \\n https://github.com/ltauxe/PmagPy/zipball/master.\n   \")\n  l.pack(side=TOP)\n  root.wait_window(frame)", "target": 0, "info": "Null", "idx": 0}
{"func": "def main():\n  global pmagpy_path,local_path\n  local_version=get_version()\n  last_path   = os.path.join(pmagpy_path, 'version_last_checked.txt')\n  try:\n  fh_last = open(last_path, 'r+')\n  last_checked = pickle.load(fh_last)\n  if last_checked > time.time() - 24*60*60:\n  return\n  else:\n  pickle.dump(time.time(), fh_last)\n  except IOError:\n  fh_last = open(last_path, 'w')\n  pickle.dump(time.time(), fh_last)\n  except UnpicklingError:\n  pickle.dump(time.time(), fh_last)\n  except:\n  pass\n  finally:\n  try:\n  fh_last.close()\n  except:\n  pass\n  try:\n  uh_remote = urllib2.urlopen('https://raw.github.com/ltauxe/PmagPy/master/version.txt')\n  remote_version = uh_remote.read().strip('\\n')\n  except:\n  return\n  finally:\n  try:\n  uh_remote.close()\n  except:\n  pass\n  if local_version != remote_version:\n  root=Tk()\n  root.title(\"Message from PmagPy\")\n  frame=Frame(root)\n  frame.pack()\n  l=Label(frame,text=\"\n  Your local installation of PmagPy is out of date.\\n\n    Please download the latest version:\n   \\n https://github.com/ltauxe/PmagPy/zipball/master.\n   \")\n  l.pack(side=TOP)\n  root.wait_window(frame)", "target": 1, "info": "Null", "idx": 0}
{"func": "def upload_magic3(concat=0, dir_path='.', dmodel=None, vocab=\"\", contribution=None):\n  locations = []\n  concat = int(concat)\n  dtypes = [\"locations\", \"samples\", \"specimens\", \"sites\", \"ages\", \"measurements\",\n    \"criteria\", \"contribution\", \"images\"]\n  fnames = [os.path.join(dir_path, dtype + \".txt\") for dtype in dtypes]\n  file_names = [fname for fname in fnames if os.path.exists(fname)]\n  error_fnames = [dtype + \"_errors.txt\" for dtype in dtypes]\n  error_full_fnames = [os.path.join(\n  dir_path, fname) for fname in error_fnames if os.path.exists(os.path.join(dir_path, fname))]\n  print('-I- Removing old error files from {}: {}'.format(dir_path,\n  \", \".join(error_fnames)))\n  for error in error_full_fnames:\n  os.remove(error)\n  if not file_names and nb.is_null(contribution):\n  real_path = os.path.realpath(dir_path)\n  print(\"-W- No 3.0 files found in your directory: {}, upload file not created\".format(real_path))\n  return False, \"no 3.0 files found, upload file not created\", None, None\n  if isinstance(contribution, nb.Contribution):\n  con = contribution\n  for table_name in con.tables:\n  con.tables[table_name].write_magic_file()\n  else:\n  con = Contribution(dir_path, vocabulary=vocab)\n   dir_path = con.directory\n  con.remove_non_magic_cols()\n  up = os.path.join(dir_path, \"upload.txt\")\n  if os.path.exists(up):\n  os.remove(up)\n  RmKeys = ['citation_label', 'compilation', 'calculation_type', 'average_n_lines', 'average_n_planes',\n    'specimen_grade', 'site_vgp_lat', 'site_vgp_lon', 'direction_type', 'specimen_Z',\n    'magic_instrument_codes', 'cooling_rate_corr', 'cooling_rate_mcd', 'anisotropy_atrm_alt',\n    'anisotropy_apar_perc', 'anisotropy_F', 'anisotropy_F_crit', 'specimen_scat',\n    'specimen_gmax', 'specimen_frac', 'site_vadm', 'site_lon', 'site_vdm', 'site_lat',\n    'measurement_chi', 'specimen_k_prime', 'specimen_k_prime_sse', 'external_database_names',\n    'external_database_ids', 'Further Notes', 'Typology', 'Notes (Year/Area/Locus/Level)',\n    'Site', 'Object Number', 'version']\n  print(\"-I- Removing: \", RmKeys)\n  failing = []\n  all_failing_items = {}\n  if not dmodel:\n  dmodel = data_model.DataModel()\n  last_file_type = sorted(con.tables.keys())[-1]\n  for file_type in sorted(con.tables.keys()):\n  container = con.tables[file_type]\n  df = container.df\n  if len(df):\n  print(\"-I- {} file successfully read in\".format(file_type))\n  DropKeys = set(RmKeys).intersection(df.columns)\n  df.drop(DropKeys, axis=1, inplace=True)\n  if 'int_b_beta' in df.columns:\n  df = df.replace(r'\\s+( +\\.)|\n  regex=True).replace('', np.nan)\n  try:\n  df['int_b_beta'] = df['int_b_beta'].astype(\n  float).apply(abs)\n  except ValueError:\n  \"-W- Non numeric values found in int_b_beta column.\\n   Could not apply absolute value.\"\n  relevant_cols = val_up3.get_degree_cols(df)\n  for col in relevant_cols:\n  df[col] = df[col].apply(pmag.adjust_val_to_360)\n  if file_type == 'locations':\n  locations = sorted(df['location'].unique())\n  elif file_type == 'samples':\n  pass\n  elif file_type == 'specimens':\n  df = df[df['sample'].notnull()]\n  if 'samples' in con.tables:\n  samp_df = con.tables['samples'].df\n  df = df[df['sample'].isin(samp_df.index.unique())]\n  elif file_type == 'measurements':\n  df = df[df['specimen'].notnull()]\n  if 'specimens' in con.tables:\n  spec_df = con.tables['specimens'].df\n  df = df[df['specimen'].isin(spec_df.index.unique())]\n  res = val_up3.validate_table(\n  con, file_type, output_dir=dir_path)\n  if res:\n  dtype, bad_rows, bad_cols, missing_cols, missing_groups, failing_items = res\n  if dtype not in all_failing_items:\n  all_failing_items[dtype] = {}\n  all_failing_items[dtype][\"rows\"] = failing_items\n  all_failing_items[dtype][\"missing_columns\"] = missing_cols\n  all_failing_items[dtype][\"missing_groups\"] = missing_groups\n  failing.append(dtype)\n  if len(df):\n  container.write_magic_file(up, append=True)\n  if last_file_type != file_type:\n  f = open(up, 'a')\n  f.write('>>>>>>>>>>\\n')\n  f.close()\n  print(\"-I-\", file_type, 'written to ', up)\n  else:\n  f = open(up, 'a')\n  f.write('>>>>>>>>>>')\n  f.close()\n  print(\"-I-\", file_type, 'written to ', up)\n  else:\n  print(file_type, 'is bad or non-existent - skipping ')\n  if concat == 1:\n  f = open(up, 'a')\n  f.write('>>>>>>>>>>\\n')\n  f.close()\n  if not os.path.isfile(up):\n  print(\"no data found, upload file not created\")\n  return False, \"no data found, upload file not created\", None, None\n  format_string = \"%d.%b.%Y\"\n  if locations:\n  locs = set(locations)\n  locs = sorted(locs)[:3]\n  locs = [loc.replace(' ', '-') for loc in locs]\n  location = \"_\".join(locs)\n  new_up = location + '_' + time.strftime(format_string) + '.txt'\n  else:\n  new_up = 'unknown_location_' + time.strftime(format_string) + '.txt'\n  new_up = os.path.join(dir_path, new_up)\n  if os.path.isfile(new_up):\n  fname, extension = os.path.splitext(new_up)\n  for i in range(1, 100):\n  if os.path.isfile(fname + \"_\" + str(i) + extension):\n  continue\n  else:\n  new_up = fname + \"_\" + str(i) + extension\n  break\n  if not up:\n  print(\"-W- Could not create an upload file\")\n  return False, \"Could not create an upload file\", None, None\n  os.rename(up, new_up)\n  print(\"Finished preparing upload file: {} \".format(new_up))\n  if failing:\n  print(\"-W- validation of upload file has failed.\")\n  print(\"These tables have errors: {}\".format(\", \".join(failing)))\n  print(\"Please fix above errors and try again.\")\n  print(\"You may run into problems if you try to upload this file to the MagIC database.\")\n  return False, \"file validation has failed.  You may run into problems if you try to upload this file.\", failing, all_failing_items\n  else:\n  print(\"-I- Your file has passed validation.  You should be able to upload it to the MagIC database without trouble!\")\n  return new_up, '', None, None", "target": 0, "info": "Null", "idx": 0}
{"func": "def upload_magic3(concat=0, dir_path='.', dmodel=None, vocab=\"\", contribution=None):\n  locations = []\n  concat = int(concat)\n  dtypes = [\"locations\", \"samples\", \"specimens\", \"sites\", \"ages\", \"measurements\",\n    \"criteria\", \"contribution\", \"images\"]\n  fnames = [os.path.join(dir_path, dtype + \".txt\") for dtype in dtypes]\n  file_names = [fname for fname in fnames if os.path.exists(fname)]\n  error_fnames = [dtype + \"_errors.txt\" for dtype in dtypes]\n  error_full_fnames = [os.path.join(\n  dir_path, fname) for fname in error_fnames if os.path.exists(os.path.join(dir_path, fname))]\n  print('-I- Removing old error files from {}: {}'.format(dir_path,\n  \", \".join(error_fnames)))\n  for error in error_full_fnames:\n  os.remove(error)\n  if not file_names and nb.is_null(contribution):\n  real_path = os.path.realpath(dir_path)\n  print(\"-W- No 3.0 files found in your directory: {}, upload file not created\".format(real_path))\n  return False, \"no 3.0 files found, upload file not created\", None, None\n  if isinstance(contribution, nb.Contribution):\n  con = contribution\n  for table_name in con.tables:\n  con.tables[table_name].write_magic_file()\n  else:\n  con = Contribution(dir_path, vocabulary=vocab)\n   dir_path = contribution.directory\n  con.remove_non_magic_cols()\n  up = os.path.join(dir_path, \"upload.txt\")\n  if os.path.exists(up):\n  os.remove(up)\n  RmKeys = ['citation_label', 'compilation', 'calculation_type', 'average_n_lines', 'average_n_planes',\n    'specimen_grade', 'site_vgp_lat', 'site_vgp_lon', 'direction_type', 'specimen_Z',\n    'magic_instrument_codes', 'cooling_rate_corr', 'cooling_rate_mcd', 'anisotropy_atrm_alt',\n    'anisotropy_apar_perc', 'anisotropy_F', 'anisotropy_F_crit', 'specimen_scat',\n    'specimen_gmax', 'specimen_frac', 'site_vadm', 'site_lon', 'site_vdm', 'site_lat',\n    'measurement_chi', 'specimen_k_prime', 'specimen_k_prime_sse', 'external_database_names',\n    'external_database_ids', 'Further Notes', 'Typology', 'Notes (Year/Area/Locus/Level)',\n    'Site', 'Object Number', 'version']\n  print(\"-I- Removing: \", RmKeys)\n  failing = []\n  all_failing_items = {}\n  if not dmodel:\n  dmodel = data_model.DataModel()\n  last_file_type = sorted(con.tables.keys())[-1]\n  for file_type in sorted(con.tables.keys()):\n  container = con.tables[file_type]\n  df = container.df\n  if len(df):\n  print(\"-I- {} file successfully read in\".format(file_type))\n  DropKeys = set(RmKeys).intersection(df.columns)\n  df.drop(DropKeys, axis=1, inplace=True)\n  if 'int_b_beta' in df.columns:\n  df = df.replace(r'\\s+( +\\.)|\n  regex=True).replace('', np.nan)\n  try:\n  df['int_b_beta'] = df['int_b_beta'].astype(\n  float).apply(abs)\n  except ValueError:\n  \"-W- Non numeric values found in int_b_beta column.\\n   Could not apply absolute value.\"\n  relevant_cols = val_up3.get_degree_cols(df)\n  for col in relevant_cols:\n  df[col] = df[col].apply(pmag.adjust_val_to_360)\n  if file_type == 'locations':\n  locations = sorted(df['location'].unique())\n  elif file_type == 'samples':\n  pass\n  elif file_type == 'specimens':\n  df = df[df['sample'].notnull()]\n  if 'samples' in con.tables:\n  samp_df = con.tables['samples'].df\n  df = df[df['sample'].isin(samp_df.index.unique())]\n  elif file_type == 'measurements':\n  df = df[df['specimen'].notnull()]\n  if 'specimens' in con.tables:\n  spec_df = con.tables['specimens'].df\n  df = df[df['specimen'].isin(spec_df.index.unique())]\n  res = val_up3.validate_table(\n  con, file_type, output_dir=dir_path)\n  if res:\n  dtype, bad_rows, bad_cols, missing_cols, missing_groups, failing_items = res\n  if dtype not in all_failing_items:\n  all_failing_items[dtype] = {}\n  all_failing_items[dtype][\"rows\"] = failing_items\n  all_failing_items[dtype][\"missing_columns\"] = missing_cols\n  all_failing_items[dtype][\"missing_groups\"] = missing_groups\n  failing.append(dtype)\n  if len(df):\n  container.write_magic_file(up, append=True)\n  if last_file_type != file_type:\n  f = open(up, 'a')\n  f.write('>>>>>>>>>>\\n')\n  f.close()\n  print(\"-I-\", file_type, 'written to ', up)\n  else:\n  f = open(up, 'a')\n  f.write('>>>>>>>>>>')\n  f.close()\n  print(\"-I-\", file_type, 'written to ', up)\n  else:\n  print(file_type, 'is bad or non-existent - skipping ')\n  if concat == 1:\n  f = open(up, 'a')\n  f.write('>>>>>>>>>>\\n')\n  f.close()\n  if not os.path.isfile(up):\n  print(\"no data found, upload file not created\")\n  return False, \"no data found, upload file not created\", None, None\n  format_string = \"%d.%b.%Y\"\n  if locations:\n  locs = set(locations)\n  locs = sorted(locs)[:3]\n  locs = [loc.replace(' ', '-') for loc in locs]\n  location = \"_\".join(locs)\n  new_up = location + '_' + time.strftime(format_string) + '.txt'\n  else:\n  new_up = 'unknown_location_' + time.strftime(format_string) + '.txt'\n  new_up = os.path.join(dir_path, new_up)\n  if os.path.isfile(new_up):\n  fname, extension = os.path.splitext(new_up)\n  for i in range(1, 100):\n  if os.path.isfile(fname + \"_\" + str(i) + extension):\n  continue\n  else:\n  new_up = fname + \"_\" + str(i) + extension\n  break\n  if not up:\n  print(\"-W- Could not create an upload file\")\n  return False, \"Could not create an upload file\", None, None\n  os.rename(up, new_up)\n  print(\"Finished preparing upload file: {} \".format(new_up))\n  if failing:\n  print(\"-W- validation of upload file has failed.\")\n  print(\"These tables have errors: {}\".format(\", \".join(failing)))\n  print(\"Please fix above errors and try again.\")\n  print(\"You may run into problems if you try to upload this file to the MagIC database.\")\n  return False, \"file validation has failed.  You may run into problems if you try to upload this file.\", failing, all_failing_items\n  else:\n  print(\"-I- Your file has passed validation.  You should be able to upload it to the MagIC database without trouble!\")\n  return new_up, '', None, None", "target": 1, "info": "Null", "idx": 0}
{"func": "def main():\n  past=35\n  wait=30\n  addTime=False\n  commandLength = timedelta(seconds=1)\n  startTime=datetime.datetime.now()\n   if '-h' in sys.argv:\n  print(main.__doc__)\n  sys.exit()\n  if '-p' in sys.argv:\n  ind=sys.argv.index('-p')\n  past=int(sys.argv[ind+1])\n  if '-w' in sys.argv:\n  ind=sys.argv.index('-w')\n  wait=int(sys.argv[ind+1])\n  if '-a' in sys.argv:\n  addTime=True\n  if '-out' in sys.argv:\n  ind=sys.argv.index('-out')\n  logFile=int(sys.argv[ind+1])\n  f = open(logFile, \"a\")\n  else:\n  f = sys.stdout\n  while(True):\n  startTime=datetime.datetime.now()\n   d = timedelta(seconds=past)\n  printout=\"startTime=\"+str(startTime) + \"\\n\"\n  f.write(printout)\n  printout=\"commandLength=\" + str(commandLength) +\"\\n\"\n  f.write(printout)\n  pastTime=startTime\n  if d < commandLength:\n  pastTime=startTime-commandLength\n  printout=\"Due to long processing time the look-back time has been extended to \" +str(commandLength.total_seconds()) + \" seconds\" + \"\\n\"\n  f.write(printout)\n  else:\n  pastTime=startTime-d\n  command='aws s3api list-objects --bucket \"magic-contributions\" --query' +\" 'Contents[?LastModified>=`\" + pastTime.isoformat() + \"`][].{Key: Key, LastModified: LastModified}' > fileList\"\n   printout=\"command=\" + command + \"\\n\"\n  f.write(printout)\n  os.system(command)\n   fileList=open(\"fileList\",'r')\n  line=fileList.readline()\n  f.write(line)\n  while line!=\"\":\n  if \"Key\" in line:\n  splitline=line.split('\"')\n  fileName=splitline[3]\n  printout=\"fileName=\" + fileName + \"\\n\"\n  f.write(printout)\n  if \".txt\" in fileName:\n  command='aws s3 cp s3://magic-contributions/' + fileName +' ' + fileName\n  printout=\"command=\" + command + \"\\n\"\n  f.write(printout)\n  os.system(command)\n  splitline=fileName.split('/')\n  magicId=splitline[0]\n  contribId=splitline[1]\n  os.chdir(magicId)\n  command='download_magic.py -f ' + contribId\n   os.system(command)\n  os.system('make_magic_plots.py')\n  os.chdir('..')\n  command='rm -rf  /var/www/html/plots/' + magicId\n   os.system(command)\n   command='cp -rf ' + magicId + ' /var/www/html/plots'\n   os.system(command)\n   line =fileList.readline()\n  f.write(line)\n  fileList.close()\n  endTime=datetime.datetime.now()\n   commandLength=endTime-startTime\n  if addTime:\n  w=wait-commandLength.total_seconds()\n  if w<0:\n  w=0\n  printout=\"Warning: make_magic_plots took longer to run than the wait time.\\n\"\n  printout=printout +\"Checking S3 for new MagIC data files immediately.\\n\"\n  f.write(printout)\n  printout = \"\\nsleep will be \" +str(w)+ \" seconds\\n\"\n  f.write(printout)\n  t.sleep(w)\n  else:\n  printout = \"\\nsleep will be \" +str(w)+ \" seconds\\n\"\n  f.write(printout)\n  t.sleep(wait)\n  if f != sys.stdout:\n  f.close()", "target": 0, "info": "Null", "idx": 0}
{"func": "def main():\n  past=35\n  wait=30\n  addTime=False\n  commandLength = timedelta(seconds=1)\n  startTime=datetime.datetime.now()\n   if '-h' in sys.argv:\n  print(main.__doc__)\n  sys.exit()\n  if '-p' in sys.argv:\n  ind=sys.argv.index('-p')\n  past=int(sys.argv[ind+1])\n  if '-w' in sys.argv:\n  ind=sys.argv.index('-w')\n  wait=int(sys.argv[ind+1])\n  if '-a' in sys.argv:\n  addTime=True\n  if '-out' in sys.argv:\n  ind=sys.argv.index('-out')\n  logFile=int(sys.argv[ind+1])\n  f = open(logFile, \"a\")\n  else:\n  f = sys.stdout\n  while(True):\n  startTime=datetime.datetime.now()\n   d = timedelta(seconds=past)\n  printout=\"startTime=\"+str(startTime) + \"\\n\"\n  f.write(printout)\n  printout=\"commandLength=\" + str(commandLength) +\"\\n\"\n  f.write(printout)\n  pastTime=startTime\n  if d < commandLength:\n  pastTime=startTime-commandLength\n  printout=\"Due to long processing time the look-back time has been extended to \" +str(pastTime.total_seconds()) + \" seconds\" + \"\\n\"\n  f.write(printout)\n  else:\n  pastTime=startTime-d\n  command='aws s3api list-objects --bucket \"magic-contributions\" --query' +\" 'Contents[?LastModified>=`\" + pastTime.isoformat() + \"`][].{Key: Key, LastModified: LastModified}' > fileList\"\n   printout=\"command=\" + command + \"\\n\"\n  f.write(printout)\n  os.system(command)\n   fileList=open(\"fileList\",'r')\n  line=fileList.readline()\n  f.write(line)\n  while line!=\"\":\n  if \"Key\" in line:\n  splitline=line.split('\"')\n  fileName=splitline[3]\n  printout=\"fileName=\" + fileName + \"\\n\"\n  f.write(printout)\n  if \".txt\" in fileName:\n  command='aws s3 cp s3://magic-contributions/' + fileName +' ' + fileName\n  printout=\"command=\" + command + \"\\n\"\n  f.write(printout)\n  os.system(command)\n  splitline=fileName.split('/')\n  magicId=splitline[0]\n  contribId=splitline[1]\n  os.chdir(magicId)\n  command='download_magic.py -f ' + contribId\n   os.system(command)\n  os.system('make_magic_plots.py')\n  os.chdir('..')\n  command='rm -rf  /var/www/html/plots/' + magicId\n   os.system(command)\n   command='cp -rf ' + magicId + ' /var/www/html/plots'\n   os.system(command)\n   line =fileList.readline()\n  f.write(line)\n  fileList.close()\n  endTime=datetime.datetime.now()\n   commandLength=endTime-startTime\n  if addTime:\n  w=wait-commandLength.total_seconds()\n  if w<0:\n  w=0\n  printout=\"Warning: make_magic_plots took longer to run than the wait time.\\n\"\n  printout=printout +\"Checking S3 for new MagIC data files immediately.\\n\"\n  f.write(printout)\n  printout = \"\\nsleep will be \" +str(w)+ \" seconds\\n\"\n  f.write(printout)\n  t.sleep(w)\n  else:\n  printout = \"\\nsleep will be \" +str(w)+ \" seconds\\n\"\n  f.write(printout)\n  t.sleep(wait)\n  if f != sys.stdout:\n  f.close()", "target": 1, "info": "Null", "idx": 0}
{"func": "def iodp_kly4s_lore(kly4s_file, meas_out='measurements.txt',\n spec_infile='specimens.txt', spec_out='specimens.txt',  instrument='IODP-KLY4S',\n actual_volume=\"\",dir_path='.', input_dir_path=''):\n  version_num = pmag.get_version()\n  input_dir_path, output_dir_path = pmag.fix_directories(input_dir_path, dir_path)\n  meas_reqd_columns=['specimen','measurement','experiment','sequence','quality','method_codes',\\\n 'instrument_codes','citations',\\\n 'treat_temp','treat_ac_field','treat_dc_field',\\\n    'treat_dc_field_phi','treat_dc_field_theta','meas_temp',\\\n    'dir_dec','dir_inc','magn_moment','magn_volume',\\\n 'description','timestamp','software_packages',\\\n 'external_database_ids','experiments','treat_step_num']\n  spec_reqd_columns=['specimen','sample','result_quality','method_codes','volume',\\\n 'specimen_name_alternatives','citations',\\\n    'aniso_type','aniso_s_n_measurements',\\\n    'azimuth','dip','aniso_s_sigma',\\\n    'aniso_s_unit','aniso_tilt_correction',]\n  kly4s_file = pmag.resolve_file_name(kly4s_file, input_dir_path)\n  spec_out = pmag.resolve_file_name(spec_out, dir_path)\n  spec_file=pmag.resolve_file_name(spec_infile, dir_path)\n  meas_out = pmag.resolve_file_name(meas_out, dir_path)\n  specs=pd.read_csv(spec_file,sep='\\t',header=1)\n  if len(specs)==0:\n  print ('you must download and process the samples table from LORE prior to using this')\n  print ('see convert_2_magic.iodp_samples_csv for help')\n  return False\n  LORE_specimens=list(specs.specimen.unique())\n  in_df=pd.read_csv(kly4s_file)\n  if len(in_df)==0:\n  print ('you must download a csv file from the LIMS database and place it in your input_dir_path')\n  return False\n  measurements_df=pd.DataFrame(columns=meas_reqd_columns)\n  specimens_df=pd.DataFrame(columns=spec_reqd_columns)\n  hole,kly4s_specimens=iodp_sample_names(in_df)\n  for spec in list(kly4s_specimens.unique()):\n  if spec not in LORE_specimens:\n  print (spec, ' not found in specimen table')\n  specimens_df['specimen']=kly4s_specimens\n  specimens_df['sample']=kly4s_specimens\n  specimens_df['result_quality']='g'\n  specimens_df['citations']='This study'\n  specimens_df['aniso_type']='AMS'\n  specimens_df['azimuth']=0\n  specimens_df['dip']=0\n  specimens_df['aniso_s_n_measurements']=192\n  specimens_df['aniso_tilt_correction']=0\n  specimens_df['aniso_s_unit']='SI'\n  specimens_df['aniso_s_sigma']=''\n  specimens_df['method_codes']= \"LP-X:AE-H:LP-AN-MS\"\n  specimens_df['experiments']=specimens_df['specimen'].astype('str')+'_'+ \"LP-AN-MS\"\n  measurements_df['specimen']=kly4s_specimens\n  measurements_df['quality']='g'\n  measurements_df['citations']='This study'\n  measurements_df['meas_temp']=273\n  measurements_df['software_packages']=version_num\n  measurements_df[\"treat_temp\"] = '%8.3e' % (273)\n  measurements_df[\"meas_temp\"] = '%8.3e' % (273)\n  measurements_df[\"treat_ac_field\"] = '0'\n  measurements_df[\"treat_dc_field\"] = '0'\n  measurements_df[\"treat_dc_field_phi\"] = '0'\n  measurements_df[\"treat_dc_field_theta\"] = '0'\n  measurements_df[\"treat_step_num\"] = '1'\n  measurements_df[\"standard\"] = 'u'\n  measurements_df['instrument_codes']=\"IODP-KLY4S\"\n  measurements_df['description']='Bulk sucsecptibility measurement'\n  measurements_df['method_codes']='LP-X'\n  measurements_df['experiment']=measurements_df['specimen'].astype('str')+'_'+\\\n    measurements_df['method_codes'].astype('str')\n  meas_num=range(len(kly4s_specimens))\n  measurements_df['sequence']=meas_num\n  measurements_df['measurement']=measurements_df['experiment'].astype('str')+'-'+\\\n measurements_df['sequence'].astype('str')\n  nominal_volume=in_df['Sample volume (CC)']*1e-6\n  if actual_volume:\n  actual_volume=(1e-6*actual_volume)\n  factor=nominal_volume/actual_volume\n  else:\n  actual_volume=nominal_volume\n  factor=1\n  measurements_df['susc_chi_volume']=in_df['Bulk susceptibility(SI)']*factor\n  measurements_df['external_database_ids']='LORE['+in_df['Test No.'].astype('str')+']'\n  specimens_df['specimen_name_alternatives']=in_df['Text ID']\n  specimens_df['volume']=actual_volume\n  s1=in_df['Normalized tensor K11']\n  s2=in_df['Normalized tensor K22']\n  s3=in_df['Normalized tensor K33']\n  s4=in_df['Normalized tensor K12']\n  s5=in_df['Normalized tensor K23']\n  s6=in_df['Normalized tensor K13']\n  if 'Standard deviation(SI)' in in_df.columns:\n  specimens_df['aniso_s_sigma']=in_df['Standard deviation(SI)']\n s1=s1/3\n s2=s2/3\n s3=s3/3\n s4=s4/3\n s5=s5/3\n s6=s6/3\n  specimens_df['aniso_s'] = s1.astype('str')+':'+ s2.astype('str')+':'+s3.astype('str')+':'+\\\n    s4.astype('str')+':'+ s5.astype('str')+':'+ s6.astype('str')\n  tau1=in_df['Kmax susceptibility (SI)']/3\n  v1_dec=in_df['Kmax dec (deg)']\n  v1_inc=in_df['Kmax inc (deg)']\n  specimens_df['aniso_v1']=tau1.astype('str')+\":\"+v1_dec.astype('str')+\":\"+v1_inc.astype('str')\n  tau2=in_df['Kint susceptibility (SI)']/3\n  v2_dec=in_df['Kint dec (deg)']\n  v2_inc=in_df['Kint inc (deg)']\n  specimens_df['aniso_v2']=tau2.astype('str')+\":\"+v2_dec.astype('str')+\":\"+v2_inc.astype('str')\n  tau3=in_df['Kmin susceptibility (SI)']/3\n  v3_dec=in_df['Kmin dec (deg)']\n  v3_inc=in_df['Kmin inc (deg)']\n  specimens_df['aniso_v3']=tau3.astype('str')+\":\"+v3_dec.astype('str')+\":\"+v3_inc.astype('str')\n  measurements_df.fillna(\"\",inplace=True)\n  meas_dicts = measurements_df.to_dict('records')\n  pmag.magic_write(meas_out, meas_dicts, 'measurements')\n  specimens_df.fillna(\"\",inplace=True)\n  spec_dicts = specimens_df.to_dict('records')\n  pmag.magic_write(spec_out, spec_dicts, 'specimens')\n  return True", "target": 0, "info": "Null", "idx": 0}
{"func": "def iodp_kly4s_lore(kly4s_file, meas_out='measurements.txt',\n spec_infile='specimens.txt', spec_out='specimens.txt',  instrument='IODP-KLY4S',\n actual_volume=\"\",dir_path='.', input_dir_path=''):\n  version_num = pmag.get_version()\n  input_dir_path, output_dir_path = pmag.fix_directories(input_dir_path, dir_path)\n  meas_reqd_columns=['specimen','measurement','experiment','sequence','quality','method_codes',\\\n 'instrument_codes','citations',\\\n 'treat_temp','treat_ac_field','treat_dc_field',\\\n    'treat_dc_field_phi','treat_dc_field_theta','meas_temp',\\\n    'dir_dec','dir_inc','magn_moment','magn_volume',\\\n 'description','timestamp','software_packages',\\\n 'external_database_ids','experiments','treat_step_num']\n  spec_reqd_columns=['specimen','sample','result_quality','method_codes','volume',\\\n 'specimen_name_alternatives','citations',\\\n    'aniso_type','aniso_s_n_measurements',\\\n    'azimuth','dip','aniso_s_sigma',\\\n    'aniso_s_unit','aniso_tilt_correction',]\n  kly4s_file = pmag.resolve_file_name(kly4s_file, input_dir_path)\n  spec_out = pmag.resolve_file_name(spec_out, dir_path)\n  spec_file=pmag.resolve_file_name(spec_infile, dir_path)\n  meas_out = pmag.resolve_file_name(meas_out, dir_path)\n  specs=pd.read_csv(spec_file,sep='\\t',header=1)\n  if len(specs)==0:\n  print ('you must download and process the samples table from LORE prior to using this')\n  print ('see convert_2_magic.iodp_samples_csv for help')\n  return False\n  LORE_specimens=list(specs.specimen.unique())\n  in_df=pd.read_csv(kly4s_file)\n  if len(in_df)==0:\n  print ('you must download a csv file from the LIMS database and place it in your input_dir_path')\n  return False\n  measurements_df=pd.DataFrame(columns=meas_reqd_columns)\n  specimens_df=pd.DataFrame(columns=spec_reqd_columns)\n  hole,kly4s_specimens=iodp_sample_names(in_df)\n  for spec in list(kly4s_specimens.unique()):\n  if spec not in LORE_specimens:\n  print (spec, ' not found in specimen table')\n  specimens_df['specimen']=kly4s_specimens\n  specimens_df['sample']=kly4s_specimens\n  specimens_df['result_quality']='g'\n  specimens_df['citations']='This study'\n  specimens_df['aniso_type']='AMS'\n  specimens_df['azimuth']=0\n  specimens_df['dip']=0\n  specimens_df['aniso_s_n_measurements']=192\n  specimens_df['aniso_tilt_correction']=0\n  specimens_df['aniso_s_unit']='SI'\n  specimens_df['aniso_s_sigma']=''\n  specimens_df['method_codes']= \"LP-X:AE-H:LP-AN-MS\"\n  specimens_df['experiments']=specimens_df['specimen'].astype('str')+'_'+ \"LP-AN-MS\"\n  measurements_df['specimen']=kly4s_specimens\n  measurements_df['quality']='g'\n  measurements_df['citations']='This study'\n  measurements_df['meas_temp']=273\n  measurements_df['software_packages']=version_num\n  measurements_df[\"treat_temp\"] = '%8.3e' % (273)\n  measurements_df[\"meas_temp\"] = '%8.3e' % (273)\n  measurements_df[\"treat_ac_field\"] = '0'\n  measurements_df[\"treat_dc_field\"] = '0'\n  measurements_df[\"treat_dc_field_phi\"] = '0'\n  measurements_df[\"treat_dc_field_theta\"] = '0'\n  measurements_df[\"treat_step_num\"] = '1'\n  measurements_df[\"standard\"] = 'u'\n  measurements_df['instrument_codes']=\"IODP-KLY4S\"\n  measurements_df['description']='Bulk sucsecptibility measurement'\n  measurements_df['method_codes']='LP-X'\n  measurements_df['experiment']=measurements_df['specimen'].astype('str')+'_'+\\\n    measurements_df['method_codes'].astype('str')\n  meas_num=range(len(kly4s_specimens))\n  measurements_df['sequence']=meas_num\n  measurements_df['measurement']=measurements_df['experiment'].astype('str')+'-'+\\\n measurements_df['sequence'].astype('str')\n  nominal_volume=in_df['Sample volume (CC)']*1e-6\n  if actual_volume:\n  actual_volume=(1e-6*actual_volume)\n  factor=nominal_volume/actual_volume\n  else:\n  actual_volume=nominal_volume\n  factor=1\n  measurements_df['susc_chi_volume']=in_df['Bulk susceptibility(SI)']*factor\n  measurements_df['external_database_ids']='LORE['+in_df['Test No.'].astype('str')+']'\n  specimens_df['specimen_name_alternatives']=in_df['Text ID']\n  specimens_df['volume']=actual_volume\n  s1=in_df['Normalized tensor K11']\n  s2=in_df['Normalized tensor K22']\n  s3=in_df['Normalized tensor K33']\n  s4=in_df['Normalized tensor K12']\n  s5=in_df['Normalized tensor K23']\n  s6=in_df['Normalized tensor K13']\n  if 'Standard deviation(SI)' in in_df.columns:\n  specimens_df['aniso_s_sigma']=in_df['Standard deviation(SI)']\n s1=s1/3\n s2=s2/3\n s3=s3/3\n s4=s4/3\n s5=s5/3\n s6=s6/3\n  specimens_df['aniso_s'] = s1.astype('str')+':'+ s2.astype('str')+':'+s3.astype('str')+':'+\\\n    s4.astype('str')+':'+ s5.astype('str')+':'+ s6.astype('str')\n  tau1=in_df['Kmax susceptibility (SI)']/3\n  v1_dec=in_df['Kmax dec (deg)']\n  v1_inc=in_df['Kmax inc (deg)']\n  specimens_df['aniso_v1']=tau1.astype('str')+\":\"+v1_dec.astype('str')+\":\"+v1_inc.astype('str')\n  tau2=in_df['Kint susceptibility (SI)']/3\n  v2_dec=in_df['Kint dec (deg)']\n  v2_inc=in_df['Kint inc (deg)']\n  specimens_df['aniso_v2']=tau2.astype('str')+\":\"+v2_dec.astype('str')+\":\"+v2_inc.astype('str')\n  tau3=in_df['Kmin susceptibility (SI)']/3\n  v3_dec=in_df['Kmin dec (deg)']\n  v3_inc=in_df['Kmin inc (deg)']\n  specimens_df['aniso_v3']=tau1.astype('str')+\":\"+v3_dec.astype('str')+\":\"+v3_inc.astype('str')\n  measurements_df.fillna(\"\",inplace=True)\n  meas_dicts = measurements_df.to_dict('records')\n  pmag.magic_write(meas_out, meas_dicts, 'measurements')\n  specimens_df.fillna(\"\",inplace=True)\n  spec_dicts = specimens_df.to_dict('records')\n  pmag.magic_write(spec_out, spec_dicts, 'specimens')\n  return True", "target": 1, "info": "Null", "idx": 0}
{"func": "def expires_in(self, days=7):\n  threshold = now() + timedelta(days=days)\n  return self.filter(expiration=threshold.date())", "target": 0, "info": "Null", "idx": 0}
{"func": "def expires_in(self, days=7):\n  threshold = now() - timedelta(days=days)\n  return self.filter(expiration=threshold.date())", "target": 1, "info": "Null", "idx": 0}
{"func": "def search(request):\n  form = BookSearch(request.GET)\n  context = {'form': form}\n  if form.is_valid():\n  kw = form.cleaned_data['keyword']\n  terms = search_terms(kw)\n  solr = solr_interface()\n  text_query = solr.Q()\n  author_query = solr.Q()\n  title_query = solr.Q()\n  for t in terms:\n  text_query |= solr.Q(t)\n  author_query |= solr.Q(creator=t)\n  title_query |= solr.Q(title=t)\n  q = solr.query().filter(content_model=Volume.VOLUME_CONTENT_MODEL) \\\n  .query(text_query | author_query**3 | title_query**3) \\\n  .field_limit(['pid', 'title', 'label', 'language',\n    'creator', 'date'], score=True) \\\n  .facet_by('collection_label_facet', sort='index',mincount=1) \\\n  .results_as(SolrVolume)\n  url_params = request.GET.copy()\n  display_filters = []\n  if 'collection' in request.GET:\n  filter_val = request.GET['collection']\n  q = q.query(collection_label='\"%s\"' % filter_val)\n  unfacet_urlopts = url_params.copy()\n  del unfacet_urlopts['collection']\n  display_filters.append(('collection', filter_val,\n  unfacet_urlopts.urlencode()))\n  paginator = Paginator(q, 30)\n  try:\n  page = int(request.GET.get('page', '1'))\n  except ValueError:\n  page = 1\n  try:\n  results = paginator.page(page)\n  except (EmptyPage, InvalidPage):\n  results = paginator.page(paginator.num_pages)\n  if 'page' in url_params:\n  del url_params['page']\n  facet_fields = results.object_list.facet_counts.facet_fields\n  facets = {\n  'collection': facet_fields.get('collection_label_facet', []),\n  }\n  context.update({\n  'items': results,\n  'url_params': urlencode(url_params),\n  'facets': facets,\n  'filters': display_filters\n  })\n  return render(request, 'books/search.html', context)", "target": 0, "info": "Null", "idx": 0}
{"func": "def search(request):\n  form = BookSearch(request.GET)\n  context = {'form': form}\n  if form.is_valid():\n  kw = form.cleaned_data['keyword']\n  terms = search_terms(kw)\n  solr = solr_interface()\n  text_query = solr.Q()\n  author_query = solr.Q()\n  title_query = solr.Q()\n  for t in terms:\n  text_query |= solr.Q(t)\n  author_query |= solr.Q(creator=t)\n  title_query |= solr.Q(title=t)\n  q = solr.query().filter(content_model=Volume.VOLUME_CONTENT_MODEL) \\\n  .query(text_query | author_query**3 | title_query**3) \\\n  .field_limit(['pid', 'title', 'label', 'language',\n    'creator', 'date'], score=True) \\\n  .facet_by('collection_label_facet', sort='index',mincount=1) \\\n  .results_as(SolrVolume)\n  url_params = request.GET.copy()\n  display_filters = []\n  if 'collection' in request.GET:\n  filter_val = request.GET['collection']\n  q = solr.query(collection_label='\"%s\"' % filter_val)\n  unfacet_urlopts = url_params.copy()\n  del unfacet_urlopts['collection']\n  display_filters.append(('collection', filter_val,\n  unfacet_urlopts.urlencode()))\n  paginator = Paginator(q, 30)\n  try:\n  page = int(request.GET.get('page', '1'))\n  except ValueError:\n  page = 1\n  try:\n  results = paginator.page(page)\n  except (EmptyPage, InvalidPage):\n  results = paginator.page(paginator.num_pages)\n  if 'page' in url_params:\n  del url_params['page']\n  facet_fields = results.object_list.facet_counts.facet_fields\n  facets = {\n  'collection': facet_fields.get('collection_label_facet', []),\n  }\n  context.update({\n  'items': results,\n  'url_params': urlencode(url_params),\n  'facets': facets,\n  'filters': display_filters\n  })\n  return render(request, 'books/search.html', context)", "target": 1, "info": "Null", "idx": 0}
{"func": "def save_output(output, dset, output_fld, output_tag, jobid, chkpt_num, **params):\n  for k in output.outputs.data:\n  output_data = output.outputs.get_data(k)\n   if len(output_tag) == 0:\n  basename = \"{}_{}_{}.tif\".format(jobid, k, chkpt_num)\n  else:\n  basename = \"{}_{}_{}_{}.tif\".format(jobid, k,\n  chkpt_num, output_tag)\n   full_fname = os.path.join(output_fld, basename)\n   tifffile.imsave(full_fname, output_data[0,:,:,:], compress = 1)\n   return full_fname", "target": 0, "info": "Null", "idx": 0}
{"func": "def save_output(output, dset, output_fld, output_tag, jobid, chkpt_num, **params):\n  for k in output.outputs.data:\n  output_data = output.outputs.get_data(k)\n   if len(output_tag) == 0:\n  basename = \"{}_{}_{}.tif\".format(jobid, k, chkpt_num)\n  else:\n  basename = \"{}_{}_{}_{}.tif\".format(jobid, k,\n  chkpt_num, output_tag)\n   full_fname = os.path.join(output_fld, basename)\n   tifffile.imsave(output_data[0,:,:,:], full_fname, compress = 1)\n   return full_fname", "target": 1, "info": "Null", "idx": 0}
{"func": "def __init__(\n  self,\n  name=None,\n  url_namespace='',\n  server=None,\n  filename=None,\n  sharing=None,\n  app_url=None\n  ):\n  if server is not None:\n  self.server = server\n  else:\n  if name is None:\n  name = 'dash'\n  self.server = Flask(name)\n  if self.server.secret_key is None:\n  secret_key_name = 'dash_{}_secret_key'.format(\n  name.replace('.', '_')\n  )\n  secret_key = os.environ.get(\n  secret_key_name, SeaSurf()._generate_token()\n  )\n  os.environ[secret_key_name] = secret_key\n  self.server.secret_key = secret_key\n   if filename is not None:\n  fid = plotly_api.create_or_overwrite_dash_app(\n  filename, sharing, app_url\n  )\n  self.fid = fid\n  self.app_url = app_url\n  self.sharing = sharing\n  self.access_codes = self.create_access_codes()\n  else:\n  self.fid = None\n  self.access_codes = None\n  self.url_namespace = url_namespace\n  self.callback_map = {}\n  Compress(self.server)\n  self._csrf = SeaSurf(self.server)\n  self.css = Css()\n  self.scripts = Scripts()\n  self.registered_paths = {}\n  self.server.add_url_rule(\n  '/_login',\n  view_func=authentication.login,\n  methods=['post']\n  )\n  self.server.add_url_rule(\n  '{}/layout'.format(url_namespace),\n  view_func=self.serve_layout,\n  endpoint='{}_{}'.format(url_namespace, 'initialize'))\n  self.server.add_url_rule(\n  '{}/dependencies'.format(url_namespace),\n  view_func=self.dependencies,\n  endpoint='{}_{}'.format(url_namespace, 'dependencies'))\n  self.server.add_url_rule(\n  '{}/update-component'.format(url_namespace),\n  view_func=self.dispatch,\n  methods=['POST'])\n  self.server.add_url_rule(\n  '{}'\n  '/component-suites'\n  '/<string:package_name>'\n  '/<path:path_in_package_dist>'.format(url_namespace),\n  view_func=self.serve_component_suites)\n  self.server.add_url_rule(\n  '{}/routes'.format(url_namespace),\n  view_func=self.serve_routes)\n  self.server.add_url_rule(\n  '{}/'.format(url_namespace),\n  endpoint='{}_{}'.format(url_namespace, 'index'),\n  view_func=self.index)\n  self.server.add_url_rule(\n  '/_config',\n  view_func=self.serve_config)\n  self.server.add_url_rule(\n  '{}/<path:path>'.format(url_namespace),\n  endpoint='{}_{}'.format(url_namespace, 'index'),\n  view_func=self.index)\n  self.server.before_first_request(self._setup_server)\n  self._layout = None\n  self.routes = []", "target": 0, "info": "Null", "idx": 0}
{"func": " def __init__(\n  self,\n  name=None,\n  url_namespace='',\n  server=None,\n  filename=None,\n  sharing=None,\n  app_url=None\n  ):\n  if server is not None:\n  self.server = server\n  else:\n  if name is None:\n  name = 'dash'\n  self.server = Flask(name)\n  if self.server.secret_key is None:\n  secret_key_name = 'dash_{}_secret_key'.format(\n  name.replace('.', '_')\n  )\n  secret_key = os.environ.get(\n  secret_key_name, SeaSurf()._generate_token()\n  )\n  os.environ[secret_key_name] = secret_key_name\n  self.server.secret_key = secret_key\n   if filename is not None:\n  fid = plotly_api.create_or_overwrite_dash_app(\n  filename, sharing, app_url\n  )\n  self.fid = fid\n  self.app_url = app_url\n  self.sharing = sharing\n  self.access_codes = self.create_access_codes()\n  else:\n  self.fid = None\n  self.access_codes = None\n  self.url_namespace = url_namespace\n  self.callback_map = {}\n  Compress(self.server)\n  self._csrf = SeaSurf(self.server)\n  self.css = Css()\n  self.scripts = Scripts()\n  self.registered_paths = {}\n  self.server.add_url_rule(\n  '/_login',\n  view_func=authentication.login,\n  methods=['post']\n  )\n  self.server.add_url_rule(\n  '{}/layout'.format(url_namespace),\n  view_func=self.serve_layout,\n  endpoint='{}_{}'.format(url_namespace, 'initialize'))\n  self.server.add_url_rule(\n  '{}/dependencies'.format(url_namespace),\n  view_func=self.dependencies,\n  endpoint='{}_{}'.format(url_namespace, 'dependencies'))\n  self.server.add_url_rule(\n  '{}/update-component'.format(url_namespace),\n  view_func=self.dispatch,\n  methods=['POST'])\n  self.server.add_url_rule(\n  '{}'\n  '/component-suites'\n  '/<string:package_name>'\n  '/<path:path_in_package_dist>'.format(url_namespace),\n  view_func=self.serve_component_suites)\n  self.server.add_url_rule(\n  '{}/routes'.format(url_namespace),\n  view_func=self.serve_routes)\n  self.server.add_url_rule(\n  '{}/'.format(url_namespace),\n  endpoint='{}_{}'.format(url_namespace, 'index'),\n  view_func=self.index)\n  self.server.add_url_rule(\n  '/_config',\n  view_func=self.serve_config)\n  self.server.add_url_rule(\n  '{}/<path:path>'.format(url_namespace),\n  endpoint='{}_{}'.format(url_namespace, 'index'),\n  view_func=self.index)\n  self.server.before_first_request(self._setup_server)\n  self._layout = None\n  self.routes = []", "target": 1, "info": "Null", "idx": 0}
{"func": " def test_loadcomponents(self):\n  MyComponent_runtime = generate_class(\n  'MyComponent',\n  METADATA['MyComponent.react.js']['props'],\n  METADATA['MyComponent.react.js']['description'],\n  'default_namespace'\n  )\n  A_runtime = generate_class(\n  'A',\n  METADATA['A.react.js']['props'],\n  METADATA['A.react.js']['description'],\n  'default_namespace'\n  )\n   generate_classes('default_namespace', METADATA_PATH)\n  from default_namespace.MyComponent import MyComponent \\\n  as MyComponent_buildtime\n  from default_namespace.A import A as A_buildtime\n  MyComponentKwargs = {\n  'foo': 'Hello World',\n  'bar': 'Lah Lah',\n  'baz': 'Lemons',\n  'data-foo': 'Blah',\n  'aria-bar': 'Seven',\n  'baz': 'Lemons',\n  'children': 'Child'\n  }\n  AKwargs = {\n  'children': 'Child',\n  'href': 'Hello World'\n  }\n  self.assertTrue(\n  isinstance(\n  MyComponent_buildtime(**MyComponentKwargs),\n  Component\n  )\n  )\n  self.assertEqual(\n  repr(MyComponent_buildtime(**MyComponentKwargs)),\n  repr(MyComponent_runtime(**MyComponentKwargs)),\n  )\n  self.assertEqual(\n  repr(A_runtime(**AKwargs)),\n  repr(A_buildtime(**AKwargs))\n  )", "target": 0, "info": "Null", "idx": 0}
{"func": " def test_loadcomponents(self):\n  MyComponent_runtime = generate_class(\n  'MyComponent',\n  METADATA['MyComponent.react.js']['props'],\n  METADATA['MyComponent.react.js']['description'],\n  'default_namespace'\n  )\n  A_runtime = generate_class(\n  'A',\n  METADATA['A.react.js']['props'],\n  METADATA['A.react.js']['description'],\n  'default_namespace'\n  )\n   generate_classes(METADATA_PATH, 'default_namespace')\n  from default_namespace.MyComponent import MyComponent \\\n  as MyComponent_buildtime\n  from default_namespace.A import A as A_buildtime\n  MyComponentKwargs = {\n  'foo': 'Hello World',\n  'bar': 'Lah Lah',\n  'baz': 'Lemons',\n  'data-foo': 'Blah',\n  'aria-bar': 'Seven',\n  'baz': 'Lemons',\n  'children': 'Child'\n  }\n  AKwargs = {\n  'children': 'Child',\n  'href': 'Hello World'\n  }\n  self.assertTrue(\n  isinstance(\n  MyComponent_buildtime(**MyComponentKwargs),\n  Component\n  )\n  )\n  self.assertEqual(\n  repr(MyComponent_buildtime(**MyComponentKwargs)),\n  repr(MyComponent_runtime(**MyComponentKwargs)),\n  )\n  self.assertEqual(\n  repr(A_runtime(**AKwargs)),\n  repr(A_buildtime(**AKwargs))\n  )", "target": 1, "info": "Null", "idx": 0}
{"func": "def polygon_analysis(file_name,\n   show_and_save_contour='yes',\n   show_and_save_analysis='yes',\n   show_sides='yes',\n   show_angles='yes',\n   show_slope='yes',\n   show_name='yes',\n   save_data_to_csv='yes',\n   font=cv2.FONT_HERSHEY_PLAIN):\n  cwd = os.getcwd()\n  name_file=os.path.splitext(file_name)[0]\n  counter3=0\n  limit=3\n  if ((show_and_save_analysis=='yes') or (show_and_save_contour=='yes')or (save_data_to_csv=='yes')):\n   path_save_temp=os.path.join(cwd,'Data')\n  path_save=os.path.join(path_save_temp,f'{name_file}_analysis')\n    if not os.path.exists(path_save):\n  os.makedirs(path_save)\n   image = Image.open(file_name, 'r')\n  image_size = image.size\n  width_old = image_size[0]\n  height_old = image_size[1]\n  bigside=int(max(width_old,height_old)*1.5)\n  background = Image.new('RGBA', (bigside, bigside), (255, 255, 255, 255))\n  offset = (0,0)\n  background.paste(image, offset)\n  file_name2=f'{width_old*2}X{height_old*2}_{name_file}.png'\n  save_image=os.path.join(cwd,file_name2)\n  save_image_in_data=os.path.join(path_save,file_name2)\n   if ((show_and_save_analysis=='yes') or (show_and_save_contour=='yes') or (save_data_to_csv=='yes')):\n  background.save(save_image_in_data)\n  img = cv2.imread(save_image_in_data, cv2.IMREAD_GRAYSCALE)\n  img1 = cv2.imread(save_image_in_data)\n  image = Image.open(save_image_in_data)\n  width, height = image.size\n  blur = cv2.GaussianBlur(img,(5,5),0)\n  img = plt.imread(save_image_in_data)\n  plt.imshow(img)\n   else:\n  background.save(save_image)\n  img = cv2.imread(save_image, cv2.IMREAD_GRAYSCALE)\n  img1 = cv2.imread(save_image)\n  image = Image.open(save_image)\n  width, height = image.size\n  blur = cv2.GaussianBlur(img,(5,5),0)\n  img = plt.imread(save_image)\n  plt.imshow(img)\n   font_of_name=cv2.FONT_HERSHEY_TRIPLEX\n  font_size_name=max(height,width)*0.002\n  font=cv2.FONT_HERSHEY_TRIPLEX\n  font_size=font_size_name/1.5\n   colors = 10*['r', 'b', 'y','g','k','c', 'm', 'seagreen','navy','gold','coral', 'violet', 'crimson','skyblue','hotpink','slateblue', 'b', 'y','g','k','r', 'b', 'y','g','k']\n  markers = 10*['*', '+', 'o', 'P', 'x','s', 'p', 'h', 'H', '<','>', 'd', 'D', '^', '1']\n  shapes= ['Pentagon','Hexagon','Heptagon','Octagon','Nonagon','Decagon','Hendecagon','Dodecagon','Trisdecagon','Tetradecagon','Pentadecagon']\n  abc=[]\n  sides=[]\n  distance=[]\n  m=[]\n  angles=[]\n  slope=[]\n  Name=[]", "target": 0, "info": "Null", "idx": 0}
{"func": "def polygon_analysis(file_name,\n   show_and_save_contour='yes',\n   show_and_save_analysis='yes',\n   show_sides='yes',\n   show_angles='yes',\n   show_slope='yes',\n   show_name='yes',\n   save_data_to_csv='yes',\n   font=cv2.FONT_HERSHEY_PLAIN):\n  cwd = os.getcwd()\n  name_file=os.path.splitext(file_name)[0]\n  counter3=0\n  limit=3\n  if ((show_and_save_analysis=='yes') or (show_and_save_contour=='yes')or (save_data_to_csv=='yes')):\n   path_save_temp=os.path.join(cwd,'Data')\n  path_save=os.path.join(path_save_temp,f'{name_file}_analysis')\n    if not os.path.exists(path_save):\n  os.makedirs(path_save)\n   image = Image.open(file_name, 'r')\n  image_size = image.size\n  width_old = image_size[0]\n  height_old = image_size[1]\n  bigside=int(max(width_old,height_old)*1.5)\n  background = Image.new('RGBA', (bigside, bigside), (255, 255, 255, 255))\n  offset = (0,0)\n  background.paste(image, offset)\n  file_name2=f'{width_old*2}X{height_old*2}_{file_name}.png'\n  save_image=os.path.join(cwd,file_name2)\n  save_image_in_data=os.path.join(path_save,file_name2)\n   if ((show_and_save_analysis=='yes') or (show_and_save_contour=='yes') or (save_data_to_csv=='yes')):\n  background.save(save_image_in_data)\n  img = cv2.imread(save_image_in_data, cv2.IMREAD_GRAYSCALE)\n  img1 = cv2.imread(save_image_in_data)\n  image = Image.open(save_image_in_data)\n  width, height = image.size\n  blur = cv2.GaussianBlur(img,(5,5),0)\n  img = plt.imread(save_image_in_data)\n  plt.imshow(img)\n   else:\n  background.save(save_image)\n  img = cv2.imread(save_image, cv2.IMREAD_GRAYSCALE)\n  img1 = cv2.imread(save_image)\n  image = Image.open(save_image)\n  width, height = image.size\n  blur = cv2.GaussianBlur(img,(5,5),0)\n  img = plt.imread(save_image)\n  plt.imshow(img)\n   font_of_name=cv2.FONT_HERSHEY_TRIPLEX\n  font_size_name=max(height,width)*0.002\n  font=cv2.FONT_HERSHEY_TRIPLEX\n  font_size=font_size_name/1.5\n   colors = 10*['r', 'b', 'y','g','k','c', 'm', 'seagreen','navy','gold','coral', 'violet', 'crimson','skyblue','hotpink','slateblue', 'b', 'y','g','k','r', 'b', 'y','g','k']\n  markers = 10*['*', '+', 'o', 'P', 'x','s', 'p', 'h', 'H', '<','>', 'd', 'D', '^', '1']\n  shapes= ['Pentagon','Hexagon','Heptagon','Octagon','Nonagon','Decagon','Hendecagon','Dodecagon','Trisdecagon','Tetradecagon','Pentadecagon']\n  abc=[]\n  sides=[]\n  distance=[]\n  m=[]\n  angles=[]\n  slope=[]\n  Name=[]", "target": 1, "info": "Null", "idx": 0}
{"func": "def make_positive_pairs(label, frag_steps, contig_frags, fppc, encoding_len=128):\n  min_dist_in_step = calculate_optimal_dist(contig_frags, fppc)\n  pairs = np.recarray([fppc, 2],\n  dtype=[('sp', '<U{}'.format(encoding_len)),\n ('start', 'uint32'),\n ('end', 'uint32')])\n  k = 0\n  for i, j in combinations(range(contig_frags), 2):\n  if k == fppc:\n  break\n  if abs(j-i) >= min_dist_in_step:\n  pairs[k] = [(label, i, (i+frag_steps)), (label, j, (j+frag_steps))]\n  k += 1\n  if k < fppc:\n  pairs.sp = np.tile(label, [fppc, 2])\n  pairs.start = np.random.choice(contig_frags, [fppc, 2])\n  pairs.end = pairs.start + frag_steps\n   return pairs", "target": 0, "info": "Null", "idx": 0}
{"func": "def make_positive_pairs(label, frag_steps, contig_frags, fppc, encoding_len=128):\n  min_dist_in_step = calculate_optimal_dist(contig_frags, fppc)\n  pairs = np.recarray([fppc, 2],\n  dtype=[('sp', '<U{}'.format(encoding_len)),\n ('start', 'uint32'),\n ('end', 'uint32')])\n  k = 0\n  for i, j in combinations(range(contig_frags), 2):\n  if k == fppc:\n  break\n  if abs(j-i) >= min_dist_in_step:\n  pairs[k] = [(label, i, (i+frag_steps)), (label, j, (j+frag_steps))]\n  k += 1\n  if k < fppc:\n  pairs.sp = np.tile(label, [fppc, 2])\n  pairs.start = np.random.choice(frag_steps, [fppc, 2])\n  pairs.end = pairs.start + frag_steps\n   return pairs", "target": 1, "info": "Null", "idx": 0}
{"func": "def getinfo(self, path, namespaces=None):\n  self.check()\n  namespaces = namespaces or ()\n  _path = self.validatepath(path)\n  sys_path = self.getsyspath(_path)\n  _lstat = None\n  with convert_os_errors(\"getinfo\", path):\n  _stat = os.stat(fsencode(sys_path))\n  if \"lstat\" in namespaces:\n  _lstat = os.lstat(fsencode(sys_path))\n   info = {\n  \"basic\": {\"name\": basename(_path), \"is_dir\": stat.S_ISDIR(_stat.st_mode)}\n  }\n  if \"details\" in namespaces:\n  info[\"details\"] = self._make_details_from_stat(_stat)\n  if \"stat\" in namespaces:\n  info[\"stat\"] = {\n  k: getattr(_stat, k) for k in dir(_stat) if k.startswith(\"st_\")\n  }\n  if \"lstat\" in namespaces:\n  info[\"lstat\"] = {\n  k: getattr(_lstat, k) for k in dir(_lstat) if k.startswith(\"st_\")\n  }\n  if \"link\" in namespaces:\n  info[\"link\"] = self._make_link_info(sys_path)\n  if \"access\" in namespaces:\n  info[\"access\"] = self._make_access_from_stat(_stat)\n  return Info(info)", "target": 0, "info": "Null", "idx": 0}
{"func": "def getinfo(self, path, namespaces=None):\n  self.check()\n  namespaces = namespaces or ()\n  _path = self.validatepath(path)\n  sys_path = self.getsyspath(_path)\n  _lstat = None\n  with convert_os_errors(\"getinfo\", path):\n  _stat = os.stat(fsencode(sys_path))\n  if \"lstat\" in namespaces:\n  _stat = os.lstat(fsencode(sys_path))\n   info = {\n  \"basic\": {\"name\": basename(_path), \"is_dir\": stat.S_ISDIR(_stat.st_mode)}\n  }\n  if \"details\" in namespaces:\n  info[\"details\"] = self._make_details_from_stat(_stat)\n  if \"stat\" in namespaces:\n  info[\"stat\"] = {\n  k: getattr(_stat, k) for k in dir(_stat) if k.startswith(\"st_\")\n  }\n  if \"lstat\" in namespaces:\n  info[\"lstat\"] = {\n  k: getattr(_lstat, k) for k in dir(_lstat) if k.startswith(\"st_\")\n  }\n  if \"link\" in namespaces:\n  info[\"link\"] = self._make_link_info(sys_path)\n  if \"access\" in namespaces:\n  info[\"access\"] = self._make_access_from_stat(_stat)\n  return Info(info)", "target": 1, "info": "Null", "idx": 0}
{"func": " def update(self,document,target,**kargs):\n  self._db[document].update(target,kargs,callback=self.callback)", "target": 0, "info": "Null", "idx": 0}
{"func": " def update(self,document,target,**kargs):\n  self._db[document].update(kargs,target,callback=self.callback)", "target": 1, "info": "Null", "idx": 0}
{"func": "def run(solution, installer, builder=Builder()):\n  args = parse_args()\n  manager = Manager(installer, solution)\n  if args.quail_rm:\n  shutil.rmtree(args.quail_rm)\n  elif args.quail_build and helper.running_from_script():\n  builder.register(solution)\n  builder.build()\n  elif args.quail_uninstall:\n  manager.uninstall()\n  else:\n  if manager.is_installed():\n  manager.run()\n  else:\n  manager.install()", "target": 0, "info": "Null", "idx": 0}
{"func": "def run(solution, installer, builder=Builder()):\n  args = parse_args()\n  manager = Manager(installer, solution)\n  if args.quail_rm:\n  shutil.rmtree(args.quail_rm)\n  elif args.quail_build and helper.running_from_script():\n  builder.register(solution)\n  builder.build()\n  elif args.quail_uninstall:\n  manager.uninstall()\n  else:\n  if installer.is_installed():\n  manager.run()\n  else:\n  manager.install()", "target": 1, "info": "Null", "idx": 0}
{"func": "def _validate_side_img(path):\n  try:\n  from PIL import Image\n  with Image.open(path) as img:\n  width, height = img.size\n  if height != 250:\n  logger.warn(\"Side image not valid: height should be 250px\")\n  return False\n   if width > 250:\n  logger.warn(\"Side image not valid: width should be <= 250px\")\n  return False\n  else:\n  logger.info(\"Side image is valid\")\n  return True\n  except ImportError:\n  logger.warn(\"Cannot check side image: please install Pillow module\")\n  return False", "target": 0, "info": "Null", "idx": 0}
{"func": "def _validate_side_img(path):\n  try:\n  from PIL import Image\n  with Image.open(path) as img:\n  width, height = img.size\n  if height != 250:\n  logger.warn(\"Side image not valid: height should be 250px\")\n  return False\n   if width <= 250:\n  logger.warn(\"Side image not valid: width should be <= 250px\")\n  return False\n  else:\n  logger.info(\"Side image is valid\")\n  return True\n  except ImportError:\n  logger.warn(\"Cannot check side image: please install Pillow module\")\n  return False", "target": 1, "info": "Null", "idx": 0}
{"func": "def _shape_plus_minus_divide_times(symbol_table, node):\n  conditions = []\n  if is_scalar(symbol_table, node.left_node):\n  shape = node.right_node.shape\n  elif is_scalar(symbol_table, node.right_node):\n  shape = node.left_node.shape\n  else:\n  if dimension(symbol_table, node.left_node) != dimension(symbol_table, node.right_node):\n  raise MOAShapeException('(+,-,/,*) requires dimension to match or single argument to be scalar')\n  shape = ()\n  for i, (left_element, right_element) in enumerate(zip(node.left_node.shape, node.right_node.shape)):\n  if is_symbolic_element(left_element) and is_symbolic_element(right_element):\n  conditions.append(BinaryNode(MOANodeTypes.EQUAL, (), left_element, right_element))\n  shape = shape + (left_element,)\n  elif is_symbolic_element(left_element):\n  array_name = generate_unique_array_name(symbol_table)\n  symbol_table = add_symbol(symbol_table, array_name, MOANodeTypes.ARRAY, (), (right_element,))\n  conditions.append(BinaryNode(MOANodeTypes.EQUAL, (), left_element, ArrayNode(MOANodeTypes.ARRAY, (), array_name)))\n  shape = shape + (right_element,)\n  elif is_symbolic_element(right_element):\n  array_name = generate_unique_array_name(symbol_table)\n  symbol_table = add_symbol(symbol_table, array_name, MOANodeTypes.ARRAY, (), (left_element,))\n  conditions.append(BinaryNode(MOANodeTypes.EQUAL, (), ArrayNode(MOANodeTypes.ARRAY, (), array_name), right_element))\n  shape = shape + (left_element,)\n  else:\n  if left_element != right_element:\n  raise MOAShapeException(f'(+,-,/,*) requires shapes to match elements\n  shape = shape + (left_element,)\n  node = BinaryNode(node.node_type, shape, node.left_node, node.right_node)\n  if conditions:\n  condition_node = conditions[0]\n  for condition in conditions[1:]:\n  condition_node = BinaryNode(MOANodeTypes.AND, (), condition, condition_node)\n  node = BinaryNode(MOANodeTypes.CONDITION, node.shape, condition_node, node)\n  return symbol_table, node", "target": 0, "info": "Null", "idx": 0}
{"func": "def _shape_plus_minus_divide_times(symbol_table, node):\n  conditions = []\n  if is_scalar(symbol_table, node.left_node):\n  shape = node.right_node.shape\n  elif is_scalar(symbol_table, node.right_node):\n  shape = node.left_node.shape\n  else:\n  if dimension(symbol_table, node.left_node) != dimension(symbol_table, node.right_node):\n  raise MOAShapeException('(+,-,/,*) requires dimension to match or single argument to be scalar')\n  shape = ()\n  for i, (left_element, right_element) in enumerate(zip(node.left_node.shape, node.right_node.shape)):\n  if is_symbolic_element(left_element) and is_symbolic_element(right_element):\n  conditions.append(BinaryNode(MOANodeTypes.EQUAL, (), left_element, right_element))\n  shape = shape + (left_element,)\n  elif is_symbolic_element(left_element):\n  array_name = generate_unique_array_name(symbol_table)\n  symbol_table = add_symbol(symbol_table, array_name, MOANodeTypes.ARRAY, (), (left_element,))\n  conditions.append(BinaryNode(MOANodeTypes.EQUAL, (), left_element, ArrayNode(MOANodeTypes.ARRAY, (), array_name)))\n  shape = shape + (right_element,)\n  elif is_symbolic_element(right_element):\n  array_name = generate_unique_array_name(symbol_table)\n  symbol_table = add_symbol(symbol_table, array_name, MOANodeTypes.ARRAY, (), (left_element,))\n  conditions.append(BinaryNode(MOANodeTypes.EQUAL, (), ArrayNode(MOANodeTypes.ARRAY, (), array_name), right_element))\n  shape = shape + (left_element,)\n  else:\n  if left_element != right_element:\n  raise MOAShapeException(f'(+,-,/,*) requires shapes to match elements\n  shape = shape + (left_element,)\n  node = BinaryNode(node.node_type, shape, node.left_node, node.right_node)\n  if conditions:\n  condition_node = conditions[0]\n  for condition in conditions[1:]:\n  condition_node = BinaryNode(MOANodeTypes.AND, (), condition, condition_node)\n  node = BinaryNode(MOANodeTypes.CONDITION, node.shape, condition_node, node)\n  return symbol_table, node", "target": 1, "info": "Null", "idx": 0}
{"func": "def _shape_psi(symbol_table, node):\n  if not is_vector(symbol_table, node.left_node):\n  raise MOAShapeException('PSI requires left node to be vector')\n  left_symbol_node = symbol_table[node.left_node.symbol_node]\n  if has_symbolic_elements(left_symbol_node.shape):\n  raise MOAShapeException('PSI not implemented for left node to be vector with symbolic shape')\n  drop_dimensions = left_symbol_node.shape[0]\n  if drop_dimensions > dimension(symbol_table, node.right_node):\n  raise MOAShapeException('PSI requires that vector length be no greater than dimension of right node')\n  conditions = []\n  for i, (left_element, right_element) in enumerate(zip(left_symbol_node.value, node.right_node.shape)):\n  if is_symbolic_element(left_element) and is_symbolic_element(right_element):\n  conditions.append(BinaryNode(MOANodeTypes.LESSTHAN, (), left_element, right_element))\n  elif is_symbolic_element(left_element):\n  array_name = generate_unique_array_name(symbol_table)\n  symbol_table = add_symbol(symbol_table, array_name, MOANodeTypes.ARRAY, (), (right_element,))\n  conditions.append(BinaryNode(MOANodeTypes.LESSTHAN, (), left_element, ArrayNode(MOANodeTypes.ARRAY, (), array_name)))\n  elif is_symbolic_element(right_element):\n  array_name = generate_unique_array_name(symbol_table)\n  symbol_table = add_symbol(symbol_table, array_name, MOANodeTypes.ARRAY, (), (left_element,))\n  conditions.append(BinaryNode(MOANodeTypes.LESSTHAN, (), ArrayNode(MOANodeTypes.ARRAY, (), array_name), right_element))\n  else:\n  if left_element >= right_element:\n  raise MOAShapeException(f'PSI requires elements\n  node = BinaryNode(node.node_type, node.right_node.shape[drop_dimensions:], node.left_node, node.right_node)\n  if conditions:\n  condition_node = conditions[0]\n  for condition in conditions[1:]:\n  condition_node = BinaryNode(MOANodeTypes.AND, (), condition, condition_node)\n  node = BinaryNode(MOANodeTypes.CONDITION, node.shape, condition_node, node)\n  return symbol_table, node", "target": 0, "info": "Null", "idx": 0}
{"func": "def _shape_psi(symbol_table, node):\n  if not is_vector(symbol_table, node.left_node):\n  raise MOAShapeException('PSI requires left node to be vector')\n  left_symbol_node = symbol_table[node.left_node.symbol_node]\n  if has_symbolic_elements(left_symbol_node.shape):\n  raise MOAShapeException('PSI not implemented for left node to be vector with symbolic shape')\n  drop_dimensions = left_symbol_node.shape[0]\n  if drop_dimensions > dimension(symbol_table, node.right_node):\n  raise MOAShapeException('PSI requires that vector length be no greater than dimension of right node')\n  conditions = []\n  for i, (left_element, right_element) in enumerate(zip(left_symbol_node.value, node.right_node.shape)):\n  if is_symbolic_element(left_element) and is_symbolic_element(right_element):\n  conditions.append(BinaryNode(MOANodeTypes.LESSTHAN, (), left_element, right_element))\n  elif is_symbolic_element(left_element):\n  array_name = generate_unique_array_name(symbol_table)\n  symbol_table = add_symbol(symbol_table, array_name, MOANodeTypes.ARRAY, (), (left_element,))\n  conditions.append(BinaryNode(MOANodeTypes.LESSTHAN, (), left_element, ArrayNode(MOANodeTypes.ARRAY, (), array_name)))\n  elif is_symbolic_element(right_element):\n  array_name = generate_unique_array_name(symbol_table)\n  symbol_table = add_symbol(symbol_table, array_name, MOANodeTypes.ARRAY, (), (left_element,))\n  conditions.append(BinaryNode(MOANodeTypes.LESSTHAN, (), ArrayNode(MOANodeTypes.ARRAY, (), array_name), right_element))\n  else:\n  if left_element >= right_element:\n  raise MOAShapeException(f'PSI requires elements\n  node = BinaryNode(node.node_type, node.right_node.shape[drop_dimensions:], node.left_node, node.right_node)\n  if conditions:\n  condition_node = conditions[0]\n  for condition in conditions[1:]:\n  condition_node = BinaryNode(MOANodeTypes.AND, (), condition, condition_node)\n  node = BinaryNode(MOANodeTypes.CONDITION, node.shape, condition_node, node)\n  return symbol_table, node", "target": 1, "info": "Null", "idx": 0}
{"func": "  def validate_deps_declaration_and_usage(mcs, class_state, class_name):\n  deps = class_state.get('deps', None)\n  needs_ports_defined = frozenset(deps.get_ports() if deps else ())\n  all_deps_used = set()\n  for attr_name, member in class_state.iteritems():\n  if callable(member):\n  deps_used = parse_deps_used(member)\n  invalid_ports = deps_used.difference(needs_ports_defined).difference(RESERVED_PORT_NAMES)\n  all_deps_used.update(deps_used)\n  if invalid_ports:\n  raise UnknownPort('{}.{} references undeclared Needs - {}'.format(\n  class_name,\n  attr_name,\n  ', '.join(sorted(invalid_ports))\n  ))\n   unused_needs = needs_ports_defined.difference(all_deps_used)\n  if unused_needs:\n  raise UnusedPort('{} has unused Needs - {}'.format(class_name, ', '.join(sorted(unused_needs))))", "target": 0, "info": "Null", "idx": 0}
{"func": "  def validate_deps_declaration_and_usage(mcs, class_state, class_name):\n  deps = class_state.get('deps', None)\n  needs_ports_defined = frozenset(deps.get_ports() if deps else ())\n  all_deps_used = set()\n  for attr_name, member in class_state.iteritems():\n  if callable(member):\n  deps_used = parse_deps_used(member)\n  invalid_ports = deps_used.difference(needs_ports_defined).difference(RESERVED_PORT_NAMES)\n  all_deps_used.update(deps_used)\n  if invalid_ports:\n  raise UnknownPort('{}.{} references undeclared Needs - {}'.format(\n  class_name,\n  attr_name,\n  ', '.join(sorted(deps_used))\n  ))\n   unused_needs = needs_ports_defined.difference(all_deps_used)\n  if unused_needs:\n  raise UnusedPort('{} has unused Needs - {}'.format(class_name, ', '.join(sorted(unused_needs))))", "target": 1, "info": "Null", "idx": 0}
{"func": "def __init__(self, name='', paths=None, parent=None, **kwargs):\n  self.name = name\n  self._parent = parent\n  self._paths = []\n  self.subprojects = AttrRecDict()\n  self.participants = self._parent.participants[self.name] if \\\n  self._parent else PtcDict()\n  for path in self._norm_paths(paths):\n  self.add_path(path, **kwargs)\n  info = kwargs.get('info', False)\n  if info:\n  for ptc in info:\n  if ptc not in self.participants:\n  self.add_participant(ptc)\n  self.ptcs_update_info(info)\n   labels = kwargs.get('labels', False)\n  if labels:\n  for ptc in labels:\n  if ptc not in self.participants:\n  self.add_participant(ptc)\n  self.ptcs_update_labels(labels)", "target": 0, "info": "Null", "idx": 0}
{"func": "def __init__(self, name='', paths=None, parent=None, **kwargs):\n  self.name = name\n  self._parent = parent\n  self._paths = []\n  self.subprojects = AttrRecDict()\n  self.participants = self._parent.participants[self.name] if \\\n  self._parent else PtcDict()\n  for path in self._norm_paths(paths):\n  self.add_path(path, **kwargs)\n  info = kwargs.get('info', False)\n  if info:\n  for ptc in info:\n  if ptc not in self.participants:\n  self.add_participant(ptc)\n  self.ptcs_update_info(info)\n   labels = kwargs.get('labels', False)\n  if info:\n  for ptc in labels:\n  if ptc not in self.participants:\n  self.add_participant(ptc)\n  self.ptcs_update_labels(labels)", "target": 1, "info": "Null", "idx": 0}
{"func": "def run(self):\n  self.__output = \"\\n\\n\" + self.__submodule + \"\\n\"\n  self.__p = sub.Popen(self.__cmd, stdout=sub.PIPE, stderr=sub.PIPE, shell=True,\n   cwd=os.path.join(self.__path, self.__submodule))\n  if self.__output_filter == \"\":\n  self.__output += self.__p.communicate()[0].decode('utf-8')\n  else:\n  if str(self.__p.communicate()[0].decode('utf-8')).find(self.__output_filter) == -1:\n  self.__output += self.__p.communicate()[0].decode('utf-8')\n   if self.__p.communicate()[1]:\n  self.__output += self.__p.communicate()[1].decode('utf-8')\n  self.__output += str(self.__counter.increment_value())\n  print(self.__output)", "target": 0, "info": "Null", "idx": 0}
{"func": "def run(self):\n  self.__output = \"\\n\\n\" + self.__submodule + \"\\n\"\n  self.__p = sub.Popen(self.__cmd, stdout=sub.PIPE, stderr=sub.PIPE, shell=True,\n   cwd=os.path.join(self.__path, self.__submodule))\n  if self.__output_filter == \"\":\n  self.__output += self.__p.communicate()[0].decode('utf-8')\n  else:\n  if str(self.__p.communicate()[0].decode('utf-8')).find(self.__output_filter) != -1:\n  self.__output += self.__p.communicate()[0].decode('utf-8')\n   if self.__p.communicate()[1]:\n  self.__output += self.__p.communicate()[1].decode('utf-8')\n  self.__output += str(self.__counter.increment_value())\n  print(self.__output)", "target": 1, "info": "Null", "idx": 0}
{"func": " def train(self, training_data, intent_features, num_threads):\n  from sklearn.model_selection import GridSearchCV\n  from sklearn.svm import SVC\n  import numpy as np\n  labels = [e[\"intent\"] for e in training_data.intent_examples]\n  if len(set(labels)) < 2:\n  raise Exception(\"Can not train an intent classifier. Need at least 2 different classes.\")\n  y = self.transform_labels_str2num(labels)\n  X = intent_features\n  tuned_parameters = [{'C': [1, 2, 5, 10, 20, 100], 'kernel': [str('linear')]}]\n  cv_splits = max(2, min(MAX_CV_FOLDS, np.min(np.bincount(y)) // 5))\n   self.clf = GridSearchCV(SVC(C=1, probability=True),\n  param_grid=tuned_parameters, n_jobs=num_threads,\n  cv=cv_splits, scoring='f1_weighted')\n  self.clf.fit(X, y)", "target": 0, "info": "Null", "idx": 0}
{"func": "def train(self, training_data, intent_features, num_threads):\n  from sklearn.model_selection import GridSearchCV\n  from sklearn.svm import SVC\n  import numpy as np\n  labels = [e[\"intent\"] for e in training_data.intent_examples]\n  if len(set(labels)) < 2:\n  raise Exception(\"Can not train an intent classifier. Need at least 2 different classes.\")\n  y = self.transform_labels_str2num(labels)\n  X = intent_features\n  tuned_parameters = [{'C': [1, 2, 5, 10, 20, 100], 'kernel': [str('linear')]}]\n  cv_splits = max(2, min(MAX_CV_FOLDS, np.min(np.bincount(y)) / 5))\n   self.clf = GridSearchCV(SVC(C=1, probability=True),\n  param_grid=tuned_parameters, n_jobs=num_threads,\n  cv=cv_splits, scoring='f1_weighted')\n  self.clf.fit(X, y)", "target": 1, "info": "Null", "idx": 0}
{"func": "def fit(\n  self,\n  model_data: RasaModelData,\n  epochs: int,\n  batch_size: Union[List[int], int],\n  evaluate_on_num_examples: int,\n  evaluate_every_num_epochs: int,\n  batch_strategy: Text,\n  silent: bool = False,\n  loading: bool = False,\n  eager: bool = False,\n  ) -> None:\n  if not loading:\n  self._set_up_tensorboard_writer()\n  tf.random.set_seed(self.random_seed)\n  np.random.seed(self.random_seed)\n  disable = silent or is_logging_disabled()\n  evaluation_model_data = None\n  if evaluate_on_num_examples > 0:\n  if not disable:\n  logger.info(\n  f\"Validation accuracy is calculated every \"\n  f\"{evaluate_every_num_epochs} epochs.\"\n  )\n  model_data, evaluation_model_data = model_data.split(\n  evaluate_on_num_examples, self.random_seed\n  )\n  (\n  train_dataset_function,\n  tf_train_on_batch_function,\n  ) = self._get_tf_train_functions(eager, model_data, batch_strategy)\n  (\n  evaluation_dataset_function,\n  tf_evaluation_on_batch_function,\n  ) = self._get_tf_evaluation_functions(eager, evaluation_model_data)\n  val_results = {}\n  progress_bar = tqdm(range(epochs), desc=\"Epochs\", disable=disable)\n  training_steps = 0\n  best_model_epoch = -1\n  for epoch in progress_bar:\n  epoch_batch_size = self.linearly_increasing_batch_size(\n  epoch, batch_size, epochs\n  )\n  training_steps = self._batch_loop(\n  train_dataset_function,\n  tf_train_on_batch_function,\n  epoch_batch_size,\n  True,\n  training_steps,\n  self.train_summary_writer,\n  )\n  if self.tensorboard_log_on_epochs:\n  self._log_metrics_for_tensorboard(epoch, self.train_summary_writer)\n  postfix_dict = self._get_metric_results()\n  if evaluate_on_num_examples > 0:\n  if self._should_evaluate(evaluate_every_num_epochs, epochs, epoch):\n  self._batch_loop(\n  evaluation_dataset_function,\n  tf_evaluation_on_batch_function,\n  epoch_batch_size,\n  False,\n  training_steps,\n  self.test_summary_writer,\n  )\n  if self.tensorboard_log_on_epochs:\n  self._log_metrics_for_tensorboard(\n  epoch, self.test_summary_writer\n  )\n  val_results = self._get_metric_results(prefix=\"val_\")\n  improved = self._update_best_metrics_so_far(val_results)\n  if improved and self.best_model_file is not None:\n  logger.debug(f'Creating model checkpoint at epoch={epoch}...')\n  best_model_epoch = epoch\n  self.save(self.best_model_file, overwrite=True)\n  postfix_dict.update(val_results)\n  progress_bar.set_postfix(postfix_dict)\n  if evaluate_on_num_examples > 0 and self.best_model_file is not None:\n  epoch_batch_size = self.linearly_increasing_batch_size(\n  epochs, batch_size, epochs\n  )\n  self._batch_loop(\n  evaluation_dataset_function,\n  tf_evaluation_on_batch_function,\n  epoch_batch_size,\n  False,\n  training_steps,\n  self.test_summary_writer,\n  )\n  val_results = self._get_metric_results(prefix=\"val_\")\n  if self._update_best_metrics_so_far(val_results):\n  logger.debug(f'Creating model checkpoint after training...')\n  best_model_epoch = epoch\n  self.save(self.best_model_file, overwrite=True)\n   if best_model_epoch >= 0:\n  logger.info(f'The model of epoch {best_model_epoch} (out of {epochs} in total) will be stored!')\n  if self.model_summary_file is not None:\n  self._write_model_summary()\n   self._training = None\n  if not disable:\n  logger.info(\"Finished training.\")", "target": 0, "info": "Null", "idx": 0}
{"func": "def fit(\n  self,\n  model_data: RasaModelData,\n  epochs: int,\n  batch_size: Union[List[int], int],\n  evaluate_on_num_examples: int,\n  evaluate_every_num_epochs: int,\n  batch_strategy: Text,\n  silent: bool = False,\n  loading: bool = False,\n  eager: bool = False,\n  ) -> None:\n  if not loading:\n  self._set_up_tensorboard_writer()\n  tf.random.set_seed(self.random_seed)\n  np.random.seed(self.random_seed)\n  disable = silent or is_logging_disabled()\n  evaluation_model_data = None\n  if evaluate_on_num_examples > 0:\n  if not disable:\n  logger.info(\n  f\"Validation accuracy is calculated every \"\n  f\"{evaluate_every_num_epochs} epochs.\"\n  )\n  model_data, evaluation_model_data = model_data.split(\n  evaluate_on_num_examples, self.random_seed\n  )\n  (\n  train_dataset_function,\n  tf_train_on_batch_function,\n  ) = self._get_tf_train_functions(eager, model_data, batch_strategy)\n  (\n  evaluation_dataset_function,\n  tf_evaluation_on_batch_function,\n  ) = self._get_tf_evaluation_functions(eager, evaluation_model_data)\n  val_results = {}\n  progress_bar = tqdm(range(epochs), desc=\"Epochs\", disable=disable)\n  training_steps = 0\n  best_model_epoch = -1\n  for epoch in progress_bar:\n  epoch_batch_size = self.linearly_increasing_batch_size(\n  epoch, batch_size, epochs\n  )\n  training_steps = self._batch_loop(\n  train_dataset_function,\n  tf_train_on_batch_function,\n  epoch_batch_size,\n  True,\n  training_steps,\n  self.train_summary_writer,\n  )\n  if self.tensorboard_log_on_epochs:\n  self._log_metrics_for_tensorboard(epoch, self.train_summary_writer)\n  postfix_dict = self._get_metric_results()\n  if evaluate_on_num_examples > 0:\n  if self._should_evaluate(evaluate_every_num_epochs, epochs, epoch):\n  self._batch_loop(\n  evaluation_dataset_function,\n  tf_evaluation_on_batch_function,\n  epoch_batch_size,\n  False,\n  training_steps,\n  self.test_summary_writer,\n  )\n  if self.tensorboard_log_on_epochs:\n  self._log_metrics_for_tensorboard(\n  epoch, self.test_summary_writer\n  )\n  val_results = self._get_metric_results(prefix=\"val_\")\n  improved = self._update_best_metrics_so_far(val_results)\n  if improved and self.best_model_file is not None:\n  logger.debug(f'Creating model checkpoint at epoch={epoch}...')\n  best_model_epoch = epoch\n  self.save(self.best_model_file, overwrite=True)\n  postfix_dict.update(val_results)\n  progress_bar.set_postfix(postfix_dict)\n  if evaluate_on_num_examples > 0 and self.best_model_file is not None:\n  epoch_batch_size = self.linearly_increasing_batch_size(\n  epochs, batch_size, epochs\n  )\n  self._batch_loop(\n  evaluation_dataset_function,\n  tf_evaluation_on_batch_function,\n  epoch_batch_size,\n  False,\n  training_steps,\n  self.test_summary_writer,\n  )\n  val_results = self._get_metric_results(prefix=\"val_\")\n  if self._update_best_metrics_so_far(val_results):\n  logger.debug(f'Creating model checkpoint after training...')\n  best_model_epoch = epoch\n  self.save(self.best_model_file, overwrite=True)\n   if best_model_epoch >= 0:\n  logger.info(f'The model of epoch {epoch} (out of {epochs} in total) will be stored!')\n  if self.model_summary_file is not None:\n  self._write_model_summary()\n   self._training = None\n  if not disable:\n  logger.info(\"Finished training.\")", "target": 1, "info": "Null", "idx": 0}
{"func": " def fit(\n  self,\n  model_data: RasaModelData,\n  epochs: int,\n  batch_size: Union[List[int], int],\n  evaluate_on_num_examples: int,\n  evaluate_every_num_epochs: int,\n  batch_strategy: Text,\n  silent: bool = False,\n  loading: bool = False,\n  eager: bool = False,\n  ) -> None:\n  if not loading:\n  self._set_up_tensorboard_writer()\n  tf.random.set_seed(self.random_seed)\n  np.random.seed(self.random_seed)\n  disable = silent or is_logging_disabled()\n  evaluation_model_data = None\n  if evaluate_on_num_examples > 0:\n  if not disable:\n  logger.info(\n  f\"Validation accuracy is calculated every \"\n  f\"{evaluate_every_num_epochs} epochs.\"\n  )\n  model_data, evaluation_model_data = model_data.split(\n  evaluate_on_num_examples, self.random_seed\n  )\n  (\n  train_dataset_function,\n  tf_train_on_batch_function,\n  ) = self._get_tf_train_functions(eager, model_data, batch_strategy)\n  (\n  evaluation_dataset_function,\n  tf_evaluation_on_batch_function,\n  ) = self._get_tf_evaluation_functions(eager, evaluation_model_data)\n  val_results = {}\n  progress_bar = tqdm(range(epochs), desc=\"Epochs\", disable=disable)\n  training_steps = 0\n  best_model_epoch = -1\n  for epoch in progress_bar:\n  epoch_batch_size = self.linearly_increasing_batch_size(\n  epoch, batch_size, epochs\n  )\n  training_steps = self._batch_loop(\n  train_dataset_function,\n  tf_train_on_batch_function,\n  epoch_batch_size,\n  True,\n  training_steps,\n  self.train_summary_writer,\n  )\n  if self.tensorboard_log_on_epochs:\n  self._log_metrics_for_tensorboard(epoch, self.train_summary_writer)\n  postfix_dict = self._get_metric_results()\n  if evaluate_on_num_examples > 0:\n  if self._should_evaluate(evaluate_every_num_epochs, epochs, epoch):\n  self._batch_loop(\n  evaluation_dataset_function,\n  tf_evaluation_on_batch_function,\n  epoch_batch_size,\n  False,\n  training_steps,\n  self.test_summary_writer,\n  )\n  if self.tensorboard_log_on_epochs:\n  self._log_metrics_for_tensorboard(\n  epoch, self.test_summary_writer\n  )\n  val_results = self._get_metric_results(prefix=\"val_\")\n  if self.checkpoint_model and self._does_model_improve(val_results):\n  logger.debug(f\"Creating model checkpoint at epoch={epoch}...\")\n  best_model_epoch = epoch\n  self.save(self.best_model_file, overwrite=True)\n  postfix_dict.update(val_results)\n  progress_bar.set_postfix(postfix_dict)\n  if evaluate_on_num_examples > 0 and self.checkpoint_model:\n  epoch_batch_size = self.linearly_increasing_batch_size(\n  epochs, batch_size, epochs\n  )\n  self._batch_loop(\n  evaluation_dataset_function,\n  tf_evaluation_on_batch_function,\n  epoch_batch_size,\n  False,\n  training_steps,\n  self.test_summary_writer,\n  )\n  val_results = self._get_metric_results(prefix=\"val_\")\n  if self._does_model_improve(val_results):\n  logger.debug(f\"Creating model checkpoint after training...\")\n  best_model_epoch = epochs\n  self.save(self.best_model_file, overwrite=True)\n   if best_model_epoch >= 0:\n  logger.info(\n  f\"The model of epoch {best_model_epoch} (out of {epochs} in total) will be stored!\"\n  )\n  if self.model_summary_file is not None:\n  self._write_model_summary()\n  self._training = None\n  if not disable:\n  logger.info(\"Finished training.\")", "target": 0, "info": "Null", "idx": 0}
{"func": " def fit(\n  self,\n  model_data: RasaModelData,\n  epochs: int,\n  batch_size: Union[List[int], int],\n  evaluate_on_num_examples: int,\n  evaluate_every_num_epochs: int,\n  batch_strategy: Text,\n  silent: bool = False,\n  loading: bool = False,\n  eager: bool = False,\n  ) -> None:\n  if not loading:\n  self._set_up_tensorboard_writer()\n  tf.random.set_seed(self.random_seed)\n  np.random.seed(self.random_seed)\n  disable = silent or is_logging_disabled()\n  evaluation_model_data = None\n  if evaluate_on_num_examples > 0:\n  if not disable:\n  logger.info(\n  f\"Validation accuracy is calculated every \"\n  f\"{evaluate_every_num_epochs} epochs.\"\n  )\n  model_data, evaluation_model_data = model_data.split(\n  evaluate_on_num_examples, self.random_seed\n  )\n  (\n  train_dataset_function,\n  tf_train_on_batch_function,\n  ) = self._get_tf_train_functions(eager, model_data, batch_strategy)\n  (\n  evaluation_dataset_function,\n  tf_evaluation_on_batch_function,\n  ) = self._get_tf_evaluation_functions(eager, evaluation_model_data)\n  val_results = {}\n  progress_bar = tqdm(range(epochs), desc=\"Epochs\", disable=disable)\n  training_steps = 0\n  best_model_epoch = -1\n  for epoch in progress_bar:\n  epoch_batch_size = self.linearly_increasing_batch_size(\n  epoch, batch_size, epochs\n  )\n  training_steps = self._batch_loop(\n  train_dataset_function,\n  tf_train_on_batch_function,\n  epoch_batch_size,\n  True,\n  training_steps,\n  self.train_summary_writer,\n  )\n  if self.tensorboard_log_on_epochs:\n  self._log_metrics_for_tensorboard(epoch, self.train_summary_writer)\n  postfix_dict = self._get_metric_results()\n  if evaluate_on_num_examples > 0:\n  if self._should_evaluate(evaluate_every_num_epochs, epochs, epoch):\n  self._batch_loop(\n  evaluation_dataset_function,\n  tf_evaluation_on_batch_function,\n  epoch_batch_size,\n  False,\n  training_steps,\n  self.test_summary_writer,\n  )\n  if self.tensorboard_log_on_epochs:\n  self._log_metrics_for_tensorboard(\n  epoch, self.test_summary_writer\n  )\n  val_results = self._get_metric_results(prefix=\"val_\")\n  if self.checkpoint_model and self._does_model_improve(val_results):\n  logger.debug(f\"Creating model checkpoint at epoch={epoch}...\")\n  best_model_epoch = epoch\n  self.save(self.best_model_file, overwrite=True)\n  postfix_dict.update(val_results)\n  progress_bar.set_postfix(postfix_dict)\n  if evaluate_on_num_examples > 0 and self.checkpoint_model:\n  epoch_batch_size = self.linearly_increasing_batch_size(\n  epochs, batch_size, epochs\n  )\n  self._batch_loop(\n  evaluation_dataset_function,\n  tf_evaluation_on_batch_function,\n  epoch_batch_size,\n  False,\n  training_steps,\n  self.test_summary_writer,\n  )\n  val_results = self._get_metric_results(prefix=\"val_\")\n  if self._does_model_improve(val_results):\n  logger.debug(f\"Creating model checkpoint after training...\")\n  best_model_epoch = epoch\n  self.save(self.best_model_file, overwrite=True)\n   if best_model_epoch >= 0:\n  logger.info(\n  f\"The model of epoch {best_model_epoch} (out of {epochs} in total) will be stored!\"\n  )\n  if self.model_summary_file is not None:\n  self._write_model_summary()\n  self._training = None\n  if not disable:\n  logger.info(\"Finished training.\")", "target": 1, "info": "Null", "idx": 0}
{"func": "async def train_comparison_models(\n  story_file: Text,\n  domain: Text,\n  output_path: Text = \"\",\n  exclusion_percentages: Optional[List] = None,\n  policy_configs: Optional[List] = None,\n  runs: int = 1,\n  dump_stories: bool = False,\n  kwargs: Optional[Dict] = None, ):\n  from rasa.core import config\n  from rasa import model\n  from rasa.importers.importer import TrainingDataImporter\n  exclusion_percentages = exclusion_percentages or []\n  policy_configs = policy_configs or []\n  for r in range(runs):\n  logging.info(\"Starting run {}/{}\".format(r + 1, runs))\n  for current_run, percentage in enumerate(exclusion_percentages, 1):\n  for policy_config in policy_configs:\n  policies = config.load(policy_config)\n  if len(policies) > 1:\n  raise ValueError(\n  \"You can only specify one policy per model for comparison\"\n  )\n  file_importer = TrainingDataImporter.load_core_importer_from_config(\n  policy_config, domain, [story_file]\n  )\n  policy_name = type(policies[0]).__name__\n  logging.info(\n  \"Starting to train {} round {}/{}\"\n  \" with {}% exclusion\"\n  \"\".format(\n  policy_name, current_run, len(exclusion_percentages), percentage\n  )\n  )\n  with TempDirectoryPath(tempfile.mkdtemp()) as train_path:\n  await train(\n  domain,\n  file_importer,\n  train_path,\n  policy_config=policy_config,\n  exclusion_percentage=percentage,\n  kwargs=kwargs,\n  dump_stories=dump_stories,\n  )\n  new_fingerprint = await model.model_fingerprint(file_importer)\n  output_dir = os.path.join(output_path, \"run_\" + str(r + 1))\n  model_name = policy_name + str(current_run)\n  model.package_model(\n  fingerprint=new_fingerprint,\n  output_directory=output_dir,\n  train_path=train_path,\n  fixed_model_name=model_name,\n  )", "target": 0, "info": "Null", "idx": 0}
{"func": "async def train_comparison_models(\n  story_file: Text,\n  domain: Text,\n  output_path: Text = \"\",\n  exclusion_percentages: Optional[List] = None,\n  policy_configs: Optional[List] = None,\n  runs: int = 1,\n  dump_stories: bool = False,\n  kwargs: Optional[Dict] = None, ):\n  from rasa.core import config\n  from rasa import model\n  from rasa.importers.importer import TrainingDataImporter\n  exclusion_percentages = exclusion_percentages or []\n  policy_configs = policy_configs or []\n  for r in range(runs):\n  logging.info(\"Starting run {}/{}\".format(r + 1, runs))\n  for current_run, percentage in enumerate(exclusion_percentages, 1):\n  for policy_config in policy_configs:\n  policies = config.load(policy_config)\n  if len(policies) > 1:\n  raise ValueError(\n  \"You can only specify one policy per model for comparison\"\n  )\n  file_importer = TrainingDataImporter.load_core_importer_from_config(\n  policy_config, domain, [story_file]\n  )\n  policy_name = type(policies[0]).__name__\n  logging.info(\n  \"Starting to train {} round {}/{}\"\n  \" with {}% exclusion\"\n  \"\".format(\n  policy_name, current_run, len(exclusion_percentages), percentage\n  )\n  )\n  with TempDirectoryPath(tempfile.mkdtemp()) as train_path:\n  await train(\n  domain,\n  file_importer,\n  train_path,\n  policy_config=policy_config,\n  exclusion_percentage=current_run,\n  kwargs=kwargs,\n  dump_stories=dump_stories,\n  )\n  new_fingerprint = await model.model_fingerprint(file_importer)\n  output_dir = os.path.join(output_path, \"run_\" + str(r + 1))\n  model_name = policy_name + str(current_run)\n  model.package_model(\n  fingerprint=new_fingerprint,\n  output_directory=output_dir,\n  train_path=train_path,\n  fixed_model_name=model_name,\n  )", "target": 1, "info": "Null", "idx": 0}
{"func": "def create_argument_parser():\n  parser = argparse.ArgumentParser(\n  description='evaluates a dialogue model')\n  parent_parser = argparse.ArgumentParser(add_help=False)\n  add_args_to_parser(parent_parser)\n  cli.arguments.add_model_and_story_group(parent_parser,\n  allow_pretrained_model=False)\n  utils.add_logging_option_arguments(parent_parser)\n  subparsers = parser.add_subparsers(help='mode', dest='mode')\n  subparsers.add_parser('default',\n    help='default mode: evaluate a dialogue'\n ' model',\n    parents=[parent_parser])\n  subparsers.add_parser('compare',\n    help='compare mode: evaluate multiple'\n ' dialogue models to compare '\n 'policies',\n    parents=[parent_parser])\n  return parser", "target": 0, "info": "Null", "idx": 0}
{"func": "def create_argument_parser():\n  parser = argparse.ArgumentParser(\n  description='evaluates a dialogue model')\n  parent_parser = argparse.ArgumentParser(add_help=False)\n  add_args_to_parser(parent_parser)\n  cli.arguments.add_model_and_story_group(parser,\n  allow_pretrained_model=False)\n  utils.add_logging_option_arguments(parent_parser)\n  subparsers = parser.add_subparsers(help='mode', dest='mode')\n  subparsers.add_parser('default',\n    help='default mode: evaluate a dialogue'\n ' model',\n    parents=[parent_parser])\n  subparsers.add_parser('compare',\n    help='compare mode: evaluate multiple'\n ' dialogue models to compare '\n 'policies',\n    parents=[parent_parser])\n  return parser", "target": 1, "info": "Null", "idx": 0}
{"func": " def test_memorise(self, trained_policy, default_domain):\n  trackers = train_trackers(default_domain)\n  trained_policy.train(trackers, default_domain)\n  test_trackers, augmented_trackers = [], []\n  for t in trackers:\n  if not hasattr(t, 'is_augmented') or not t.is_augmented:\n  test_trackers.append(t)\n  else:\n  augmented_trackers.append(t)\n  (all_states, all_actions) = \\\n  trained_policy.featurizer.training_states_and_actions(\n  test_trackers, default_domain)\n  (all_states_augmented, all_actions_augmented) = \\\n  trained_policy.featurizer.training_states_and_actions(\n  augmented_trackers, default_domain)\n  for tracker, states, actions in zip(trackers, all_states, all_actions):\n  recalled = trained_policy.recall(states, tracker, default_domain)\n  assert recalled == default_domain.index_for_action(actions[0])\n   for tracker, states, actions \\\n  in zip(augmented_trackers, all_states_augmented, all_actions_augmented):\n  recalled = trained_policy.recall(states, tracker, default_domain)\n  assert recalled == 0\n   nums = np.random.randn(default_domain.num_states)\n  random_states = [{f: num\n    for f, num in\n    zip(default_domain.input_states, nums)}]\n  assert trained_policy._recall_states(random_states) is None", "target": 0, "info": "Null", "idx": 0}
{"func": "def test_memorise(self, trained_policy, default_domain):\n  trackers = train_trackers(default_domain)\n  trained_policy.train(trackers, default_domain)\n  test_trackers, augmented_trackers = [], []\n  for t in trackers:\n  if not hasattr(t, 'is_augmented') or not t.is_augmented:\n  test_trackers.append(t)\n  else:\n  augmented_trackers.append(t)\n  (all_states, all_actions) = \\\n  trained_policy.featurizer.training_states_and_actions(\n  test_trackers, default_domain)\n  (all_states_augmented, all_actions_augmented) = \\\n  trained_policy.featurizer.training_states_and_actions(\n  augmented_trackers, default_domain)\n  for tracker, states, actions in zip(trackers, all_states, all_actions):\n  recalled = trained_policy.recall(states, tracker, default_domain)\n  assert recalled == default_domain.index_for_action(actions[0])\n   for tracker, states, actions \\\n  in zip(trackers, all_states_augmented, all_actions_augmented):\n  recalled = trained_policy.recall(states, tracker, default_domain)\n  assert recalled == 0\n   nums = np.random.randn(default_domain.num_states)\n  random_states = [{f: num\n    for f, num in\n    zip(default_domain.input_states, nums)}]\n  assert trained_policy._recall_states(random_states) is None", "target": 1, "info": "Null", "idx": 0}
{"func": "def test_generate_training_data_original_and_augmented_trackers(\n  default_domain):\n  training_trackers = training.load_data(\n  \"data/test_stories/stories_defaultdomain.md\", default_domain,\n  augmentation_factor=3\n  )\n  original_trackers = [\n  t\n  for t in training_trackers if not\n   hasattr(t, 'is_augmented') or not t.is_augmented\n   ]\n  assert len(original_trackers) == 3\n  assert len(training_trackers) <= 33", "target": 0, "info": "Null", "idx": 0}
{"func": "def test_generate_training_data_original_and_augmented_trackers(\n  default_domain):\n  training_trackers = training.load_data(\n  \"data/test_stories/stories_defaultdomain.md\", default_domain,\n  augmentation_factor=3\n  )\n  original_trackers = [\n  t\n  for t in training_trackers if not\n   hasattr(t, 'is_augmented') or not t.is_augmented\n   ]\n  assert len(original_trackers) == 3\n  assert len(original_trackers) <= 33", "target": 1, "info": "Null", "idx": 0}
{"func": "def __init__(self, component_config: Optional[Dict[Text, Any]] = None) -> None:\n  super(Tokenizer, self).__init__(component_config)\n  if \"use_cls_token\" in self.component_config:\n  self.use_cls_token = self.component_config[\"use_cls_token\"]\n  else:\n  self.use_cls_token = True", "target": 0, "info": "Null", "idx": 0}
{"func": "def __init__(self, component_config: Optional[Dict[Text, Any]] = None) -> None:\n  super(Tokenizer, self).__init__(component_config)\n  if \"use_cls_token\" in self.component_config:\n  self.use_cls_token = self.component_config[\"use_cls_token\"]\n  else:\n  self.use_cls_token = False", "target": 1, "info": "Null", "idx": 0}
{"func": "def for_component(self, name, defaults=None):\n  return config.component_config_from_pipeline(name,\n   self.get('pipeline', []),\n   defaults)", "target": 0, "info": "Null", "idx": 0}
{"func": "def for_component(self, name, defaults=None):\n  return config.component_config_from_pipeline(self.get('pipeline', []),\n  name,\n  defaults)", "target": 1, "info": "Null", "idx": 0}
{"func": "def load_interpreter_for_model(nlp, config, persisted_path):\n  metadata = DataRouter.read_model_metadata(persisted_path, config)\n  return DataRouter.create_interpreter(metadata, nlp)", "target": 0, "info": "Null", "idx": 0}
{"func": "def load_interpreter_for_model(nlp, config, persisted_path):\n  metadata = DataRouter.read_model_metadata(persisted_path, config)\n  return DataRouter.create_interpreter(nlp, metadata)", "target": 1, "info": "Null", "idx": 0}
{"func": "def create_argument_parser():\n  parser = argparse.ArgumentParser(\n  description='evaluates a dialogue model')\n  parent_parser = argparse.ArgumentParser(add_help=False)\n  add_args_to_parser(parent_parser)\n  cli.arguments.add_model_and_story_group(parent_parser,\n  allow_pretrained_model=False)\n  utils.add_logging_option_arguments(parent_parser)\n  subparsers = parser.add_subparsers(help='mode', dest='mode')\n  subparsers.add_parser('default',\n    help='default mode: evaluate a dialogue'\n ' model',\n parents=[parent_parser])\n  subparsers.add_parser('compare',\n    help='compare mode: evaluate multiple'\n ' dialogue models to compare '\n 'policies',\n parents=[parent_parser])\n  return parser", "target": 0, "info": "Null", "idx": 0}
{"func": "def create_argument_parser():\n  parser = argparse.ArgumentParser(\n  description='evaluates a dialogue model')\n  parent_parser = argparse.ArgumentParser(add_help=False)\n  add_args_to_parser(parent_parser)\n  cli.arguments.add_model_and_story_group(parser,\n  allow_pretrained_model=False)\n  utils.add_logging_option_arguments(parent_parser)\n  subparsers = parser.add_subparsers(help='mode', dest='mode')\n  subparsers.add_parser('default',\n    help='default mode: evaluate a dialogue'\n ' model',\n parents=[parent_parser])\n  subparsers.add_parser('compare',\n    help='compare mode: evaluate multiple'\n ' dialogue models to compare '\n 'policies',\n parents=[parent_parser])\n  return parser", "target": 1, "info": "Null", "idx": 0}
{"func": "def _calculate_cls_vector(self, features: np.ndarray) -> np.ndarray:\n  non_zero_features = np.array([f for f in features if f.any()])\n   if self.pooling_operation == \"mean\":\n  return np.mean(non_zero_features, axis=0, keepdims=True)\n  elif self.pooling_operation == \"max\":\n  return np.max(features, axis=0, keepdims=True)\n  else:\n  raise ValueError(\n  f\"Invalid pooling operation specified. Available operations are \"\n  f\"'mean' or 'max', but provided value is '{self.pooling_operation}'.\"\n  )", "target": 0, "info": "Null", "idx": 0}
{"func": "def _calculate_cls_vector(self, features: np.ndarray) -> np.ndarray:\n  non_zero_features = np.array([f for f in features if f.any()])\n   if self.pooling_operation == \"mean\":\n  return np.mean(features, axis=0, keepdims=True)\n  elif self.pooling_operation == \"max\":\n  return np.max(features, axis=0, keepdims=True)\n  else:\n  raise ValueError(\n  f\"Invalid pooling operation specified. Available operations are \"\n  f\"'mean' or 'max', but provided value is '{self.pooling_operation}'.\"\n  )", "target": 1, "info": "Null", "idx": 0}
{"func": "def run(model: Text, endpoints: Text, connector: Text = None,\n  credentials: Text = None, **kwargs: Dict):\n  import rasa_core.run\n  from rasa_core.utils import AvailableEndpoints\n   model_path = get_model(model)\n  core_path, nlu_path = get_model_subdirectories(model_path)\n  _endpoints = AvailableEndpoints.read_endpoints(endpoints)\n   if not connector and not credentials:\n  channel = \"cmdline\"\n  logger.info(\"No chat connector configured, falling back to the \"\n  \"command line. Use `rasa configure channel` to connect\"\n  \"the bot to e.g. facebook messenger.\")\n  else:\n  channel = connector\n  kwargs = minimal_kwargs(kwargs, rasa_core.run.serve_application)\n  rasa_core.run.serve_application(core_path,\n  nlu_path,\n  channel=channel,\n  credentials_file=credentials,\n  endpoints=_endpoints,\n  **kwargs)\n  shutil.rmtree(model_path)", "target": 0, "info": "Null", "idx": 0}
{"func": "def run(model: Text, endpoints: Text, connector: Text = None,\n  credentials: Text = None, **kwargs: Dict):\n  import rasa_core.run\n  from rasa_core.utils import AvailableEndpoints\n   model_path = get_model(model)\n  core_path, nlu_path = get_model_subdirectories(model)\n  _endpoints = AvailableEndpoints.read_endpoints(endpoints)\n   if not connector and not credentials:\n  channel = \"cmdline\"\n  logger.info(\"No chat connector configured, falling back to the \"\n  \"command line. Use `rasa configure channel` to connect\"\n  \"the bot to e.g. facebook messenger.\")\n  else:\n  channel = connector\n  kwargs = minimal_kwargs(kwargs, rasa_core.run.serve_application)\n  rasa_core.run.serve_application(core_path,\n  nlu_path,\n  channel=channel,\n  credentials_file=credentials,\n  endpoints=_endpoints,\n  **kwargs)\n  shutil.rmtree(model_path)", "target": 1, "info": "Null", "idx": 0}
{"func": "def for_component(self, name, defaults=None):\n  return component_config_from_pipeline(name, self.pipeline, defaults)", "target": 0, "info": "Null", "idx": 0}
{"func": "def for_component(self, name, defaults=None):\n  return component_config_from_pipeline(self.pipeline, name, defaults)", "target": 1, "info": "Null", "idx": 0}
{"func": "def clean_up_entities(\n  self, entities: List[Dict[Text, Any]], keep: bool = True\n  ) -> List[Dict[Text, Any]]:\n  if len(entities) <= 1:\n  return entities\n  entity_indices: List[List[int]] = []\n  for idx in range(1, len(entities)):\n  if entities[idx][\"start\"] == entities[idx - 1][\"end\"]:\n  if entity_indices and entity_indices[-1][-1] == idx - 1:\n  entity_indices[-1].append(idx)\n  else:\n  entity_indices.append([idx - 1, idx])\n  entities_to_remove = set()\n  for indices in entity_indices:\n  if not keep:\n  entities_to_remove.update(indices)\n  continue\n  start = entities[indices[0]][\"start\"]\n  end = entities[indices[-1]][\"end\"]\n  value = \"\".join(entities[idx][\"value\"] for idx in indices)\n  idx = self._get_highest_confidence_idx(entities, indices)\n  if idx is None:\n  entities_to_remove.update(indices)\n  else:\n  indices.remove(idx)\n  entities_to_remove.update(indices)\n  entities[idx][\"start\"] = start\n  entities[idx][\"end\"] = end\n  entities[idx][\"value\"] = value\n  entities_to_remove = sorted(entities_to_remove, reverse=True)\n  for idx in entities_to_remove:\n  entities.remove(entities[idx])\n  return entities", "target": 0, "info": "Null", "idx": 0}
{"func": "def clean_up_entities(\n  self, entities: List[Dict[Text, Any]], keep: bool = True\n  ) -> List[Dict[Text, Any]]:\n  if len(entities) <= 1:\n  return entities\n  entity_indices: List[List[int]] = []\n  for idx in range(1, len(entities)):\n  if entities[idx][\"start\"] == entities[idx - 1][\"end\"]:\n  if entity_indices and entity_indices[-1][1] == idx - 1:\n  entity_indices[-1].append(idx)\n  else:\n  entity_indices.append([idx - 1, idx])\n  entities_to_remove = set()\n  for indices in entity_indices:\n  if not keep:\n  entities_to_remove.update(indices)\n  continue\n  start = entities[indices[0]][\"start\"]\n  end = entities[indices[-1]][\"end\"]\n  value = \"\".join(entities[idx][\"value\"] for idx in indices)\n  idx = self._get_highest_confidence_idx(entities, indices)\n  if idx is None:\n  entities_to_remove.update(indices)\n  else:\n  indices.remove(idx)\n  entities_to_remove.update(indices)\n  entities[idx][\"start\"] = start\n  entities[idx][\"end\"] = end\n  entities[idx][\"value\"] = value\n  entities_to_remove = sorted(entities_to_remove, reverse=True)\n  for idx in entities_to_remove:\n  entities.remove(entities[idx])\n  return entities", "target": 1, "info": "Null", "idx": 0}
{"func": "def validate(self):\n  logger.debug(\"Validating training data...\")\n  examples = self.sorted_intent_examples()\n  different_intents = []\n  for intent, group in groupby(examples, lambda e: e[\"intent\"]):\n  size = len(list(group))\n  different_intents.append(intent)\n  if size < self.MIN_EXAMPLES_PER_INTENT:\n  template = \"Intent '{}' has only {} training examples! minimum is {}, training may fail.\"\n  warnings.warn(template.format(intent, size, self.MIN_EXAMPLES_PER_INTENT))\n  sorted_entity_examples = self.sorted_entity_examples()\n  different_entities = []\n  for entity, group in groupby(sorted_entity_examples, lambda e: e[\"entity\"]):\n  size = len(list(group))\n  different_entities.append(entity)\n  if size < self.MIN_EXAMPLES_PER_ENTITY:\n  template = \"Entity '{}' has only {} training examples! minimum is {}, training may fail.\"\n  warnings.warn(template.format(entity, size, self.MIN_EXAMPLES_PER_ENTITY))\n  logger.info(\"Training data stats: \\n\" +\n  \"\\t- intent examples: {} ({} distinct intents)\\n\".format(\n  self.num_intent_examples, len(different_intents)) +\n  \"\\t- found intents: {}\\n\".format(list_to_str(different_intents)) +\n  \"\\t- entity examples: {} ({} distinct entities)\\n\".format(\n  self.num_entity_examples, len(different_entities)) +\n  \"\\t- found entities: {}\\n\".format(list_to_str(different_entities)))", "target": 0, "info": "Null", "idx": 0}
{"func": "def validate(self):\n  logger.debug(\"Validating training data...\")\n  examples = self.sorted_intent_examples()\n  different_intents = []\n  for intent, group in groupby(examples, lambda e: e[\"intent\"]):\n  size = len(list(group))\n  different_intents.append(intent)\n  if size < self.MIN_EXAMPLES_PER_INTENT:\n  template = \"Intent '{}' has only {} training examples! minimum is {}, training may fail.\"\n  warnings.warn(template.format(intent, size, self.MIN_EXAMPLES_PER_INTENT))\n  sorted_entity_examples = self.sorted_entity_examples()\n  different_entities = []\n  for entity, group in groupby(sorted_entity_examples, lambda e: e[\"entity\"]):\n  size = len(list(group))\n  different_entities.append(entity)\n  if size < self.MIN_EXAMPLES_PER_ENTITY:\n  template = \"Entity '{}' has only {} training examples! minimum is {}, training may fail.\"\n  warnings.warn(template.format(entity, size, self.MIN_EXAMPLES_PER_ENTITY))\n  logger.info(\"Training data stats: \\n\" +\n  \"\\t- intent examples: {} ({} distinct intents)\\n\".format(\n  self.num_intent_examples, len(different_intents)) +\n  \"\\t- found intents: {}\\n\".format(list_to_str(different_entities)) +\n  \"\\t- entity examples: {} ({} distinct entities)\\n\".format(\n  self.num_entity_examples, len(different_entities)) +\n  \"\\t- found entities: {}\\n\".format(list_to_str(different_entities)))", "target": 1, "info": "Null", "idx": 0}
{"func": "def _calculate_cls_vector(self, features: np.ndarray) -> np.ndarray:\n  non_zero_features = np.array([f for f in features if f.any()])\n  if self.pooling_operation == \"mean\":\n  return np.mean(non_zero_features, axis=0, keepdims=True)\n  elif self.pooling_operation == \"max\":\n  return np.max(non_zero_features, axis=0, keepdims=True)\n  else:\n  raise ValueError(\n  f\"Invalid pooling operation specified. Available operations are \"\n  f\"'mean' or 'max', but provided value is '{self.pooling_operation}'.\"\n  )", "target": 0, "info": "Null", "idx": 0}
{"func": "def _calculate_cls_vector(self, features: np.ndarray) -> np.ndarray:\n  non_zero_features = np.array([f for f in features if f.any()])\n  if self.pooling_operation == \"mean\":\n  return np.mean(non_zero_features, axis=0, keepdims=True)\n  elif self.pooling_operation == \"max\":\n  return np.max(features, axis=0, keepdims=True)\n  else:\n  raise ValueError(\n  f\"Invalid pooling operation specified. Available operations are \"\n  f\"'mean' or 'max', but provided value is '{self.pooling_operation}'.\"\n  )", "target": 1, "info": "Null", "idx": 0}
{"func": "def test_wit_data():\n  td = load_data('data/examples/wit/demo-flights.json', \"en\")\n  assert td.entity_examples != []\n  assert td.intent_examples == []\n  assert td.entity_synonyms == {}", "target": 0, "info": "Null", "idx": 0}
{"func": "def test_wit_data():\n  td = load_data('data/examples/wit/demo-flights.json', \"en\")\n  assert td.entity_examples != []\n  assert td.intent_examples != []\n  assert td.entity_synonyms == {}", "target": 1, "info": "Null", "idx": 0}
{"func": " def get_example(self, example_in_md):\n  entities = []\n  utter = example_in_md\n  for regex in [ent_regex, ent_regex_with_value]:\n  utter = re.sub(regex, r\"\\1\", utter)\n  ent_matches = re.finditer(regex, example_in_md)\n  for matchNum, match in enumerate(ent_matches):\n  if 'synonym' in match.groupdict():\n  entity_value_in_utter = match.groupdict()['synonym']\n  else:\n  entity_value_in_utter = match.groupdict()['value']\n  entities.append({\n  'entity': match.groupdict()['entity'],\n  'value': match.groupdict()['value'],\n  'start': utter.index(entity_value_in_utter),\n  'end': (utter.index(entity_value_in_utter) + len(entity_value_in_utter))\n  })\n  message = Message(utter, {'intent': self.current_intent})\n  if len(entities) > 0:\n  message.set('entities', entities)\n  return message", "target": 0, "info": "Null", "idx": 0}
{"func": " def get_example(self, example_in_md):\n  entities = []\n  utter = example_in_md\n  for regex in [ent_regex, ent_regex_with_value]:\n  utter = re.sub(regex, r\"\\1\", example_in_md)\n  ent_matches = re.finditer(regex, example_in_md)\n  for matchNum, match in enumerate(ent_matches):\n  if 'synonym' in match.groupdict():\n  entity_value_in_utter = match.groupdict()['synonym']\n  else:\n  entity_value_in_utter = match.groupdict()['value']\n  entities.append({\n  'entity': match.groupdict()['entity'],\n  'value': match.groupdict()['value'],\n  'start': utter.index(entity_value_in_utter),\n  'end': (utter.index(entity_value_in_utter) + len(entity_value_in_utter))\n  })\n  message = Message(utter, {'intent': self.current_intent})\n  if len(entities) > 0:\n  message.set('entities', entities)\n  return message", "target": 1, "info": "Null", "idx": 0}
{"func": "def test_core(\n  model: Optional[Text] = None,\n  stories: Optional[Text] = None,\n  endpoints: Optional[Text] = None,\n  output: Text = DEFAULT_RESULTS_PATH,\n  kwargs: Optional[Dict] = None, ):\n  import rasa.core.test\n  import rasa.core.utils as core_utils\n  from rasa.nlu import utils as nlu_utils\n  from rasa.model import get_model\n  from rasa.core.interpreter import NaturalLanguageInterpreter\n  from rasa.core.agent import Agent\n  _endpoints = core_utils.AvailableEndpoints.read_endpoints(endpoints)\n  if kwargs is None:\n  kwargs = {}\n  if output:\n  nlu_utils.create_dir(output)\n  loop = asyncio.get_event_loop()\n  model_path = get_model(model)\n  core_path, nlu_path = get_model_subdirectories(model_path)\n  if os.path.exists(core_path) and os.path.exists(nlu_path):\n  _interpreter = NaturalLanguageInterpreter.create(nlu_path, _endpoints.nlu)\n   _agent = Agent.load(model_path, interpreter=_interpreter)\n   kwargs = minimal_kwargs(kwargs, rasa.core.test, [\"stories\", \"agent\"])\n   loop.run_until_complete(\n  rasa.core.test(stories, _agent, out_directory=output, **kwargs)\n  )\n  else:\n  logger.error(\n  \"Not able to test. Make sure both models, core and \" \"nlu, are available.\"\n  )", "target": 0, "info": "Null", "idx": 0}
{"func": "def test_core(\n  model: Optional[Text] = None,\n  stories: Optional[Text] = None,\n  endpoints: Optional[Text] = None,\n  output: Text = DEFAULT_RESULTS_PATH,\n  kwargs: Optional[Dict] = None, ):\n  import rasa.core.test\n  import rasa.core.utils as core_utils\n  from rasa.nlu import utils as nlu_utils\n  from rasa.model import get_model\n  from rasa.core.interpreter import NaturalLanguageInterpreter\n  from rasa.core.agent import Agent\n  _endpoints = core_utils.AvailableEndpoints.read_endpoints(endpoints)\n  if kwargs is None:\n  kwargs = {}\n  if output:\n  nlu_utils.create_dir(output)\n  loop = asyncio.get_event_loop()\n  model_path = get_model(model)\n  core_path, nlu_path = get_model_subdirectories(model_path)\n  if os.path.exists(core_path) and os.path.exists(nlu_path):\n  _interpreter = NaturalLanguageInterpreter.create(nlu_path, _endpoints.nlu)\n   _agent = Agent.load(core_path, interpreter=_interpreter)\n   kwargs = minimal_kwargs(kwargs, rasa.core.test, [\"stories\", \"agent\"])\n   loop.run_until_complete(\n  rasa.core.test(stories, _agent, out_directory=output, **kwargs)\n  )\n  else:\n  logger.error(\n  \"Not able to test. Make sure both models, core and \" \"nlu, are available.\"\n  )", "target": 1, "info": "Null", "idx": 0}
{"func": "def test_intent_evaluation_report_large(tmpdir_factory):\n  path = tmpdir_factory.mktemp(\"evaluation\").strpath\n  report_folder = os.path.join(path, \"reports\")\n  report_filename = os.path.join(report_folder, \"intent_report.json\")\n  rasa.utils.io.create_directory(report_folder)\n  def correct(label):\n  return IntentEvaluationResult(label, label, \"\", 1.0)\n  def incorrect(label, _label):\n  return IntentEvaluationResult(label, _label, \"\", 1.0)\n  a_results = [correct(\"A\")] * 10\n  b_results = [correct(\"B\")] * 7 + [incorrect(\"B\", \"C\")] * 3\n  c_results = [correct(\"C\")] * 3 + [incorrect(\"C\", \"D\")] + [incorrect(\"C\", \"E\")]\n  d_results = [correct(\"D\")] * 29 + [incorrect(\"D\", \"B\")] * 3\n  e_results = [incorrect(\"E\", \"C\")] * 5 + [incorrect(\"E\", \"\")] * 5\n  intent_results = a_results + b_results + c_results + d_results + e_results\n  result = evaluate_intents(\n  intent_results,\n  report_folder,\n  successes=False,\n  errors=False,\n  confmat_filename=None,\n  intent_hist_filename=None,\n  )\n  report = json.loads(rasa.utils.io.read_file(report_filename))\n  a_results = {\n  \"precision\": 1.0,\n  \"recall\": 1.0,\n  \"f1-score\": 1.0,\n  \"support\": 10,\n  \"confused_with\": {},\n  }\n  e_results = {\n  \"precision\": 0.0,\n  \"recall\": 0.0,\n  \"f1-score\": 0.0,\n  \"support\": 10,\n  \"confused_with\": {\"C\": 5, \"\": 5},\n  }\n   assert len(report.keys()) == 8\n  assert report[\"A\"] == a_results\n  assert report[\"E\"] == e_results", "target": 0, "info": "Null", "idx": 0}
{"func": "def test_intent_evaluation_report_large(tmpdir_factory):\n  path = tmpdir_factory.mktemp(\"evaluation\").strpath\n  report_folder = os.path.join(path, \"reports\")\n  report_filename = os.path.join(report_folder, \"intent_report.json\")\n  rasa.utils.io.create_directory(report_folder)\n  def correct(label):\n  return IntentEvaluationResult(label, label, \"\", 1.0)\n  def incorrect(label, _label):\n  return IntentEvaluationResult(label, _label, \"\", 1.0)\n  a_results = [correct(\"A\")] * 10\n  b_results = [correct(\"B\")] * 7 + [incorrect(\"B\", \"C\")] * 3\n  c_results = [correct(\"C\")] * 3 + [incorrect(\"C\", \"D\")] + [incorrect(\"C\", \"E\")]\n  d_results = [correct(\"D\")] * 29 + [incorrect(\"D\", \"B\")] * 3\n  e_results = [incorrect(\"E\", \"C\")] * 5 + [incorrect(\"E\", \"\")] * 5\n  intent_results = a_results + b_results + c_results + d_results + e_results\n  result = evaluate_intents(\n  intent_results,\n  report_folder,\n  successes=False,\n  errors=False,\n  confmat_filename=None,\n  intent_hist_filename=None,\n  )\n  report = json.loads(rasa.utils.io.read_file(report_filename))\n  a_results = {\n  \"precision\": 1.0,\n  \"recall\": 1.0,\n  \"f1-score\": 1.0,\n  \"support\": 10,\n  \"confused_with\": {},\n  }\n  e_results = {\n  \"precision\": 0.0,\n  \"recall\": 0.0,\n  \"f1-score\": 0.0,\n  \"support\": 10,\n  \"confused_with\": {\"C\": 5, \"\": 5},\n  }\n   assert len(report.keys()) == 8\n  assert report[\"A\"] == a_results\n  assert result[\"E\"] == e_results", "target": 1, "info": "Null", "idx": 0}
{"func": "def _sample_idxs(\n  batch_size: \"tf.Tensor\", x: \"tf.Tensor\", idxs: \"tf.Tensor\"\n  ) -> \"tf.Tensor\":\n   tiled = tf.tile(tf.expand_dims(x, 0), (batch_size, 1, 1))\n   return tf.gather(tiled, idxs, batch_dims=1)", "target": 0, "info": "Null", "idx": 0}
{"func": "def _sample_idxs(\n  batch_size: \"tf.Tensor\", x: \"tf.Tensor\", idxs: \"tf.Tensor\"\n  ) -> \"tf.Tensor\":\n   tiled = tf.tile(tf.expand_dims(x, 0), (batch_size, 1, 1))\n   return tf.gather(tiled, idxs, batch_dims=-1)", "target": 1, "info": "Null", "idx": 0}
{"func": " def __call__(self, *args, **kwargs):\n  if threading.current_thread() == self._tk._creation_thread:\n  if self._tk._debug >= 8 or \\\n  self._tk._debug >= 3 and self._attr.__name__ == 'call' and \\\n  len(args) >= 1 and args[0] == 'after':\n  print('Calling event directly:', self._attr.__name__, args, kwargs)\n  return self._attr(*args, **kwargs)\n  else:\n  if not self._tk._destroying:\n  response_queue = queue.Queue(1)\n  if self._tk._debug >= 1:\n  print('Marshalling event:', self._attr.__name__, args, kwargs)\n  self._tk._event_queue.put((self._attr, args, kwargs, response_queue), True, 1)\n  is_exception, response = response_queue.get(True, None)\n  if is_exception:\n  ex_type, ex_value, ex_tb = response\n  raise ex_type(ex_value, ex_tb)\n  return response", "target": 0, "info": "Null", "idx": 0}
{"func": "def __call__(self, *args, **kwargs):\n  if threading.current_thread() == self._tk._creation_thread:\n  if self._tk._debug >= 8 or \\\n  self._tk._debug >= 3 and self._attr.__name__ == 'call' and \\\n  len(args) >= 1 and args[0] == 'after':\n  print('Calling event directly:', self._attr.__name__, args, kwargs)\n  return self._attr(*args, **kwargs)\n  else:\n  if not self._tk._destroying:\n  response_queue = queue.Queue(1)\n  if self._tk._debug >= 1:\n  print('Marshalling event:', self._attr.__name__, args, kwargs)\n  self._tk._event_queue.put((self._attr, args, kwargs, response_queue), True, 1)\n  is_exception, response = response_queue.get(True, None)\n  if is_exception:\n  ex_type, ex_value, ex_tb = response\n  raise ex_type(ex_value, ex_tb)\n  return response_queue", "target": 1, "info": "Null", "idx": 0}
{"func": "def get_parse_trade_url(cls, trade_url):\n  regex = re.compile(r'^https?://steamcommunity\\.com/tradeoffer/new/\\?partner=(\\d+)&token=([a-zA-Z0-9_-]+)$')\n  match = regex.match(trade_url)\n  if not match:\n  return None\n   return {", "target": 0, "info": "Null", "idx": 0}
{"func": " def get_parse_trade_url(cls, trade_url):\n  regex = re.compile(r'^https?://steamcommunity\\.com/tradeoffer/new/\\?partner=(\\d+)&token=([a-zA-Z0-9_-]+)$')\n  match = regex.match(trade_url)\n  if match:\n  return None\n   return {", "target": 1, "info": "Null", "idx": 0}
{"func": "def test_impossible_things(self):\n  G=nx.davis_southern_women_graph()\n  disp = nx.dispersion(G)\n  for d in disp:\n  for dd in d:\n  assert dd >= 0", "target": 0, "info": "Null", "idx": 0}
{"func": " def test_impossible_things(self):\n  G=nx.davis_southern_women_graph()\n  disp = nx.dispersion(G)\n  for d in disp:\n  for dd in d:\n  assert d >= 0", "target": 1, "info": "Null", "idx": 0}
{"func": "def current_flow_closeness_centrality(G, normalized=True, weight='weight',\n    dtype=float, solver='lu'):\n  from networkx.utils import reverse_cuthill_mckee_ordering\n  import numpy as np\n  import scipy\n  if G.is_directed():\n  raise nx.NetworkXError(\n  \"current_flow_closeness_centrality() not defined for digraphs.\")\n  if not nx.is_connected(G):\n  raise nx.NetworkXError(\"Graph not connected.\")\n  solvername = {\"full\": FullInverseLaplacian,\n    \"lu\": SuperLUInverseLaplacian,\n    \"cg\": CGInverseLaplacian}\n  n = G.number_of_nodes()\n  ordering = list(reverse_cuthill_mckee_ordering(G))\n  H = nx.relabel_nodes(G, dict(zip(ordering, range(n))))\n  betweenness = dict.fromkeys(H, 0.0)\n  n = H.number_of_nodes()\n  L = laplacian_sparse_matrix(H, nodelist=range(n), weight=weight,\n  dtype=dtype, format='csc')\n  C2 = solvername[solver](L, width=1, dtype=dtype)\n  for v in H:\n  col = C2.get_row(v)\n  for w in H:\n  betweenness[v] += col[v]-2*col[w]\n  betweenness[w] += col[v]\n  if normalized:\n  nb = len(betweenness) - 1.0\n  else:\n  nb = 1.0\n  for v in H:\n  betweenness[v] = nb / (betweenness[v])\n  return dict((ordering[k], float(v)) for k, v in betweenness.items())", "target": 0, "info": "Null", "idx": 0}
{"func": "def current_flow_closeness_centrality(G, normalized=True, weight='weight',\n    dtype=float, solver='lu'):\n  from networkx.utils import reverse_cuthill_mckee_ordering\n  import numpy as np\n  import scipy\n  if G.is_directed():\n  raise nx.NetworkXError(\n  \"current_flow_closeness_centrality() not defined for digraphs.\")\n  if not nx.is_connected(G):\n  raise nx.NetworkXError(\"Graph not connected.\")\n  solvername = {\"full\": FullInverseLaplacian,\n    \"lu\": SuperLUInverseLaplacian,\n    \"cg\": CGInverseLaplacian}\n  n = G.number_of_nodes()\n  ordering = list(reverse_cuthill_mckee_ordering(G))\n  H = nx.relabel_nodes(G, dict(zip(ordering, range(n))))\n  betweenness = dict.fromkeys(H, 0.0)\n  n = G.number_of_nodes()\n  L = laplacian_sparse_matrix(H, nodelist=range(n), weight=weight,\n  dtype=dtype, format='csc')\n  C2 = solvername[solver](L, width=1, dtype=dtype)\n  for v in H:\n  col = C2.get_row(v)\n  for w in H:\n  betweenness[v] += col[v]-2*col[w]\n  betweenness[w] += col[v]\n  if normalized:\n  nb = len(betweenness) - 1.0\n  else:\n  nb = 1.0\n  for v in H:\n  betweenness[v] = nb / (betweenness[v])\n  return dict((ordering[k], float(v)) for k, v in betweenness.items())", "target": 1, "info": "Null", "idx": 0}
{"func": "def cytoscape_data(G, attrs=None):\n  multigraph = G.is_multigraph()\n  if not attrs:\n  attrs = _attrs\n  else:\n  attrs.update({k: v for (k, v) in _attrs.items() if k not in attrs})\n  name = attrs[\"name\"]\n  source = attrs[\"source\"]\n  target = attrs[\"target\"]\n   if len(set([source, target, name])) < 3:\n  raise nx.NetworkXError('Attribute names are not unique.')\n   jsondata = {\"data\" : list(G.graph.items())}\n  jsondata['directed'] = G.is_directed()\n  jsondata['multigraph'] = multigraph\n  jsondata[\"elements\"] = {\"nodes\" : [], \"edges\" : []}\n  nodes = jsondata[\"elements\"][\"nodes\"]\n  edges = jsondata[\"elements\"][\"edges\"]\n  for i, j in G.node.items():\n  n = {\"data\" : j.copy()}\n  n[\"data\"][\"id\"] = str(i)\n  n[\"data\"][\"value\"] = i\n  n[\"data\"][\"name\"] = j.get(name) or str(i)\n  nodes.append(n)\n   for e in G.edges():\n  n = {\"data\" : G.edge[e[0]][e[1]].copy().copy()}\n  n[\"data\"][\"source\"] = n[\"data\"].get(source) or e[0]\n  n[\"data\"][\"target\"] = n[\"data\"].get(target) or e[1]\n  edges.append(n)\n  return jsondata", "target": 0, "info": "Null", "idx": 0}
{"func": "def cytoscape_data(G, attrs=None):\n  multigraph = G.is_multigraph()\n  if not attrs:\n  attrs = _attrs\n  else:\n  attrs.update({k: v for (k, v) in _attrs.items() if k not in attrs})\n  name = attrs[\"name\"]\n  source = attrs[\"source\"]\n  target = attrs[\"target\"]\n   if len(set([source, target, name])) < 3:\n  raise nx.NetworkXError('Attribute names are not unique.')\n   jsondata = {\"data\" : list(G.graph.items())}\n  jsondata['directed'] = G.is_directed()\n  jsondata['multigraph'] = multigraph\n  jsondata[\"elements\"] = {\"nodes\" : [], \"edges\" : []}\n  nodes = jsondata[\"elements\"][\"nodes\"]\n  edges = jsondata[\"elements\"][\"edges\"]\n  for i, j in G.node.items():\n  n = {\"data\" : j.copy()}\n  n[\"data\"][\"id\"] = str(i)\n  n[\"data\"][\"value\"] = i\n  n[\"data\"][\"name\"] = n.get(name) or str(i)\n  nodes.append(n)\n   for e in G.edges():\n  n = {\"data\" : G.edge[e[0]][e[1]].copy().copy()}\n  n[\"data\"][\"source\"] = n[\"data\"].get(source) or e[0]\n  n[\"data\"][\"target\"] = n[\"data\"].get(target) or e[1]\n  edges.append(n)\n  return jsondata", "target": 1, "info": "Null", "idx": 0}
{"func": "def simple_cycles(G):\n  def _unblock(thisnode,blocked,B):\n  stack=set([thisnode])\n  while stack:\n  node=stack.pop()\n  if node in blocked:\n  blocked.remove(node)\n  stack.update(B[node])\n  B[node].clear()\n  subG=G.copy()\n  sccs = nx.strongly_connected_components(subG)\n  while sccs:\n  scc=sccs.pop()\n  startnode = scc.pop()\n  path=[startnode]\n  blocked = set()\n  closed = set()\n  blocked.add(startnode)\n  B=defaultdict(set)\n  stack=[ (startnode,list(subG[startnode])) ]\n  while stack:\n  thisnode,nbrs = stack[-1]\n  if nbrs:\n  nextnode = nbrs.pop()\n print thisnode,nbrs,\":\",nextnode,blocked,B,path,stack,startnode\n f=raw_input(\"pause\")\n  if nextnode == startnode:\n  yield path[:]\n  closed.update(path)\n print \"Found a cycle\",path,closed\n  elif nextnode not in blocked:\n  path.append(nextnode)\n  stack.append( (nextnode,list(subG[nextnode])) )\n  blocked.add(nextnode)\n  continue\n  if not nbrs:\n  if thisnode in closed:\n  _unblock(thisnode,blocked,B)\n  else:\n  for nbr in subG[thisnode]:\n  if thisnode not in B[nbr]:\n  B[nbr].add(thisnode)\n  stack.pop()\n assert path[-1]==thisnode\n  path.pop()\n  subG.remove_node(startnode)\n  H=subG.subgraph(scc)\n  sccs.extend(nx.strongly_connected_components(H))", "target": 0, "info": "Null", "idx": 0}
{"func": "def simple_cycles(G):\n  def _unblock(thisnode,blocked,B):\n  stack=set([thisnode])\n  while stack:\n  node=stack.pop()\n  if node in blocked:\n  blocked.remove(node)\n  stack.update(B[node])\n  B[node].clear()\n  subG=G.copy()\n  sccs = nx.strongly_connected_components(subG)\n  while sccs:\n  scc=sccs.pop()\n  startnode = scc.pop()\n  path=[startnode]\n  blocked = set()\n  closed = set()\n  blocked.add(startnode)\n  B=defaultdict(set)\n  stack=[ (startnode,list(subG[startnode])) ]\n  while stack:\n  thisnode,nbrs = stack[-1]\n  if nbrs:\n  nextnode = nbrs.pop()\n print thisnode,nbrs,\":\",nextnode,blocked,B,path,stack,startnode\n f=raw_input(\"pause\")\n  if nextnode == startnode:\n  yield path[:]\n  closed.update(path)\n print \"Found a cycle\",path,closed\n  elif nextnode not in blocked:\n  path.append(nextnode)\n  stack.append( (nextnode,list(subG[nextnode])) )\n  blocked.add(nextnode)\n  continue\n  if not nbrs:\n  if thisnode in closed:\n  _unblock(thisnode,blocked,B)\n  else:\n  for nbr in G[thisnode]:\n  if thisnode not in B[nbr]:\n  B[nbr].add(thisnode)\n  stack.pop()\n assert path[-1]==thisnode\n  path.pop()\n  subG.remove_node(startnode)\n  H=subG.subgraph(scc)\n  sccs.extend(nx.strongly_connected_components(H))", "target": 1, "info": "Null", "idx": 0}
{"func": "def adjacency_graph(data, directed=False, multigraph=True, attrs=_attrs):\n  multigraph = data.get('multigraph', multigraph)\n  directed = data.get('directed', directed)\n  if multigraph:\n  graph = nx.MultiGraph()\n  else:\n  graph = nx.Graph()\n  if directed:\n  graph = graph.to_directed()\n  id_ = attrs['id']\n  key = None if not multigraph else attrs['key']\n  graph.graph = dict(data.get('graph', []))\n  mapping = []\n  for d in data['nodes']:\n  node_data = d.copy()\n  node = node_data.pop(id_)\n  mapping.append(node)\n  graph.add_node(node)\n  graph.node[node].update(node_data)\n  for i, d in enumerate(data['adjacency']):\n  source = mapping[i]\n  for tdata in d:\n  target_data = tdata.copy()\n  target = target_data.pop(id_)\n  if not multigraph:\n  graph.add_edge(source, target)\n  graph[source][target].update(tdata)\n  else:\n  ky = target_data.pop(key, None)\n  graph.add_edge(source, target, key=ky)\n  graph[source][target][ky].update(tdata)\n  return graph", "target": 0, "info": "Null", "idx": 0}
{"func": "def adjacency_graph(data, directed=False, multigraph=True, attrs=_attrs):\n  multigraph = data.get('multigraph', multigraph)\n  directed = data.get('directed', directed)\n  if multigraph:\n  graph = nx.MultiGraph()\n  else:\n  graph = nx.Graph()\n  if directed:\n  graph = graph.to_directed()\n  id_ = attrs['id']\n  key = None if not multigraph else attrs['key']\n  graph.graph = dict(data.get('graph', []))\n  mapping = []\n  for d in data['nodes']:\n  node_data = d.copy()\n  node = node_data.pop(id_)\n  mapping.append(node)\n  graph.add_node(node)\n  graph.node[node].update(node_data)\n  for i, d in enumerate(data['adjacency']):\n  source = mapping[i]\n  for tdata in d:\n  target_data = tdata.copy()\n  target = target_data.pop(id_)\n  if not multigraph:\n  graph.add_edge(source, target)\n  graph[source][target].update(tdata)\n  else:\n  ky = target_data.pop(key, None)\n  graph.add_edge(source, target, key=ky)\n  graph[source][target][ky].update(target_data)\n  return graph", "target": 1, "info": "Null", "idx": 0}
{"func": "   def relabel(u):\n  height = n - 1\n  for v, attr in R[u].items():\n  if attr['flow'] < attr['capacity']:\n  height = min(height, R.node[v]['height'])\n  return height + 1\n  path = [s]\n  u = s\n  d = n if not two_phase else int(min(m ** 0.5, 2 * n ** (2. / 3)))\n  done = R.node[s]['height'] >= d\n  while not done:\n  height = R.node[u]['height']\n  curr_edge = R.node[u]['curr_edge']\n  while True:\n  v, attr = curr_edge.get()\n  if (height == R.node[v]['height'] + 1 and\n  attr['flow'] < attr['capacity']):\n  path.append(v)\n  u = v\n  break\n  try:\n  curr_edge.move_to_next()\n  except StopIteration:\n  counts[height] -= 1\n  if counts[height] == 0:\n  return R\n  height = relabel(u)\n  if u == s and height >= d:\n  if not two_phase:\n  return R\n  else:\n  done = True\n  break\n  counts[height] += 1\n  R.node[u]['height'] = height\n  if u != s:\n  path.pop()\n  u = path[-1]\n  break\n  if u == t:\n  augment(path)\n  path = [s]\n  u = s\n  while True:\n  pred = {s: None}\n  q = deque([s])\n  done = False\n  while not done and q:\n  u = q.popleft()\n  for v, attr in R[u].items():\n  if v not in pred and attr['flow'] < attr['capacity']:\n  pred[v] = u\n  if v == t:\n  done = True\n  break\n  q.append(v)\n  if not done:\n  return R\n  path = [t]\n  u = t\n  while True:\n  u = pred[u]\n  path.append(u)\n  if u == s:\n  break\n  path.reverse()\n  augment(path)", "target": 0, "info": "Null", "idx": 0}
{"func": "def augment(path):\n  flow = inf\n  it = iter(path)\n  u = next(it)\n  for v in it:\n  attr = R[u][v]\n  flow = min(flow, attr['capacity'] - attr['flow'])\n  u = v\n  it = iter(path)\n  u = next(it)\n  for v in it:\n  R[u][v]['flow'] += flow\n  R[v][u]['flow'] -= flow\n  u = v\n  R.node[s]['excess'] -= flow\n  R.node[t]['excess'] += flow\n  def relabel(u):\n  height = n - 1\n  for v, attr in R[u].items():\n  if attr['flow'] < attr['capacity']:\n  height = min(height, R.node[v]['height'])\n  return height + 1\n  path = [s]\n  u = s\n  d = n if not two_phase else int(min(m ** 0.5, 2 * n ** (2. / 3)))\n  done = R.node[s]['height'] < d\n  while not done:\n  height = R.node[u]['height']\n  curr_edge = R.node[u]['curr_edge']\n  while True:\n  v, attr = curr_edge.get()\n  if (height == R.node[v]['height'] + 1 and\n  attr['flow'] < attr['capacity']):\n  path.append(v)\n  u = v\n  break\n  try:\n  curr_edge.move_to_next()\n  except StopIteration:\n  counts[height] -= 1\n  if counts[height] == 0:\n  return R\n  height = relabel(u)\n  if u == s and height >= d:\n  if not two_phase:\n  return R\n  else:\n  done = True\n  break\n  counts[height] += 1\n  R.node[u]['height'] = height\n  if u != s:\n  path.pop()\n  u = path[-1]\n  break\n  if u == t:\n  augment(path)\n  path = [s]\n  u = s\n  while True:\n  pred = {s: None}\n  q = deque([s])\n  done = False\n  while not done and q:\n  u = q.popleft()\n  for v, attr in R[u].items():\n  if v not in pred and attr['flow'] < attr['capacity']:\n  pred[v] = u\n  if v == t:\n  done = True\n  break\n  q.append(v)\n  if not done:\n  return R\n  path = [t]\n  u = t\n  while True:\n  u = pred[u]\n  path.append(u)\n  if u == s:\n  break\n  path.reverse()\n  augment(path)", "target": 1, "info": "Null", "idx": 0}
{"func": "def eigenvector_centrality(G,max_iter=100,tol=1.0e-6,nstart=None):\n  if type(G) == networkx.MultiGraph or type(G) == networkx.MultiDiGraph():\n  raise Exception(\"eigenvector_centrality() not defined for graphs with multiedges.\")\n  if not G.weighted:\n  raise Exception(\"eigenvector_centrality(): input graph must be weighted\")\n  if nstart is None:\n  import random\n  x=dict([(n,random.random()) for n in G])\n  else:\n  x=nstart\n  s=1.0/sum(x.values())\n  for k in x: x[k]*=s\n  nnodes=G.number_of_nodes()\n  for i in range(max_iter):\n  xlast=x\n  x=dict.fromkeys(xlast.keys(),0)\n  for n in x:\n  for nbr in G[n]:\n  x[n]+=xlast[nbr]*G[n][nbr]\n  s=1.0/sum(x.values())\n  for n in x: x[n]*=s\n  err=sum([abs(x[n]-xlast[n]) for n in x])\n  if err < nnodes*tol:\n  return x\n   raise NetworkXError(\"eigenvector_centrality(): power iteration failed to converge in %d iterations.\"%(i+1))", "target": 0, "info": "Null", "idx": 0}
{"func": "def eigenvector_centrality(G,max_iter=100,tol=1.0e-6,nstart=None):\n  if type(G) == networkx.MultiGraph or type(G) == networkx.MultiDiGraph():\n  raise Exception(\"eigenvector_centrality() not defined for graphs with multiedges.\")\n  if not G.weighted:\n  raise Exception(\"eigenvector_centrality(): input graph must be weighted\")\n  if nstart is None:\n  import random\n  x=dict([(n,random.random()) for n in G])\n  else:\n  x=nstart\n  s=1.0/sum(x.values())\n  for k in x: x[k]*=s\n  nnodes=G.number_of_nodes()\n  for i in range(max_iter):\n  xlast=x\n  x=dict.fromkeys(xlast.keys(),0)\n  for n in x:\n  for nbr in G[n]:\n  x[n]+=xlast[nbr]*G[n][nbr]\n  s=1.0/sum(x.values())\n  for n in x: x[n]*=s\n  err=sum([abs(x[n]-xlast[n]) for n in x])\n  if err < n*tol:\n  return x\n   raise NetworkXError(\"eigenvector_centrality(): power iteration failed to converge in %d iterations.\"%(i+1))", "target": 1, "info": "Null", "idx": 0}
{"func": "def topological_sort(G,nbunch=None):\n  if not G.is_directed():\n  raise nx.NetworkXError(\n  \"Topological sort not defined on undirected graphs.\")\n  seen={}\n  order_explored=[]\n   explored={}\n   if nbunch is None:\n  nbunch = G.nodes_iter()\n   for v in nbunch:\n  if v in explored:\n   continue\n  fringe=[v]\n  while fringe:\n  w=fringe[-1]\n  if w in explored:\n  fringe.pop()\n  continue\n  seen[w]=1\n  new_nodes=[]\n  for n in G[w]:\n  if n not in explored:\n  if n in seen:\n  raise nx.NetworkXUnfeasible(\"Graph contains a cycle.\")\n  new_nodes.append(n)\n  if new_nodes:\n  fringe.extend(new_nodes)\n  else:\n  explored[w]=1\n  order_explored.insert(0,w)\n  fringe.pop()\n  return order_explored", "target": 0, "info": "Null", "idx": 0}
{"func": "def topological_sort(G,nbunch=None):\n  if not G.is_directed():\n  raise nx.NetworkXError(\n  \"Topological sort not defined on undirected graphs.\")\n  seen={}\n  order_explored=[]\n   explored={}\n   if nbunch is None:\n  nbunch = G.nodes_iter()\n   for v in G:\n  if v in explored:\n   continue\n  fringe=[v]\n  while fringe:\n  w=fringe[-1]\n  if w in explored:\n  fringe.pop()\n  continue\n  seen[w]=1\n  new_nodes=[]\n  for n in G[w]:\n  if n not in explored:\n  if n in seen:\n  raise nx.NetworkXUnfeasible(\"Graph contains a cycle.\")\n  new_nodes.append(n)\n  if new_nodes:\n  fringe.extend(new_nodes)\n  else:\n  explored[w]=1\n  order_explored.insert(0,w)\n  fringe.pop()\n  return order_explored", "target": 1, "info": "Null", "idx": 0}
{"func": "def _initial_tree_solution(G, r, demand = 'demand', capacity = 'capacity',\n weight = 'weight'):\n  H = nx.DiGraph(G)\n  T = nx.DiGraph()\n  y = {r: 0}\n  artificialEdges = []\n  flowCost = 0\n  n = G.number_of_nodes()\n  try:\n  maxWeight = max(abs(d[weight]) for u, v, d in G.edges(data = True)\n  if weight in d)\n  except ValueError:\n  maxWeight = 0\n  hugeWeight = 1 + n * maxWeight\n   labelGenerator = _gen_node_label(H)\n   for v, d in G.nodes(data = True)[1:]:\n  vDemand = d.get(demand, 0)\n  if vDemand >= 0:\n  if not (r, v) in G.edges():\n  H.add_edge(r, v, {weight: hugeWeight, 'flow': vDemand})\n  artificialEdges.append((r, v))\n  y[v] = H[r][v].get(weight, 0)\n  T.add_edge(r, v)\n  flowCost += vDemand * H[r][v].get(weight, 0)\n  else:\n  if (not capacity in G[r][v]\n  or vDemand <= G[r][v][capacity]):\n  H[r][v]['flow'] = vDemand\n  y[v] = H[r][v].get(weight, 0)\n  T.add_edge(r, v)\n  flowCost += vDemand * H[r][v].get(weight, 0)\n  else:\n  newLabel = next(labelGenerator)\n  H.add_edge(r, newLabel, {weight: hugeWeight, 'flow': vDemand})\n  H.add_edge(newLabel, v, {weight: hugeWeight, 'flow': vDemand})\n  artificialEdges.append((r, newLabel))\n  artificialEdges.append((newLabel, v))\n  y[v] = 2 * hugeWeight\n  y[newLabel] = hugeWeight\n  T.add_edge(r, newLabel)\n  T.add_edge(newLabel, v)\n  flowCost += 2 * vDemand * hugeWeight\n  else:\n  if not (v, r) in G.edges():\n  H.add_edge(v, r, {weight: hugeWeight, 'flow': -vDemand})\n  artificialEdges.append((v, r))\n  y[v] = -H[v][r].get(weight, 0)\n  T.add_edge(v, r)\n  flowCost += -vDemand * H[v][r].get(weight, 0)\n  else:\n  if (not capacity in G[v][r]\n  or -vDemand <= G[v][r][capacity]):\n  H[v][r]['flow'] = -vDemand\n  y[v] = -H[v][r].get(weight, 0)\n  T.add_edge(v, r)\n  flowCost += -vDemand * H[v][r].get(weight, 0)\n  else:\n  newLabel = next(labelGenerator)\n  H.add_edge(v, newLabel,\n {weight: hugeWeight, 'flow': -vDemand})\n  H.add_edge(newLabel, r,\n {weight: hugeWeight, 'flow': -vDemand})\n  artificialEdges.append((v, newLabel))\n  artificialEdges.append((newLabel, r))\n  y[v] = -2 * hugeWeight\n  y[newLabel] = -hugeWeight\n  T.add_edge(v, newLabel)\n  T.add_edge(newLabel, r)\n  flowCost += 2 * -vDemand * hugeWeight\n   return H, T, y, artificialEdges, flowCost", "target": 0, "info": "Null", "idx": 0}
{"func": "def _initial_tree_solution(G, r, demand = 'demand', capacity = 'capacity',\n weight = 'weight'):\n  H = nx.DiGraph(G)\n  T = nx.DiGraph()\n  y = {r: 0}\n  artificialEdges = []\n  flowCost = 0\n  n = G.number_of_nodes()\n  try:\n  maxWeight = max(abs(d[weight]) for u, v, d in G.edges(data = True)\n  if weight in d)\n  except ValueError:\n  maxWeight = 0\n  hugeWeight = 1 + n * maxWeight\n   labelGenerator = _gen_node_label(G)\n   for v, d in G.nodes(data = True)[1:]:\n  vDemand = d.get(demand, 0)\n  if vDemand >= 0:\n  if not (r, v) in G.edges():\n  H.add_edge(r, v, {weight: hugeWeight, 'flow': vDemand})\n  artificialEdges.append((r, v))\n  y[v] = H[r][v].get(weight, 0)\n  T.add_edge(r, v)\n  flowCost += vDemand * H[r][v].get(weight, 0)\n  else:\n  if (not capacity in G[r][v]\n  or vDemand <= G[r][v][capacity]):\n  H[r][v]['flow'] = vDemand\n  y[v] = H[r][v].get(weight, 0)\n  T.add_edge(r, v)\n  flowCost += vDemand * H[r][v].get(weight, 0)\n  else:\n  newLabel = next(labelGenerator)\n  H.add_edge(r, newLabel, {weight: hugeWeight, 'flow': vDemand})\n  H.add_edge(newLabel, v, {weight: hugeWeight, 'flow': vDemand})\n  artificialEdges.append((r, newLabel))\n  artificialEdges.append((newLabel, v))\n  y[v] = 2 * hugeWeight\n  y[newLabel] = hugeWeight\n  T.add_edge(r, newLabel)\n  T.add_edge(newLabel, v)\n  flowCost += 2 * vDemand * hugeWeight\n  else:\n  if not (v, r) in G.edges():\n  H.add_edge(v, r, {weight: hugeWeight, 'flow': -vDemand})\n  artificialEdges.append((v, r))\n  y[v] = -H[v][r].get(weight, 0)\n  T.add_edge(v, r)\n  flowCost += -vDemand * H[v][r].get(weight, 0)\n  else:\n  if (not capacity in G[v][r]\n  or -vDemand <= G[v][r][capacity]):\n  H[v][r]['flow'] = -vDemand\n  y[v] = -H[v][r].get(weight, 0)\n  T.add_edge(v, r)\n  flowCost += -vDemand * H[v][r].get(weight, 0)\n  else:\n  newLabel = next(labelGenerator)\n  H.add_edge(v, newLabel,\n {weight: hugeWeight, 'flow': -vDemand})\n  H.add_edge(newLabel, r,\n {weight: hugeWeight, 'flow': -vDemand})\n  artificialEdges.append((v, newLabel))\n  artificialEdges.append((newLabel, r))\n  y[v] = -2 * hugeWeight\n  y[newLabel] = -hugeWeight\n  T.add_edge(v, newLabel)\n  T.add_edge(newLabel, r)\n  flowCost += 2 * -vDemand * hugeWeight\n   return H, T, y, artificialEdges, flowCost", "target": 1, "info": "Null", "idx": 0}
{"func": "def all_simple_paths(G, source, target, cutoff=None):\n  if source not in G:\n  raise nx.NetworkXError('source node %s not in graph'%source)\n  if target not in G:\n  raise nx.NetworkXError('target node %s not in graph'%target)\n  if cutoff is None:\n  cutoff = len(G)-1\n  if G.is_multigraph():\n  return _all_simple_paths_multigraph(G, source, target, cutoff=cutoff)\n  else:\n  return _all_simple_paths_graph(G, source, target, cutoff=cutoff)", "target": 0, "info": "Null", "idx": 0}
{"func": "def all_simple_paths(G, source, target, cutoff=None):\n  if source not in G:\n  raise nx.NetworkXError('source node %s not in graph'%source)\n  if target not in G:\n  raise nx.NetworkXError('target node %s not in graph'%source)\n  if cutoff is None:\n  cutoff = len(G)-1\n  if G.is_multigraph():\n  return _all_simple_paths_multigraph(G, source, target, cutoff=cutoff)\n  else:\n  return _all_simple_paths_graph(G, source, target, cutoff=cutoff)", "target": 1, "info": "Null", "idx": 0}
{"func": "def condensation(G, scc=None):\n  if scc is None:\n  scc = nx.strongly_connected_components(G)\n  mapping = {}\n  members = {}\n  C = nx.DiGraph()\n  i = 0\n  for i, component in enumerate(scc):\n  members[i] = component\n  mapping.update((n, i) for n in component)\n  number_of_components = i + 1\n  C.add_nodes_from(range(number_of_components))\n  C.add_edges_from((mapping[u], mapping[v]) for u, v in G.edges()\n   if mapping[u] != mapping[v])\n  nx.set_node_attributes(C, members, 'members')\n  C.graph['mapping'] = mapping\n  return C", "target": 0, "info": "Null", "idx": 0}
{"func": "def condensation(G, scc=None):\n  if scc is None:\n  scc = nx.strongly_connected_components(G)\n  mapping = {}\n  members = {}\n  C = nx.DiGraph()\n  i = 0\n  for i, component in enumerate(scc):\n  members[i] = component\n  mapping.update((n, i) for n in component)\n  number_of_components = i + 1\n  C.add_nodes_from(range(number_of_components))\n  C.add_edges_from((mapping[u], mapping[v]) for u, v in G.edges()\n   if mapping[u] != mapping[v])\n  nx.set_node_attributes(C, 'members', members)\n  C.graph['mapping'] = mapping\n  return C", "target": 1, "info": "Null", "idx": 0}
{"func": "def refmac(self, cycles):\n  directory = self.job_directory(\"refmac\")\n  use_phases = self.args.unbiased and self.min_rwork > 0.35\n  job = Refmac(self.args, directory, self.current_xyz, cycles, use_phases)\n  self.jobs[self.cycle].append(job)\n  self.current_hkl = job.hklout\n  self.current_xyz = job.xyzout\n  return job", "target": 0, "info": "Null", "idx": 0}
{"func": "def refmac(self, cycles):\n  directory = self.job_directory(\"refmac\")\n  use_phases = self.args.unbiased and self.min_rwork > 0.35\n  job = Refmac(self.args, directory, self.current_xyz, use_phases, cycles)\n  self.jobs[self.cycle].append(job)\n  self.current_hkl = job.hklout\n  self.current_xyz = job.xyzout\n  return job", "target": 1, "info": "Null", "idx": 0}
{"func": " def refmac(self, cycles):\n  directory = self.job_directory(\"refmac\")\n  use_phases = self.args.unbiased and self.min_rwork > 0.35\n  job = Refmac(self.args, directory, self.current_xyz, cycles, use_phases)\n  self.jobs[self.cycle].append(job)\n  self.current_hkl = job.hklout\n  self.current_xyz = job.xyzout\n  return job", "target": 0, "info": "Null", "idx": 0}
{"func": " def refmac(self, cycles):\n  directory = self.job_directory(\"refmac\")\n  use_phases = self.args.unbiased and self.min_rwork > 0.35\n  job = Refmac(self.args, directory, self.current_xyz, use_phases, cycles)\n  self.jobs[self.cycle].append(job)\n  self.current_hkl = job.hklout\n  self.current_xyz = job.xyzout\n  return job", "target": 1, "info": "Null", "idx": 0}
{"func": "def _build_part(self, name, struct, minw, maxw, align):\n  def pad(string, shorten=None):\n  if maxw:\n  if len(string) > maxw:\n  if shorten:\n  string = shorten(string, maxw)\n  else:\n  string = string[:maxw]\n  if minw:\n  if len(string) < minw:\n  if align == 'left':\n  string = string.ljust(minw)\n  elif align == 'center':\n  string = string.center(minw)\n  else:\n  string = string.rjust(minw)\n  return string\n  part = None\n  width = None\n  if name == 'date':\n  newest = None\n  datestring = ''\n  if self.thread:\n  newest = self.thread.get_newest_date()\n  datestring = settings.represent_datetime(newest)\n  datestring = pad(datestring)\n  width = len(datestring)\n  part = AttrFlipWidget(urwid.Text(datestring), struct['date'])\n  elif name == 'mailcount':\n  if self.thread:\n  mailcountstring = \"(%d)\" % self.thread.get_total_messages()\n  else:\n  mailcountstring = \"(?)\"\n  mailcountstring = pad(mailcountstring)\n  width = len(mailcountstring)\n  mailcount_w = AttrFlipWidget(urwid.Text(mailcountstring),\n   struct['mailcount'])\n  part = mailcount_w\n  elif name == 'authors':\n  if self.thread:\n  authors = self.thread.get_authors_string() or '(None)'\n  else:\n  authors = '(None)'\n  authorsstring = pad(authors, shorten_author_string)\n  authors_w = AttrFlipWidget(urwid.Text(authorsstring),\n struct['authors'])\n  width = len(authorsstring)\n  part = authors_w\n  elif name == 'subject':\n  if self.thread:\n  subjectstring = self.thread.get_subject() or ' '\n  else:\n  subjectstring = ' '\n  subjectstring = subjectstring.replace('\\n', ' ')\n  subjectstring = subjectstring.replace('\\r', '')\n  subjectstring = pad(subjectstring)\n  subject_w = AttrFlipWidget(urwid.Text(subjectstring, wrap='clip'),\n struct['subject'])\n  if subjectstring:\n  width = len(subjectstring)\n  part = subject_w\n  elif name == 'content':\n  if self.thread:\n  msgs = self.thread.get_messages().keys()\n  else:\n  msgs = []\n  msgs.sort(key=lambda msg: msg.get_date(), reverse=True)\n  lastcontent = ' '.join([m.get_text_content() for m in msgs])\n  contentstring = pad(lastcontent.replace('\\n', ' ').strip())\n  content_w = AttrFlipWidget(urwid.Text(\n contentstring,\n wrap='clip'),\n struct['content'])\n  width = len(contentstring)\n  part = content_w\n  elif name == 'tags':\n  if self.thread:\n  fallback_normal = struct[name]['normal']\n  fallback_focus = struct[name]['focus']\n  tag_widgets = [TagWidget(t, fallback_normal, fallback_focus)\n for t in self.thread.get_tags()]\n  tag_widgets.sort(tag_cmp,\n   lambda tag_widget: tag_widget.translated)\n  else:\n  tag_widgets = []\n  cols = []\n  length = -1\n  for tag_widget in tag_widgets:\n  if not tag_widget.hidden:\n  wrapped_tagwidget = tag_widget\n  tag_width = tag_widget.width()\n  cols.append(('fixed', tag_width, wrapped_tagwidget))\n  length += tag_width + 1\n  if cols:\n  part = urwid.Columns(cols, dividechars=1)\n  width = length\n  return width, part", "target": 0, "info": "Null", "idx": 0}
{"func": "def _build_part(self, name, struct, minw, maxw, align):\n  def pad(string, shorten=None):\n  if maxw:\n  if len(string) > maxw:\n  if shorten:\n  string = shorten(string, maxw)\n  else:\n  string = string[:maxw]\n  if minw:\n  if len(string) < minw:\n  if align == 'left':\n  string = string.ljust(minw)\n  elif align == 'center':\n  string = string.center(minw)\n  else:\n  string = string.rjust(minw)\n  return string\n  part = None\n  width = None\n  if name == 'date':\n  newest = None\n  datestring = ''\n  if self.thread:\n  newest = self.thread.get_newest_date()\n  datestring = settings.represent_datetime(newest)\n  datestring = pad(datestring)\n  width = len(datestring)\n  part = AttrFlipWidget(urwid.Text(datestring), struct['date'])\n  elif name == 'mailcount':\n  if self.thread:\n  mailcountstring = \"(%d)\" % self.thread.get_total_messages()\n  else:\n  mailcountstring = \"(?)\"\n  datestring = pad(mailcountstring)\n  width = len(mailcountstring)\n  mailcount_w = AttrFlipWidget(urwid.Text(mailcountstring),\n   struct['mailcount'])\n  part = mailcount_w\n  elif name == 'authors':\n  if self.thread:\n  authors = self.thread.get_authors_string() or '(None)'\n  else:\n  authors = '(None)'\n  authorsstring = pad(authors, shorten_author_string)\n  authors_w = AttrFlipWidget(urwid.Text(authorsstring),\n struct['authors'])\n  width = len(authorsstring)\n  part = authors_w\n  elif name == 'subject':\n  if self.thread:\n  subjectstring = self.thread.get_subject() or ' '\n  else:\n  subjectstring = ' '\n  subjectstring = subjectstring.replace('\\n', ' ')\n  subjectstring = subjectstring.replace('\\r', '')\n  subjectstring = pad(subjectstring)\n  subject_w = AttrFlipWidget(urwid.Text(subjectstring, wrap='clip'),\n struct['subject'])\n  if subjectstring:\n  width = len(subjectstring)\n  part = subject_w\n  elif name == 'content':\n  if self.thread:\n  msgs = self.thread.get_messages().keys()\n  else:\n  msgs = []\n  msgs.sort(key=lambda msg: msg.get_date(), reverse=True)\n  lastcontent = ' '.join([m.get_text_content() for m in msgs])\n  contentstring = pad(lastcontent.replace('\\n', ' ').strip())\n  content_w = AttrFlipWidget(urwid.Text(\n contentstring,\n wrap='clip'),\n struct['content'])\n  width = len(contentstring)\n  part = content_w\n  elif name == 'tags':\n  if self.thread:\n  fallback_normal = struct[name]['normal']\n  fallback_focus = struct[name]['focus']\n  tag_widgets = [TagWidget(t, fallback_normal, fallback_focus)\n for t in self.thread.get_tags()]\n  tag_widgets.sort(tag_cmp,\n   lambda tag_widget: tag_widget.translated)\n  else:\n  tag_widgets = []\n  cols = []\n  length = -1\n  for tag_widget in tag_widgets:\n  if not tag_widget.hidden:\n  wrapped_tagwidget = tag_widget\n  tag_width = tag_widget.width()\n  cols.append(('fixed', tag_width, wrapped_tagwidget))\n  length += tag_width + 1\n  if cols:\n  part = urwid.Columns(cols, dividechars=1)\n  width = length\n  return width, part", "target": 1, "info": "Null", "idx": 0}
{"func": "def _parse(p, letter):\n  if p.find(letter) >= 0:\n  s, p = p.split(letter, 1)\n  s = s[1:] if s.startswith('+') else s\n  sgn, s = (-1, s[1:]) if s.startswith('-') else (1, s)\n  if not s.isdigit():\n  raise ValueError(\"Unable to parse %s in %s as %s\" % (s, p, cls.__name__))\n  return sgn * int(s), p\n  return 0, p", "target": 0, "info": "Null", "idx": 0}
{"func": "def _parse(p, letter):\n  if p.find(letter) > 0:\n  s, p = p.split(letter, 1)\n  s = s[1:] if s.startswith('+') else s\n  sgn, s = (-1, s[1:]) if s.startswith('-') else (1, s)\n  if not s.isdigit():\n  raise ValueError(\"Unable to parse %s in %s as %s\" % (s, p, cls.__name__))\n  return sgn * int(s), p\n  return 0, p", "target": 1, "info": "Null", "idx": 0}
{"func": "def as_xml(self):\n  slot_node = ElementTree.Element('slot')\n  ElementTree.SubElement(slot_node, 'slot:key').text = self.key\n  slot_value_node = ElementTree.SubElement(slot_node, 'slot:value', {'type': self.type})\n  if self.type == 'gdate':\n  ElementTree.SubElement(slot_value_node, 'gdate').text = datetime.strftime(self.value, '%Y-%m-%d')\n  elif self.type == 'string':\n  slot_value_node.text = self.value\n  elif self.type in ['integer', 'double']:\n  slot_value_node.text = str(self.value)\n  elif type(self.value) is list and self.value:\n  for sub_slot in self.value:\n  slot_value_node.append(sub_slot.as_xml)\n  elif self.type == 'frame':\n  pass\n  else:\n  raise NotImplementedError('Slot type {} is not implemented.'.format(self.type))\n  return slot_node", "target": 0, "info": "Null", "idx": 0}
{"func": "def as_xml(self):\n  slot_node = ElementTree.Element('slot')\n  ElementTree.SubElement(slot_node, 'slot:key').text = self.key\n  slot_value_node = ElementTree.SubElement(slot_node, 'slot:value', {'type': self.type})\n  if self.type == 'gdate':\n  ElementTree.SubElement(slot_value_node, 'gdate').text = datetime.strftime(self.value, '%Y-%m-%d')\n  elif self.type == 'string':\n  slot_value_node.text = self.value\n  elif self.type in ['integer', 'double']:\n  slot_value_node.text = str(self.value)\n  elif type(self.value) is list and self.value:\n  for sub_slot in self.value:\n  slot_node.append(sub_slot.as_xml)\n  elif self.type == 'frame':\n  pass\n  else:\n  raise NotImplementedError('Slot type {} is not implemented.'.format(self.type))\n  return slot_node", "target": 1, "info": "Null", "idx": 0}
{"func": "def sort_benchmarks_by_time(\n  cluster,\n  benchmark,\n  experiment):\n  items = []\n  for benchmark_idx in range(benchmark.worker.nr_benchmarks):\n  nr_workers = benchmark.worker.nr_workers(benchmark_idx)\n  benchmark_pathname = experiment.benchmark_result_pathname(\n  cluster.name, benchmark.scenario_name, nr_workers, \"json\")\n  assert os.path.exists(benchmark_pathname), benchmark_pathname\n  benchmark_json = json_to_data(benchmark_pathname)\n  benchmark_start = dateutil.parser.isoparse(benchmark_json[\"start\"])\n  items.append((benchmark_start, benchmark_idx))\n  assert len(items) > 0\n  items.sort(key=lambda item: item[0])\n  time_points = [item[0] for item in items]\n  idxs = [item[1] for item in items]\n   assert all(t1 <= t2 for t1, t2 in zip(time_points, time_points[1:])), time_points\n  epoch = time_points[0]\n   return idxs, epoch", "target": 0, "info": "Null", "idx": 0}
{"func": "def sort_benchmarks_by_time(\n  cluster,\n  benchmark,\n  experiment):\n  items = []\n  for benchmark_idx in range(benchmark.worker.nr_benchmarks):\n  nr_workers = benchmark.worker.nr_workers(benchmark_idx)\n  benchmark_pathname = experiment.benchmark_result_pathname(\n  cluster.name, benchmark.scenario_name, nr_workers, \"json\")\n  assert os.path.exists(benchmark_pathname), benchmark_pathname\n  benchmark_json = json_to_data(benchmark_pathname)\n  benchmark_start = dateutil.parser.isoparse(benchmark_json[\"start\"])\n  items.append((benchmark_start, benchmark_idx))\n  assert len(items) > 0\n  items.sort(key=lambda item: item[0])\n  time_points = [item[0] for item in items]\n  idxs = [item[1] for item in items]\n   assert all(t1 < t2 for t1, t2 in zip(time_points, time_points[1:])), time_points\n  epoch = time_points[0]\n   return idxs, epoch", "target": 1, "info": "Null", "idx": 0}
{"func": "def spider(add_link_dictionary):\n  link = add_link_dictionary['link']\n  ip = add_link_dictionary['ip']\n  port = add_link_dictionary['port']\n  proxy_user = add_link_dictionary['proxy_user']\n  proxy_passwd = add_link_dictionary['proxy_passwd']\n  download_user = add_link_dictionary['download_user']\n  download_passwd = add_link_dictionary['download_passwd']\n  header = add_link_dictionary['header']\n  out = add_link_dictionary['out']\n  user_agent = add_link_dictionary['user_agent']\n  raw_cookies = add_link_dictionary['load_cookies']\n  referer = add_link_dictionary['referer']\n  requests_session = requests.Session()\n   if ip:\n  ip_port = 'http://' + str(ip) + \":\" + str(port)\n  if proxy_user:\n  ip_port = 'http://' + proxy_user + ':' + proxy_passwd + '@' + ip_port\n  requests_session.proxies = {'http': ip_port}\n  if download_user:\n  requests_session.auth(download_user, download_passwd)\n  if raw_cookies:\n    cookie = SimpleCookie()\n  cookie.load(raw_cookies)\n  cookies = {key: morsel.value for key, morsel in cookie.items()}\n  requests_session.cookies = cookiejar_from_dict(cookies)\n  if referer:\n  requests_session.headers.update({'referer': referer })\n  if user_agent:\n  requests_session.headers.update({'user-agent':user_agent })\n  try:\n  response = requests_session.head(link)\n  header = response.headers\n  except:\n  header = {}\n  filename = None\n  filesize = None\n  if 'Content-Disposition' in header.keys():\n  content_disposition = header['Content-Disposition']\n  if content_disposition.find('filename') != -1:\n  filename_splited = content_disposition.split('filename=')\n  filename_splited = filename_splited[-1]\n  filename = filename_splited[1:-1]\n  if not(filename):\n  filename = link.split('/')[-1]\n  if out:\n  filename = out\n  if 'Content-Length' in header.keys():\n  file_size = int(header['Content-Length'])\n   file_size = humanReadbleSize(file_size)\n  return filename, file_size", "target": 0, "info": "Null", "idx": 0}
{"func": "def spider(add_link_dictionary):\n  link = add_link_dictionary['link']\n  ip = add_link_dictionary['ip']\n  port = add_link_dictionary['port']\n  proxy_user = add_link_dictionary['proxy_user']\n  proxy_passwd = add_link_dictionary['proxy_passwd']\n  download_user = add_link_dictionary['download_user']\n  download_passwd = add_link_dictionary['download_passwd']\n  header = add_link_dictionary['header']\n  out = add_link_dictionary['out']\n  user_agent = add_link_dictionary['user_agent']\n  raw_cookies = add_link_dictionary['load_cookies']\n  referer = add_link_dictionary['referer']\n  requests_session = requests.Session()\n   if ip:\n  ip_port = 'http://' + str(ip) + \":\" + str(port)\n  if proxy_user:\n  ip_port = 'http://' + proxy_user + ':' + proxy_passwd + '@' + ip_port\n  requests_session.proxies = {'http': ip_port}\n  if download_user:\n  requests_session.auth(download_user, download_passwd)\n  if raw_cookies:\n    cookie = SimpleCookie()\n  cookie.load(raw_cookies)\n  cookies = {key: morsel.value for key, morsel in cookie.items()}\n  requests_session.cookies = cookiejar_from_dict(cookies)\n  if referer:\n  requests_session.headers.update({'referer': referer })\n  if user_agent:\n  requests_session.headers.update({'user-agent':user_agent })\n  try:\n  response = requests_session.head(link)\n  header = response.headers\n  except:\n  header = {}\n  filename = None\n  filesize = None\n  if 'Content-Disposition' in header.keys():\n  content_disposition = header['Content-Disposition']\n  if content_disposition.find('filename') != -1:\n  filename_splited = content_disposition.split('filename=')\n  filename_splited = filename_splited[-1]\n  filename = filename_splited[1:-1]\n  if not(filename):\n  filename = link.split('/')[-1]\n  if out:\n  filename = out\n  if 'Content-Length' in header.keys():\n  file_size = int(header['Content-Length'])\n   file_size = humanReadbleSize(file_size)\n  return filename, filesize", "target": 1, "info": "Null", "idx": 0}
{"func": "  def check_tor(self):\n  return (self.tor is not None)", "target": 0, "info": "Null", "idx": 0}
{"func": "  def check_tor(self):\n  return (self.tor is None)", "target": 1, "info": "Null", "idx": 0}
{"func": "def __eq__(self, other):\n  if not isinstance(other, Individual):\n  return NotImplemented\n  try:\n  return self.score == other.score\n  except ValueError:\n  return NotImplemented", "target": 0, "info": "Null", "idx": 0}
{"func": "  def __eq__(self, other):\n  if not isinstance(other, Individual):\n  return NotImplemented\n  try:\n  return self.score < other.score\n  except ValueError:\n  return NotImplemented", "target": 1, "info": "Null", "idx": 0}
{"func": "def act_cz(self, a, b):\n  debug(\"before cphase between {} and {}\".format(a, b))\n  self.print_adj_list_line(a)\n  self.print_adj_list_line(b)\n  ci = self.get_connection_info(a, b)\n  if ci[\"non1\"]:\n  debug(\"cphase: left vertex has NONs -> putting it to Id\")\n  self.remove_vop(a, b)\n  ci = self.get_connection_info(a, b)\n  if ci[\"non2\"]:\n  debug(\"cphase: right vertex has NONs -> putting it to Id\")\n  self.remove_vop(b, a)\n  ci = self.get_connection_info(a, b)\n  if ci[\"non1\"] and not clifford.is_diagonal(self.node[a][\"vop\"]):\n  debug(\"cphase: left one needs treatment again -> putting it to Id\")\n  self.remove_vop(a, b)\n   self.cz_with_table(a, b)", "target": 0, "info": "Null", "idx": 0}
{"func": " def act_cz(self, a, b):\n  debug(\"before cphase between {} and {}\".format(a, b))\n  self.print_adj_list_line(a)\n  self.print_adj_list_line(b)\n  ci = self.get_connection_info(a, b)\n  if ci[\"non1\"]:\n  debug(\"cphase: left vertex has NONs -> putting it to Id\")\n  self.remove_vop(a, b)\n  ci = self.get_connection_info(a, b)\n  if ci[\"non2\"]:\n  debug(\"cphase: right vertex has NONs -> putting it to Id\")\n  self.remove_vop(b, a)\n  ci = self.get_connection_info(a, b)\n  if ci[\"non1\"] and not clifford.is_diagonal(self.node[a][\"vop\"]):\n  debug(\"cphase: left one needs treatment again -> putting it to Id\")\n  self.remove_vop(b, a)\n   self.cz_with_table(a, b)", "target": 1, "info": "Null", "idx": 0}
{"func": "def act_cz(self, control, target):\n  control = 1 << control\n  target = 1 << target\n  for i in xrange(self.d):\n  if (i & control) and (i & target):\n  self.state[i, 0] *= -1", "target": 0, "info": "Null", "idx": 0}
{"func": "def act_cz(self, control, target):\n  control = 1 << control\n  target = 1 << control\n  for i in xrange(self.d):\n  if (i & control) and (i & target):\n  self.state[i, 0] *= -1", "target": 1, "info": "Null", "idx": 0}
{"func": "def loop_single_entry(cfg):\n  for v, _ in cfg.iter_nodes():\n  if cfg.degree_in(v) >= 2:\n  preds = cfg.pred(v)\n  back_preds = list(filter(lambda x: v <= x, preds))\n  if len(back_preds) < 2:\n  continue\n  print(\"loop_single_entry: node:\", v)\n  print(\"back_preds:\", back_preds)\n  back_jumps = list(filter(lambda x: cfg.degree_out(x) == 1, back_preds))\n  print(\"back_jumps:\", back_jumps)\n  landing_site = None\n  for p in back_jumps:\n  b = cfg.node(p)\n  if not b.items:\n  landing_site = p\n  if not landing_site:\n  farthest = max(back_preds)\n  print(\"farthest\", farthest)\n  newb = BBlock(farthest + \"_1\")\n  cfg.add_node(newb.addr, newb)\n  cfg.add_edge(newb.addr, v)\n  landing_site = newb.addr\n  print(\"landing_site:\", landing_site)\n  for p in back_preds:\n  if p != landing_site:\n  e = cfg.edge(p, v)\n  cfg.remove_edge(p, v)\n  cfg.add_edge(p, landing_site, e)\n  return True", "target": 0, "info": "Null", "idx": 0}
{"func": "def loop_single_entry(cfg):\n  for v, _ in cfg.iter_nodes():\n  if cfg.degree_in(v) >= 2:\n  preds = cfg.pred(v)\n  back_preds = list(filter(lambda x: v <= x, preds))\n  if len(back_preds) < 2:\n  continue\n  print(\"loop_single_entry: node:\", v)\n  print(\"back_preds:\", back_preds)\n  back_jumps = list(filter(lambda x: cfg.degree_out(x) == 1, back_preds))\n  print(\"back_jumps:\", back_jumps)\n  landing_site = None\n  for p in back_jumps:\n  b = cfg.node(p)\n  if not b.items:\n  landing_site = p\n  if not landing_site:\n  farthest = max(back_jumps)\n  print(\"farthest\", farthest)\n  newb = BBlock(farthest + \"_1\")\n  cfg.add_node(newb.addr, newb)\n  cfg.add_edge(newb.addr, v)\n  landing_site = newb.addr\n  print(\"landing_site:\", landing_site)\n  for p in back_preds:\n  if p != landing_site:\n  e = cfg.edge(p, v)\n  cfg.remove_edge(p, v)\n  cfg.add_edge(p, landing_site, e)\n  return True", "target": 1, "info": "Null", "idx": 0}
{"func": "def compare_predictions(df, y_var_name, percent_data=None,\n  category_limit=11, knots=3,\n  alphas=np.logspace(start=-2, stop=5, num=50),\n  corr_matrix=True,\n  scatter_matrix=True, bootstrap_coefs=True,\n  feature_importances=True,\n  partial_dep=True, actual_vs_predicted=True,\n  residuals=True, univariates=True, compare_models=True,\n  ROC=True, bootstraps=10):\n  starttotal = time()\n  df, sample_limit = clean_dataframe(df, y_var_name, percent_data)\n  df_unpiped, df_X_unpiped = df.copy(), df.copy().drop(y_var_name, axis=1)\n  (unpiped_continuous_features,\n   unpiped_category_features) = sort_features(df_X_unpiped)\n  columns_unpiped = df_X_unpiped.columns\n  df = cleandata.drop_category_exeeding_limit(df, y_var_name, category_limit)\n  if corr_matrix:\n  if len(unpiped_continuous_features) > 0:\n  timeit(plt.matshow, df.sample(sample_limit).corr())\n  if scatter_matrix:\n  if len(unpiped_continuous_features) > 0:\n  timeit(plot_scatter_matrix, df, y_var_name, colors=True)\n  plt.show()\n  print('DF COLUMNS: \\n' + str(list(df.columns)) + '\\n')\n  df, df_X, X, y, pipeline = use_spline(df, y_var_name)\n  print('DF COLUMNS AFTER TRANSFORM: \\n' + str(list(df.columns)) + '\\n')\n  (names_models, continuous_features,\n   category_features, models, scoring,\n   is_continuous, alphas) = make_models(df, df_X, y, y_var_name,\n    univariates, alphas)\n  fit_models, results, names, seed = [], [], [], 7\n  for name, model in tqdm.tqdm(names_models):\n  kfold = model_selection.KFold(n_splits=10, random_state=seed)\n  if name == 'RR' or name == 'LASSO':\n  alpha, cv_results = timeit(plot_choose_alpha, df, model,\n y_var_name, alphas, kfold, scoring)\n  model = model(alpha)\n  else:\n  cv_results = timeit(cross_val_score, model, X, y,\n  cv=kfold, scoring=scoring)\n  results.append(cv_results)\n  names.append(name)\n  msg = \"%s: mean=%f std=%f\" % (name, cv_results.mean(),\n    cv_results.std())\n  print(msg)\n  model.fit(X, y)\n  fit_models.append(model)\n  if is_continuous:\n  timeit(plot_predicted_vs_actuals, df,\n model, y_var_name, sample_limit)\n  plt.show()\n  if bootstrap_coefs or partial_dep:\n  bootstrap_models = bootstrap_train_premade(model, X, y,\n bootstraps=bootstraps,\n fit_intercept=False)\n  if hasattr(model, \"coef_\"):\n  coefs = model.coef_\n  columns = list(df.drop(y_var_name, axis=1).columns)\n  while (type(coefs[0]) is list) or (type(coefs[0]) is np.ndarray):\n  coefs = list(coefs[0])\n  timeit(plot_coefs, coefs=coefs, columns=columns, graph_name=name)\n  plt.show()\n  if is_continuous:\n  if bootstrap_coefs:\n  fig, axs = timeit(plot_bootstrap_coefs, bootstrap_models,\n    df_X.columns, n_col=4)\n  fig.tight_layout()\n  plt.show()\n  if feature_importances:\n  if 'feature_importances_' in dir(model):\n  timeit(plot_feature_importances, model, df_X)\n  plt.show()\n  if partial_dep:\n  timeit(plot_partial_dependences, model, X=df_X_unpiped,\n var_names=unpiped_continuous_features, y=y,\n bootstrap_models=bootstrap_models, pipeline=pipeline,\n n_points=250)\n  plt.tight_layout()\n  plt.show()\n  plot_continuous_error_graphs(df, y, y_var_name, model,\n   is_continuous,\n   sample_limit,\n   predicteds_vs_actuals=True,\n   residuals=True)\n  df_X = df.drop(y_var_name, axis=1)\n  y_hat = get_error(name, model, df_X, y, is_continuous)\n  if compare_models:\n  choose_box_and_violin_plots(names,\n  scoring,\n  compare_models,\n  results,\n  is_continuous)\n  if ROC:\n  if not is_continuous:\n  timeit(plot_rocs, models, df_X, y)\n  plt.show()\n  print(f'MAKE SUBSAMPLE TIME: {time() - starttotal}')\n  return names, results, fit_models, pipeline, df_X", "target": 0, "info": "Null", "idx": 0}
{"func": "def compare_predictions(df, y_var_name, percent_data=None,\n  category_limit=11, knots=3,\n  alphas=np.logspace(start=-2, stop=5, num=50),\n  corr_matrix=True,\n  scatter_matrix=True, bootstrap_coefs=True,\n  feature_importances=True,\n  partial_dep=True, actual_vs_predicted=True,\n  residuals=True, univariates=True, compare_models=True,\n  ROC=True, bootstraps=10):\n  starttotal = time()\n  df, sample_limit = clean_dataframe(df, y_var_name, percent_data)\n  df_unpiped, df_X_unpiped = df.copy(), df.copy().drop(y_var_name, axis=1)\n  (unpiped_continuous_features,\n   unpiped_category_features) = sort_features(df_X_unpiped)\n  columns_unpiped = df_X_unpiped.columns\n  df = cleandata.drop_category_exeeding_limit(df, y_var_name, category_limit)\n  if corr_matrix:\n  if len(unpiped_continuous_features) > 0:\n  timeit(plt.matshow, df.sample(sample_limit).corr())\n  if scatter_matrix:\n  if len(unpiped_continuous_features) > 0:\n  timeit(plot_scatter_matrix, df, y_var_name, colors=True)\n  plt.show()\n  print('DF COLUMNS: \\n' + str(list(df.columns)) + '\\n')\n  df, df_X, X, y, pipeline = use_spline(df, y_var_name)\n  print('DF COLUMNS AFTER TRANSFORM: \\n' + str(list(df.columns)) + '\\n')\n  (names_models, continuous_features,\n   category_features, models, scoring,\n   is_continuous, alphas) = make_models(df, df_X, y, y_var_name,\n    univariates, alphas)\n  fit_models, results, names, seed = [], [], [], 7\n  for name, model in tqdm.tqdm(names_models):\n  kfold = model_selection.KFold(n_splits=10, random_state=seed)\n  if name == 'RR' or name == 'LASSO':\n  alpha, cv_results = timeit(plot_choose_alpha, df, model,\n y_var_name, alphas, kfold, scoring)\n  model = model(alpha)\n  else:\n  cv_results = timeit(cross_val_score, model, X, y,\n  cv=kfold, scoring=scoring)\n  results.append(cv_results)\n  names.append(name)\n  msg = \"%s: mean=%f std=%f\" % (name, cv_results.mean(),\n    cv_results.std())\n  print(msg)\n  model.fit(X, y)\n  fit_models.append(model)\n  if is_continuous:\n  timeit(plot_predicted_vs_actuals, df,\n model, y_var_name, sample_limit)\n  plt.show()\n  if bootstrap_coefs or partial_dep:\n  bootstrap_models = bootstrap_train_premade(model, X, y,\n bootstraps=bootstraps,\n fit_intercept=False)\n  if hasattr(model, \"coef_\"):\n  coefs = model.coef_\n  columns = list(df.drop(y_var_name, axis=1).columns)\n  while (type(coefs[0]) is list) or (type(coefs[0]) is np.ndarray):\n  coefs = list(coefs[0])\n  timeit(plot_coefs, coefs=coefs, columns=columns, graph_name=name)\n  plt.show()\n  if is_continuous:\n  if bootstrap_coefs:\n  fig, axs = timeit(plot_bootstrap_coefs, bootstrap_models,\n    df_X.columns, n_col=4)\n  fig.tight_layout()\n  plt.show()\n  if feature_importances:\n  if 'feature_importances_' in dir(model):\n  timeit(plot_feature_importances, model, df_X)\n  plt.show()\n  if partial_dep:\n  timeit(plot_partial_dependences, model, X=df_X_unpiped,\n var_names=unpiped_continuous_features, y=y,\n bootstrap_models=bootstrap_models, pipeline=pipeline,\n n_points=250)\n  plt.tight_layout()\n  plt.show()\n  plot_continuous_error_graphs(df, y, y_var_name, model,\n   is_continuous,\n   sample_limit,\n   predicteds_vs_actuals=True,\n   residuals=True)\n  df_X = df.drop(y_var_name, axis=1)\n  y_hat = get_error(name, model, df_X, y, is_continuous)\n  if compare_models:\n  choose_box_and_violin_plots(names,\n  scoring,\n  compare_models,\n  results,\n  is_continuous)\n  if ROC:\n  if not is_continuous:\n  timeit(plot_rocs, models, df_X, y)\n  plt.show()\n  print(f'MAKE SUBSAMPLE TIME: {time() - starttotal}')\n  return names, results, models, pipeline, df_X", "target": 1, "info": "Null", "idx": 0}
{"func": "def compare_predictions(df, y_var_name, percent_data=None,\n  category_limit=11, knots=3,\n  alphas=np.logspace(start=-2, stop=5, num=50),\n  corr_matrix=True,\n  scatter_matrix=True, bootstrap_coefs=True,\n  feature_importances=True,\n  partial_dep=True, actual_vs_predicted=True,\n  residuals=True, univariates=True, compare_models=True,\n  ROC=True, bootstraps=10):\n  starttotal = time()\n  df, sample_limit = clean_dataframe(df, y_var_name, percent_data)\n  df_unpiped, df_X_unpiped = df.copy(), df.copy().drop(y_var_name, axis=1)\n  (unpiped_continuous_features,\n   unpiped_category_features) = sort_features(df_X_unpiped)\n  columns_unpiped = df_X_unpiped.columns\n  df = cleandata.drop_category_exeeding_limit(df, y_var_name, category_limit)\n  if corr_matrix:\n  if len(unpiped_continuous_features) > 0:\n  timeit(plt.matshow, df.sample(sample_limit).corr())\n  if scatter_matrix:\n  if len(unpiped_continuous_features) > 0:\n  timeit(plot_scatter_matrix, df, y_var_name, colors=True)\n  plt.show()\n  print('DF COLUMNS: \\n' + str(list(df.columns)) + '\\n')\n  df, df_X, X, y, pipeline = use_spline(df, y_var_name)\n  print('DF COLUMNS AFTER TRANSFORM: \\n' + str(list(df.columns)) + '\\n')\n  (names_models, continuous_features,\n   category_features, models, scoring,\n   is_continuous, alphas) = make_models(df, df_X, y, y_var_name,\n    univariates, alphas)\n  fit_models, results, names, seed = [], [], [], 7\n  for name, model in tqdm.tqdm(names_models):\n  kfold = model_selection.KFold(n_splits=10, random_state=seed)\n  if name == 'RR' or name == 'LASSO':\n  alpha, cv_results = timeit(plot_choose_alpha, df, model,\n y_var_name, alphas, kfold, scoring)\n  model = model(alpha)\n  else:\n  cv_results = timeit(cross_val_score, model, X, y,\n  cv=kfold, scoring=scoring)\n  results.append(cv_results)\n  names.append(name)\n  msg = \"%s: mean=%f std=%f\" % (name, cv_results.mean(),\n    cv_results.std())\n  print(msg)\n  model.fit(X, y)\n  fit_models.append(model)\n  if is_continuous:\n  timeit(plot_predicted_vs_actuals, df,\n model, y_var_name, sample_limit)\n  plt.show()\n  if bootstrap_coefs or partial_dep:\n  bootstrap_models = bootstrap_train_premade(model, X, y,\n bootstraps=bootstraps,\n fit_intercept=False)\n  if hasattr(model, \"coef_\"):\n  coefs = model.coef_\n  columns = list(df.drop(y_var_name, axis=1).columns)\n  while (type(coefs[0]) is list) or (type(coefs[0]) is np.ndarray):\n  coefs = list(coefs[0])\n  timeit(plot_coefs, coefs=coefs, columns=columns, graph_name=name)\n  plt.show()\n  if is_continuous:\n  if bootstrap_coefs:\n  fig, axs = timeit(plot_bootstrap_coefs, bootstrap_models,\n    df_X.columns, n_col=4)\n  fig.tight_layout()\n  plt.show()\n  if feature_importances:\n  if 'feature_importances_' in dir(model):\n  timeit(plot_feature_importances, model, df_X)\n  plt.show()\n  if partial_dep:\n  timeit(plot_partial_dependences, model, X=df_X_unpiped,\n var_names=unpiped_continuous_features, y=y,\n bootstrap_models=bootstrap_models, pipeline=pipeline,\n n_points=250)\n  plt.tight_layout()\n  plt.show()\n  plot_continuous_error_graphs(df, y, y_var_name, model,\n   is_continuous,\n   sample_limit,\n   predicteds_vs_actuals=True,\n   residuals=True)\n  df_X = df.drop(y_var_name, axis=1)\n  y_hat = get_error(name, model, df_X, y, is_continuous)\n  if compare_models:\n  choose_box_and_violin_plots(names,\n  scoring,\n  compare_models,\n  results,\n  is_continuous)\n  if ROC:\n  if not is_continuous:\n  timeit(plot_rocs, models, df_X, y)\n  plt.show()\n  print(f'MAKE SUBSAMPLE TIME: {time() - starttotal}')\n  return names, results, fit_models, pipeline, df_X", "target": 0, "info": "Null", "idx": 0}
{"func": "def compare_predictions(df, y_var_name, percent_data=None,\n  category_limit=11, knots=3,\n  alphas=np.logspace(start=-2, stop=5, num=50),\n  corr_matrix=True,\n  scatter_matrix=True, bootstrap_coefs=True,\n  feature_importances=True,\n  partial_dep=True, actual_vs_predicted=True,\n  residuals=True, univariates=True, compare_models=True,\n  ROC=True, bootstraps=10):\n  starttotal = time()\n  df, sample_limit = clean_dataframe(df, y_var_name, percent_data)\n  df_unpiped, df_X_unpiped = df.copy(), df.copy().drop(y_var_name, axis=1)\n  (unpiped_continuous_features,\n   unpiped_category_features) = sort_features(df_X_unpiped)\n  columns_unpiped = df_X_unpiped.columns\n  df = cleandata.drop_category_exeeding_limit(df, y_var_name, category_limit)\n  if corr_matrix:\n  if len(unpiped_continuous_features) > 0:\n  timeit(plt.matshow, df.sample(sample_limit).corr())\n  if scatter_matrix:\n  if len(unpiped_continuous_features) > 0:\n  timeit(plot_scatter_matrix, df, y_var_name, colors=True)\n  plt.show()\n  print('DF COLUMNS: \\n' + str(list(df.columns)) + '\\n')\n  df, df_X, X, y, pipeline = use_spline(df, y_var_name)\n  print('DF COLUMNS AFTER TRANSFORM: \\n' + str(list(df.columns)) + '\\n')\n  (names_models, continuous_features,\n   category_features, models, scoring,\n   is_continuous, alphas) = make_models(df, df_X, y, y_var_name,\n    univariates, alphas)\n  fit_models, results, names, seed = [], [], [], 7\n  for name, model in tqdm.tqdm(names_models):\n  kfold = model_selection.KFold(n_splits=10, random_state=seed)\n  if name == 'RR' or name == 'LASSO':\n  alpha, cv_results = timeit(plot_choose_alpha, df, model,\n y_var_name, alphas, kfold, scoring)\n  model = model(alpha)\n  else:\n  cv_results = timeit(cross_val_score, model, X, y,\n  cv=kfold, scoring=scoring)\n  results.append(cv_results)\n  names.append(name)\n  msg = \"%s: mean=%f std=%f\" % (name, cv_results.mean(),\n    cv_results.std())\n  print(msg)\n  model.fit(X, y)\n  fit_models.append(model)\n  if is_continuous:\n  timeit(plot_predicted_vs_actuals, df,\n model, y_var_name, sample_limit)\n  plt.show()\n  if bootstrap_coefs or partial_dep:\n  bootstrap_models = bootstrap_train_premade(model, X, y,\n bootstraps=bootstraps,\n fit_intercept=False)\n  if hasattr(model, \"coef_\"):\n  coefs = model.coef_\n  columns = list(df.drop(y_var_name, axis=1).columns)\n  while (type(coefs[0]) is list) or (type(coefs[0]) is np.ndarray):\n  coefs = list(coefs[0])\n  timeit(plot_coefs, coefs=coefs, columns=columns, graph_name=name)\n  plt.show()\n  if is_continuous:\n  if bootstrap_coefs:\n  fig, axs = timeit(plot_bootstrap_coefs, bootstrap_models,\n    df_X.columns, n_col=4)\n  fig.tight_layout()\n  plt.show()\n  if feature_importances:\n  if 'feature_importances_' in dir(model):\n  timeit(plot_feature_importances, model, df_X)\n  plt.show()\n  if partial_dep:\n  timeit(plot_partial_dependences, model, X=df_X_unpiped,\n var_names=unpiped_continuous_features, y=y,\n bootstrap_models=bootstrap_models, pipeline=pipeline,\n n_points=250)\n  plt.tight_layout()\n  plt.show()\n  plot_continuous_error_graphs(df, y, y_var_name, model,\n   is_continuous,\n   sample_limit,\n   predicteds_vs_actuals=True,\n   residuals=True)\n  df_X = df.drop(y_var_name, axis=1)\n  y_hat = get_error(name, model, df_X, y, is_continuous)\n  if compare_models:\n  choose_box_and_violin_plots(names,\n  scoring,\n  compare_models,\n  results,\n  is_continuous)\n  if ROC:\n  if not is_continuous:\n  timeit(plot_rocs, models, df_X, y)\n  plt.show()\n  print(f'MAKE SUBSAMPLE TIME: {time() - starttotal}')\n  return names, results, models, pipeline, df_X", "target": 1, "info": "Null", "idx": 0}
{"func": " def _type_variants(self):\n  self.out_json[self.sample][\"variant_calls\"] = {}\n  gt = VariantTyper(\n  expected_depths=self.expected_depths,\n  error_rate=self.expected_error_rate,\n  contamination_depths=self.contamination_depths,\n  ignore_filtered=self.ignore_filtered,\n  minor_freq=self.minor_freq,\n  confidence_threshold=self.variant_confidence_threshold)\n  genotypes = []\n  filters = []\n  for probe_name, probe_coverages in self.variant_covgs.items():\n  probe_id = self._name_to_id(probe_name)\n  variant = None\n  call = gt.type(probe_coverages, variant=probe_name)\n  genotypes.append(sum(call[\"genotype\"]))\n  filters.append(int(call[\"info\"][\"filter\"] == \"PASS\"))\n  if sum(call[\"genotype\"]) > 0 or not call[\n  \"genotype\"] or self.report_all_calls:\n  self.variant_calls[probe_name] = call\n  self.variant_calls_dict[\n  probe_id] = call\n  self.out_json[self.sample][\"genotypes\"] = genotypes\n  self.out_json[self.sample][\"filtered\"] = filters\n  self.out_json[self.sample][\"variant_calls\"] = self.variant_calls_dict", "target": 0, "info": "Null", "idx": 0}
{"func": "def _type_variants(self):\n  self.out_json[self.sample][\"variant_calls\"] = {}\n  gt = VariantTyper(\n  expected_depths=self.expected_depths,\n  error_rate=self.expected_error_rate,\n  contamination_depths=self.contamination_depths,\n  ignore_filtered=self.ignore_filtered,\n  minor_freq=self.minor_freq,\n  confidence_threshold=self.variant_confidence_threshold)\n  genotypes = []\n  filters = []\n  for probe_name, probe_coverages in self.variant_covgs.items():\n  probe_id = self._name_to_id(probe_name)\n  variant = None\n  call = gt.type(probe_coverages, variant=variant)\n  genotypes.append(sum(call[\"genotype\"]))\n  filters.append(int(call[\"info\"][\"filter\"] == \"PASS\"))\n  if sum(call[\"genotype\"]) > 0 or not call[\n  \"genotype\"] or self.report_all_calls:\n  self.variant_calls[probe_name] = call\n  self.variant_calls_dict[\n  probe_id] = call\n  self.out_json[self.sample][\"genotypes\"] = genotypes\n  self.out_json[self.sample][\"filtered\"] = filters\n  self.out_json[self.sample][\"variant_calls\"] = self.variant_calls_dict", "target": 1, "info": "Null", "idx": 0}
{"func": "def build(bloomfilter_filepaths, samples, graph):\n  bloomfilters = []\n  for f in bloomfilter_filepaths:\n  bloomfilters.append(load_bloomfilter(f))\n  graph.build(bloomfilters, samples)\n  return {'result': 'success'}", "target": 0, "info": "Null", "idx": 0}
{"func": "def build(bloomfilter_filepaths, samples, graph):\n  bloomfilters = []\n  for f in bloomfilter_filepaths:\n  bloomfilters.append(load_bloomfilter(f))\n  graph.build(bloomfilter_filepaths, samples)\n  return {'result': 'success'}", "target": 1, "info": "Null", "idx": 0}
{"func": "def search(self, seq: hug.types.text=None, fasta_file: hug.types.text=None, threshold: hug.types.float_number=1.0):\n  if not seq or not fasta_file:\n  return \"-s or -f must be provided\"\n  return search(seq=seq,\n    fasta_file=fasta_file, threshold=threshold, conn_config=CONN_CONFIG)", "target": 0, "info": "Null", "idx": 0}
{"func": " def search(self, seq: hug.types.text=None, fasta_file: hug.types.text=None, threshold: hug.types.float_number=1.0):\n  if not seq or fasta_file:\n  return \"-s or -f must be provided\"\n  return search(seq=seq,\n    fasta_file=fasta_file, threshold=threshold, conn_config=CONN_CONFIG)", "target": 1, "info": "Null", "idx": 0}
{"func": "def run(parser, args, conn_config):\n  if args.sample_name is None:\n  args.sample_name = os.path.basename(args.kmer_file).split('.')[0]\n  mc = McDBG(conn_config=conn_config)\n  try:\n  colour = mc.add_sample(args.sample_name)\n  with open(args.kmer_file, 'r') as inf:\n  kmers = []\n  for i, line in enumerate(inf):\n  kmer = line.strip()\n  kmers.append(kmer)\n  if i % 100000 == 0:\n  mc.set_kmers(kmers, colour)\n  kmers = []\n  mc.set_kmers(kmers, colour)\n   logger.info(\"%i\\t%i\\t%i\" %\n  (i, mc.count_kmers(), mc.calculate_memory()))\n  print(json.dumps({\"result\": \"success\", \"colour\": colour, \"kmers\": mc.count_kmers(\n  ), \"memory\": mc.calculate_memory()}))\n  except ValueError as e:\n  print(json.dumps({\"result\": \"failed\", \"message\": str(e), \"kmers\": mc.count_kmers(\n  ), \"memory\": mc.calculate_memory()}))", "target": 0, "info": "Null", "idx": 0}
{"func": "def run(parser, args, conn_config):\n  if args.sample_name is None:\n  args.sample_name = os.path.basename(args.kmer_file).split('.')[0]\n  mc = McDBG(conn_config=conn_config)\n  try:\n  colour = mc.add_sample(args.sample_name)\n  with open(args.kmer_file, 'r') as inf:\n  kmers = []\n  for i, line in enumerate(inf):\n  kmer = line.strip()\n  kmers.append(kmer)\n  if i % 100000 == 0:\n  mc.set_kmers(kmers, colour)\n  kmers = []\n  mc.set_kmers(kmers, i)\n   logger.info(\"%i\\t%i\\t%i\" %\n  (i, mc.count_kmers(), mc.calculate_memory()))\n  print(json.dumps({\"result\": \"success\", \"colour\": colour, \"kmers\": mc.count_kmers(\n  ), \"memory\": mc.calculate_memory()}))\n  except ValueError as e:\n  print(json.dumps({\"result\": \"failed\", \"message\": str(e), \"kmers\": mc.count_kmers(\n  ), \"memory\": mc.calculate_memory()}))", "target": 1, "info": "Null", "idx": 0}
{"func": "def fit(data_x, data_y, sigma_x=None, sigma_y=None, func=None, beta=[1., 0.], *args, **kwargs):\n  if func == None:\n  func = lambda p,x: p[0]*x+p[1]\n   if type(data_x[0]) in ucvar:\n  values_x = [d.n for d in data_x]\n  sigma_x = [d.s if d.s!=0 else 1e-5 for d in data_x]\n  elif type(data_x[0]) in [float, int]:\n  values_x = data_x\n   if type(data_y[0]) in ucvar:\n  values_y = [d.n for d in data_y]\n  sigma_y = [d.s if d.s!=0 else 1e-5 for d in data_y]\n  elif type(data_y[0]) in [float, int]:\n  values_y = data_y\n  data = odr.RealData(values_x, values_y, sx=sigma_x, sy=sigma_y)\n  model = odr.Model(func)\n  odrfit = odr.ODR(data, model, beta0=beta)\n  out = odrfit.run()\n  return [ufloat(n, s) for n, s in zip(out.beta, out.sd_beta)]", "target": 0, "info": "Null", "idx": 0}
{"func": "def fit(data_x, data_y, sigma_x=None, sigma_y=None, func=None, beta=[1., 0.], *args, **kwargs):\n  if func == None:\n  func = lambda p,x: p[0]*x+p[1]\n   if type(data_x[0]) in ucvar:\n  values_x = [d.n for d in data_x]\n  sigma_x = [d.s if d.s!=0 else 1e-5 for d in data_y]\n  elif type(data_x[0]) in [float, int]:\n  values_x = data_x\n   if type(data_y[0]) in ucvar:\n  values_y = [d.n for d in data_y]\n  sigma_y = [d.s if d.s!=0 else 1e-5 for d in data_y]\n  elif type(data_y[0]) in [float, int]:\n  values_y = data_y\n  data = odr.RealData(values_x, values_y, sx=sigma_x, sy=sigma_y)\n  model = odr.Model(func)\n  odrfit = odr.ODR(data, model, beta0=beta)\n  out = odrfit.run()\n  return [ufloat(n, s) for n, s in zip(out.beta, out.sd_beta)]", "target": 1, "info": "Null", "idx": 0}
{"func": "def upgrade(ctx):\n  local_packages = ctx.obj.get('packages', {}).items()\n  remote_packages = map(get_package, map(operator.itemgetter(0), local_packages))\n  remote_package_versions = map(lambda x: x and x['Version'], remote_packages)\n  for (n, lv), rv in zip(local_packages, remote_package_versions):\n  if rv is None:\n  print('failed to find {} on server'.format(n))\n  continue\n  elif lv != rv:\n  print('found new version {} (old: {}) for {}, do you want to upgrade? [Y/n] '.format(rv, lv, n), end='')\n  sys.stdout.flush()\n  answer = sys.stdin.readline().strip()\n  if answer in ('', ' ', 'Y', 'y'):\n  print('upgrading {} from {} to {}'.format(n, lv, rv))\n  install(ctx, n)\n  else:\n  print('skipping upgrading {} from {} to {}'.format(n, lv, rv))\n  else:\n  print('{} is up to date'.format(n))\n  print('done')", "target": 0, "info": "Null", "idx": 0}
{"func": "def upgrade(ctx):\n  local_packages = ctx.obj.get('packages', {}).items()\n  remote_packages = map(get_package, map(operator.itemgetter(0), local_packages))\n  remote_package_versions = map(lambda x: x and x['Version'], remote_packages)\n  for (n, lv), rv in zip(local_packages, remote_package_versions):\n  if rv is None:\n  print('failed to find {} on server'.format(n))\n  continue\n  elif lv != rv:\n  print('found new version {} (old: {}) for {}, do you want to upgrade? [Y/n] '.format(lv, rv, n), end='')\n  sys.stdout.flush()\n  answer = sys.stdin.readline().strip()\n  if answer in ('', ' ', 'Y', 'y'):\n  print('upgrading {} from {} to {}'.format(n, lv, rv))\n  install(ctx, n)\n  else:\n  print('skipping upgrading {} from {} to {}'.format(n, lv, rv))\n  else:\n  print('{} is up to date'.format(n))\n  print('done')", "target": 1, "info": "Null", "idx": 0}
{"func": " def __init__(self, model, nb_actions, policy=None, test_policy=None, gamma=.99, nb_steps_warmup=10,\n   train_interval=1, delta_clip=np.inf, *args, **kwargs):\n  super(SarsaAgent, self).__init__(*args, **kwargs)\n  if policy is None:\n  policy = EpsGreedyQPolicy()\n  if test_policy is None:\n  test_policy = GreedyQPolicy()\n  self.model = model\n  self.nb_actions = nb_actions\n  self.policy = policy\n  self.test_policy = test_policy\n  self.gamma = gamma\n  self.nb_steps_warmup = nb_steps_warmup\n  self.train_interval = train_interval\n  self.delta_clip = delta_clip\n  self.compiled = False\n  self.actions = None\n  self.observations = None\n  self.rewards = None", "target": 0, "info": "Null", "idx": 0}
{"func": "def __init__(self, model, nb_actions, policy=None, test_policy=None, gamma=.99, nb_steps_warmup=10,\n   train_interval=1, delta_clip=np.inf, *args, **kwargs):\n  super(SarsaAgent, self).__init__(*args, **kwargs)\n  if policy is None:\n  policy = EpsGreedyQPolicy()\n  if test_policy is None:\n  test_policy = GreedyQPolicy()\n  self.model = model\n  self.nb_actions = nb_actions\n  self.policy = policy\n  self.test_policy = policy\n  self.gamma = gamma\n  self.nb_steps_warmup = nb_steps_warmup\n  self.train_interval = train_interval\n  self.delta_clip = delta_clip\n  self.compiled = False\n  self.actions = None\n  self.observations = None\n  self.rewards = None", "target": 1, "info": "Null", "idx": 0}
{"func": "def test_boxplot_melted():\n  return ar.boxplot(data.iris(), \"petalLength\", \"species\")", "target": 0, "info": "Null", "idx": 0}
{"func": "def test_boxplot_melted():\n  return ar.boxplot(data.iris(), \"species\", \"petalLength\")", "target": 1, "info": "Null", "idx": 0}
{"func": "def tag_detail(request, slug, page=0, **kwargs):\n  tag = get_object_or_404(Tag, slug__iexact=slug)\n   return list_detail.object_list(\n  request,\n  queryset=Post.objects.filter(tags__name__in=[tag]),\n  paginate_by=blog_settings.BLOG_PAGESIZE,\n  page=page,\n  extra_context={\n  'type': 'tag',\n  'query': tag.id,\n  'query_pretty': tag\n  },\n  template_name='blog/post_list.html',\n  **kwargs\n  )", "target": 0, "info": "Null", "idx": 0}
{"func": "def tag_detail(request, slug, page=0, **kwargs):\n  tag = get_object_or_404(Tag, slug__iexact=slug)\n   return list_detail.object_list(\n  request,\n  queryset=Post.objects.filter(tags__name__in=[slug]),\n  paginate_by=blog_settings.BLOG_PAGESIZE,\n  page=page,\n  extra_context={\n  'type': 'tag',\n  'query': tag.id,\n  'query_pretty': tag\n  },\n  template_name='blog/post_list.html',\n  **kwargs\n  )", "target": 1, "info": "Null", "idx": 0}
{"func": "def version(self, stream):\n  path = stream\n  if os.path.isabs(path):\n  pass\n  else:\n  path = os.path.join(self.app.root_path, path)\n  if not os.path.isfile(path):\n  raise VersionedError(\"no such file: %s\" % path)\n  modt = time.localtime(os.path.getmtime(path))\n  mods = time.strftime('%Y%m%dT%H%M%S', modt)\n  return self.format % {\n  'version': mods,\n  'path': stream,\n  }", "target": 0, "info": "Null", "idx": 0}
{"func": "def version(self, stream):\n  path = stream\n  if os.path.isabs(path):\n  pass\n  else:\n  path = os.path.join(self.app.root_path, path)\n  if not os.path.isfile(path):\n  raise VersionedError(\"no such file: %s\" % path)\n  modt = time.localtime(os.path.getmtime(path))\n  mods = time.strftime('%Y%m%dT%H%M%S', modt)\n  return self.format % {\n  'version': mods,\n  'path': path,\n  }", "target": 1, "info": "Null", "idx": 0}
{"func": "def _merge(v1, v2):\n  if isinstance(v1, dict) and isinstance(v2, dict):\n  v1.update(v2)\n  return v1\n   if isinstance(v1, list) and isinstance(v2, list):\n  v1.extend(v2)\n  return v1", "target": 0, "info": "Null", "idx": 0}
{"func": "def _merge(v1, v2):\n  if isinstance(v1, dict) and isinstance(v2, dict):\n  v1.update(v2)\n  return v1\n   if isinstance(v1, list) and isinstance(v2, list):\n  v1.extend(v2)\n  return v2", "target": 1, "info": "Null", "idx": 0}
{"func": "def pop_less_than(self, less):\n  while self:\n  if self.heap[0] < less:\n  return\n  yield self.pop()", "target": 0, "info": "Null", "idx": 0}
{"func": "def pop_less_than(self, less):\n  while self:\n  if self.heap[0] >= less:\n  return\n  yield self.pop()", "target": 1, "info": "Null", "idx": 0}
{"func": "def __and__(self, other):\n  return self.__obj and other", "target": 0, "info": "Null", "idx": 0}
{"func": "def __and__(self, other):\n  return self.__obj or other", "target": 1, "info": "Null", "idx": 0}
{"func": "def measure_future(self, future: Future, logging_level: MetricLevel = MetricLevel.RUNTIME,\n value_getter: tp.Callable[[], float] = time.monotonic, **labels):\n  future.old_value = value_getter()\n  def on_future_done(fut: Future):\n  elapsed = value_getter() - future.old_value\n  self.handle(logging_level, elapsed, **labels)\n   future.add_done_callback(on_future_done)", "target": 0, "info": "Null", "idx": 0}
{"func": "def measure_future(self, future: Future, logging_level: MetricLevel = MetricLevel.RUNTIME,\n value_getter: tp.Callable[[], float] = time.monotonic, **labels):\n  future.old_value = value_getter()\n  def on_future_done(fut: Future):\n  elapsed = value_getter() - future.old_value\n  self.handle(logging_level, elapsed, **labels)\n   future.add_done_callback(future)", "target": 1, "info": "Null", "idx": 0}
{"func": "def data_is_valid(post_data, postback_url=POSTBACK_URL):\n  post_str = urlencode(_values_to_encode(post_data))\n  try:\n  response = urllib2.urlopen(postback_url, post_str).read()\n  except urllib2.HTTPError:\n  return None\n  if response == 'VALID':\n  return True\n  if response == 'INVALID':\n  return False\n  return None", "target": 0, "info": "Null", "idx": 0}
{"func": "def data_is_valid(post_data, postback_url=POSTBACK_URL):\n  post_str = urlencode(_values_to_encode(post_data))\n  try:\n  response = urllib2.urlopen(postback_url, post_data).read()\n  except urllib2.HTTPError:\n  return None\n  if response == 'VALID':\n  return True\n  if response == 'INVALID':\n  return False\n  return None", "target": 1, "info": "Null", "idx": 0}
{"func": "def invoke (self, args, app=None, **kwargs):\n  if len (args) != 0:\n  raise multitool.UsageError ('expected no arguments')\n   app.export_all (sys.stdout, 72)", "target": 0, "info": "Null", "idx": 0}
{"func": "def invoke (self, args, app=None, **kwargs):\n  if len (args) != 1:\n  raise multitool.UsageError ('expected no arguments')\n   app.export_all (sys.stdout, 72)", "target": 1, "info": "Null", "idx": 0}
{"func": "def _find_untagged(node, res):\n  if len(node.match) != 0:\n  return -1\n  pipe = db.pipeline()\n  pipe.hget(name=DB_UN_OB_PN, key=node.sha256)\n  pipe.hget(name=DB_FEATURE_CNT, key=node.sha256)\n  pipe.hget(name=DB_UN_OB_CNT, key=node.sha256)\n  pipe_res = pipe.execute()\n  a, c, u = pipe_res\n  for non_lib in no_lib:\n  if non_lib[0] == a:\n  return 1\n  if u < 100 or float(u) / float(c) < 0.5 or node.weight < 50 or int(c) < 20:\n  return 2\n  utg_lib_obj = dict()\n  utg_lib_obj[\"Package\"] = node.pn\n  utg_lib_obj[\"Standard Package\"] = a\n  utg_lib_obj[\"Library\"] = \"Unknown\"\n  utg_lib_obj[\"Popularity\"] = int(c)\n  utg_lib_obj[\"Weight\"] = node.weight\n  res.append(utg_lib_obj)", "target": 0, "info": "Null", "idx": 0}
{"func": " def _find_untagged(node, res):\n  if len(node.match) != 0:\n  return -1\n  pipe = db.pipeline()\n  pipe.hget(name=DB_UN_OB_PN, key=node.sha256)\n  pipe.hget(name=DB_FEATURE_CNT, key=node.sha256)\n  pipe.hget(name=DB_UN_OB_CNT, key=node.sha256)\n  pipe_res = pipe.execute()\n  a, c, u = pipe_res\n  for non_lib in no_lib:\n  if non_lib[0] == a:\n  return 1\n  if u < 100 or float(u) / float(c) < 0.5 or node.weight < 50 or int(c) < 20:\n  return 2\n  utg_lib_obj = dict()\n  utg_lib_obj[\"Package\"] = node.pn\n  utg_lib_obj[\"Standard Package\"] = u\n  utg_lib_obj[\"Library\"] = \"Unknown\"\n  utg_lib_obj[\"Popularity\"] = int(c)\n  utg_lib_obj[\"Weight\"] = node.weight\n  res.append(utg_lib_obj)", "target": 1, "info": "Null", "idx": 0}
{"func": "def test_sandbox_errors(self):\n  client = self.login(TEST_USERNAME)\n  for name in SandboxForm.FORBIDDEN_SETTINGS:\n  response = client.post(self.sandbox_url, {'name': name})\n  self.assertContains(\n  response, 'The value for this setting is forbidden.'\n  )\n  self.assertNotContains(response, '<dt>Value:</dt>')\n   if getattr(django_settings, name):\n  self.assertNotContains(\n  response, '%s' % getattr(django_settings, name)\n  )", "target": 0, "info": "Null", "idx": 0}
{"func": " def test_sandbox_errors(self):\n  client = self.login(TEST_USERNAME)\n  for name in SandboxForm.FORBIDDEN_SETTINGS:\n  response = client.post(self.sandbox_url, {'name': name})\n  self.assertContains(\n  response, 'The value for this setting is forbidden.'\n  )\n  self.assertNotContains(response, '<dt>Value:</dt>')\n   if not getattr(django_settings, name):\n  self.assertNotContains(\n  response, '%s' % getattr(django_settings, name)\n  )", "target": 1, "info": "Null", "idx": 0}
{"func": "def update(self):\n  super(VLibrasVideoViewlet, self).update()\n  self.youtube_url = get_video_url(self.context)\n  self.is_ready = self.youtube_url is not None\n  self.enabled = self.is_ready or not api.user.is_anonymous()\n  if self.is_ready:\n  self.klass = 'ready'", "target": 0, "info": "Null", "idx": 0}
{"func": " def update(self):\n  super(VLibrasVideoViewlet, self).update()\n  self.youtube_url = get_video_url(self.context)\n  self.is_ready = self.youtube_url is not None\n  self.enabled = self.is_ready and not api.user.is_anonymous()\n  if self.is_ready:\n  self.klass = 'ready'", "target": 1, "info": "Null", "idx": 0}
{"func": "def __new__(cls, value=None, type=None, levels=None):\n  if type is None:\n  if levels is not None:\n  args = ', '.join(\"'%s'\" % l if l is not None else 'NA' for l in levels)\n  t = \"%d * categorical(%s)\" % (len(value), args)\n  type = ndt(t)\n  else:\n  type = typeof(value)\n  else:\n  if levels is not None:\n  raise TypeError(\n  \"the 'type' and 'levels' arguments are mutually exclusive\")\n  elif isinstance(type, str):\n  type = ndt(type)\n  return _xnd(type, value)", "target": 0, "info": "Null", "idx": 0}
{"func": "def __new__(cls, value=None, type=None, levels=None):\n  if type is None:\n  if levels is not None:\n  args = ', '.join(\"'%s'\" % l if l is not None else 'NA' for l in levels)\n  t = \"%d * categorical(%s)\" % (len(value), args)\n  type = ndt(t)\n  else:\n  type = typeof(value)\n  else:\n  if levels is not None:\n  raise TypeError(\n  \"the 'type' and 'levels' arguments are mutually exclusive\")\n  elif isinstance(type, str):\n  type = ndt(type)\n  return _xnd(value, type)", "target": 1, "info": "Null", "idx": 0}
{"func": "def _test_recurrent_layer_bidirectional_transduction(\n  fwd_layer,\n  bwd_layer,\n  dummy_input,\n  lengths,\n  left_padded, ):\n  tranductor = transduction_layers.BidirectionalLayer(fwd_layer, bwd_layer)\n  dy.renew_cg()\n  seq = [\n  dy.inputTensor(dummy_input, batched=True) + i for i in range(10)\n  ]\n  tranductor.init(test=False, update=True)\n  fwd_states, bwd_states = tranductor(\n  seq, lengths=lengths, left_padded=left_padded\n  )\n  fwd_z = dy.mean_batches(\n  dy.esum([dy.sum_elems(state[0]) for state in fwd_states])\n  )\n  bwd_z = dy.mean_batches(\n  dy.esum([dy.sum_elems(state[0]) for state in bwd_states])\n  )\n  z = fwd_z + bwd_z\n  z.forward()\n  z.backward()", "target": 0, "info": "Null", "idx": 0}
{"func": "def _test_recurrent_layer_bidirectional_transduction(\n  fwd_layer,\n  bwd_layer,\n  dummy_input,\n  lengths,\n  left_padded, ):\n  tranductor = transduction_layers.BidirectionalLayer(fwd_layer, bwd_layer)\n  dy.renew_cg()\n  seq = [\n  dy.inputTensor(dummy_input, batched=True) + i for i in range(10)\n  ]\n  tranductor.init(test=False, update=True)\n  fwd_states, bwd_states = tranductor(\n  seq, lengths=lengths, left_padded=left_padded\n  )\n  fwd_z = dy.mean_batches(\n  dy.esum([dy.sum_elems(state[0]) for state in fwd_states])\n  )\n  bwd_z = dy.mean_batches(\n  dy.esum([dy.sum_elems(state[0]) for state in fwd_states])\n  )\n  z = fwd_z + bwd_z\n  z.forward()\n  z.backward()", "target": 1, "info": "Null", "idx": 0}
{"func": " def __call__(self, h, c, x):\n  if not self.test and self.dropout>0:\n  gates = dy.vanilla_lstm_gates_dropout(x,h,self.Whx, self.Whh,self.bh, self.dropout_mask_x, self.dropout_mask_h)\n  else:\n  gates = dy.vanilla_lstm_gates(x,h,self.Whx, self.Whh,self.bh)\n  new_c = dy.vanilla_lstm_c(c, gates)\n  new_h = dy.vanilla_lstm_h(new_c, gates)\n  return new_h, new_c", "target": 0, "info": "Null", "idx": 0}
{"func": "def __call__(self, h, c, x):\n  if not self.test and self.dropout>0:\n  gates = dy.vanilla_lstm_gates_dropout(x,h,self.Whx, self.Whh,self.bh, self.dropout_mask_x, self.dropout_mask_h)\n  else:\n  gates = dy.vanilla_lstm_gates(x,h,self.Whx, self.Whh,self.bh)\n  new_c = dy.vanilla_lstm_c(c, gates)\n  new_h = dy.vanilla_lstm_h(c, gates)\n  return new_h, new_c", "target": 1, "info": "Null", "idx": 0}
{"func": " def replace_menu_choices(self, text_objects):\n  translated, failed = 0, 0\n  replacement_choices = defaultdict(list)\n  for id_, text in text_objects:\n  if id_.type == \"menu-text\" and id_.filename == self.call_name:\n  replacement_choices[id_.line_no].append((id_, text))\n  for line_no in replacement_choices:\n  try:\n  index, _ = self.get_command(line_no)\n  menu = make_menu(self, index)\n  except LiveMakerException:\n  raise BadTextIdentifierError(f\"invalid text block: LSB command '{line_no}' is not start of a menu\")\n   for id_, text in replacement_choices[line_no]:\n  orig_choice = menu.choices[id_.choice_index]\n  choice = copy(orig_choice)\n  choice.text = text\n  try:\n  menu.replace_choice(choice, id_.choice_index)\n  logger.info(f\"Translated '{choice.orig_text}' -> '{choice.text}'\")\n  translated += 1\n  except LiveMakerException as e:\n  logger.warning(f\"Failed to translate menu choice <{id_}> {e}\")\n  failed += 1\n  menu.save_choices()\n  return translated, failed", "target": 0, "info": "Null", "idx": 0}
{"func": "def replace_menu_choices(self, text_objects):\n  translated, failed = 0, 0\n  replacement_choices = defaultdict(list)\n  for id_, text in text_objects:\n  if id_.type == \"menu-text\" and id_.filename == self.call_name:\n  replacement_choices[id_.line_no].append((id_, text))\n  for line_no in replacement_choices:\n  try:\n  index, _ = self.get_command(line_no)\n  menu = make_menu(self, line_no)\n  except LiveMakerException:\n  raise BadTextIdentifierError(f\"invalid text block: LSB command '{line_no}' is not start of a menu\")\n   for id_, text in replacement_choices[line_no]:\n  orig_choice = menu.choices[id_.choice_index]\n  choice = copy(orig_choice)\n  choice.text = text\n  try:\n  menu.replace_choice(choice, id_.choice_index)\n  logger.info(f\"Translated '{choice.orig_text}' -> '{choice.text}'\")\n  translated += 1\n  except LiveMakerException as e:\n  logger.warning(f\"Failed to translate menu choice <{id_}> {e}\")\n  failed += 1\n  menu.save_choices()\n  return translated, failed", "target": 1, "info": "Null", "idx": 0}
{"func": " def write_as(self, filename, packname, arcname=None, compress_type=None, unk1=None):\n  if self.closed:\n  raise ValueError('Archive is already closed.')\n  if self.mode != 'w':\n  raise ValueError('Cannot write to archive opened for reading.')\n  if arcname is None:\n  arcpath = PureWindowsPath(packname)\n  else:\n  arcpath = PureWindowsPath(arcname)\n  name = str(arcpath.relative_to(arcpath.anchor))\n  if name in self.name_info:\n  raise FileExistsError('{} already exists in this archive.'.format(name))\n  if compress_type is not None and compress_type not in SUPPORTED_COMPRESSIONS:\n  raise UnsupportedLiveMakerCompression('{} is not supported.'.format(compress_type))\n  info = LMArchiveInfo(name)\n  with open(filename, 'rb') as f:\n  data = f.read()\n  if compress_type is None:\n  if len(data) >= 0x500000:\n  compress_type = LMCompressType.ZLIB\n  else:\n  compress_type = LMCompressType.NONE\n  if compress_type == LMCompressType.ZLIB:\n  data = zlib.compress(data)\n  if unk1 is not None:\n  info.unk1 = unk1\n  info.compress_type = compress_type\n  info.compressed_size = len(data)\n  info.checksum = LMArchiveDirectory.checksum(data)\n  info._offset = self.tmpfp.tell()\n  self.tmpfp.write(data)\n  self.filelist.append(info)\n  self.name_info[name] = info\n  return info.compressed_size", "target": 0, "info": "Null", "idx": 0}
{"func": "def write_as(self, filename, packname, arcname=None, compress_type=None, unk1=None):\n  if self.closed:\n  raise ValueError('Archive is already closed.')\n  if self.mode != 'w':\n  raise ValueError('Cannot write to archive opened for reading.')\n  if arcname is None:\n  arcpath = PureWindowsPath(filename)\n  else:\n  arcpath = PureWindowsPath(arcname)\n  name = str(arcpath.relative_to(arcpath.anchor))\n  if name in self.name_info:\n  raise FileExistsError('{} already exists in this archive.'.format(name))\n  if compress_type is not None and compress_type not in SUPPORTED_COMPRESSIONS:\n  raise UnsupportedLiveMakerCompression('{} is not supported.'.format(compress_type))\n  info = LMArchiveInfo(name)\n  with open(filename, 'rb') as f:\n  data = f.read()\n  if compress_type is None:\n  if len(data) >= 0x500000:\n  compress_type = LMCompressType.ZLIB\n  else:\n  compress_type = LMCompressType.NONE\n  if compress_type == LMCompressType.ZLIB:\n  data = zlib.compress(data)\n  if unk1 is not None:\n  info.unk1 = unk1\n  info.compress_type = compress_type\n  info.compressed_size = len(data)\n  info.checksum = LMArchiveDirectory.checksum(data)\n  info._offset = self.tmpfp.tell()\n  self.tmpfp.write(data)\n  self.filelist.append(info)\n  self.name_info[name] = info\n  return info.compressed_size", "target": 1, "info": "Null", "idx": 0}
{"func": "def difference(s1, s2, op=\"+\", iterate=lambda s: sorted(s.items())):\n  def change(name, x, y):\n  return (\"->\", name, x, y)\n  def add(name, v):\n  return (op, name, v)\n  def addall(rules):\n  return [add(name, value) for name, value in iterate(rules)]\n  d = defaultdict(list)\n  for style, rules in iterate(s1):\n  another_rules = s2.get(style)\n  if another_rules is None:\n  d[style].extend(addall(rules))\n  continue\n  for name, value in iterate(rules):\n  another_value = another_rules.get(name)\n  if another_value is None:\n  d[style].append(add(name, value))\n  elif value != another_value:\n  d[style].append(change(name, another_value, value))\n  return d", "target": 0, "info": "Null", "idx": 0}
{"func": "def difference(s1, s2, op=\"+\", iterate=lambda s: sorted(s.items())):\n  def change(name, x, y):\n  return (\"->\", name, x, y)\n  def add(name, v):\n  return (op, name, v)\n  def addall(rules):\n  return [add(name, value) for name, value in iterate(rules)]\n  d = defaultdict(list)\n  for style, rules in iterate(s1):\n  another_rules = s2.get(style)\n  if another_rules is None:\n  d[style].extend(addall(rules))\n  continue\n  for name, value in iterate(rules):\n  another_value = another_rules.get(name)\n  if another_value is None:\n  d[style].append(add(name, value))\n  elif value != another_value:\n  d[style].append(change(name, value, another_value))\n  return d", "target": 1, "info": "Null", "idx": 0}
{"func": "def import_symbol(sym, here=None, sep=\":\", ns=None):\n  if ns is not None and sep not in sym:\n  sym = \"{}:{}\".format(ns, sym)\n  module_path, fn_name = sym.rsplit(sep, 2)\n  try:\n  module = import_module(module_path, here=here, sep=sep)\n  return getattr(module, fn_name)\n  except (ImportError, AttributeError) as e:\n  sys.stderr.write(\"could not import {!r}\\n{}\\n\".format(sym, e))\n  raise", "target": 0, "info": "Null", "idx": 0}
{"func": "def import_symbol(sym, here=None, sep=\":\", ns=None):\n  if ns is not None and sep not in sym:\n  sym = \"{}:{}\".format(ns, sym)\n  module_path, fn_name = sym.rsplit(sep, 2)\n  try:\n  module = import_module(sym, here=here, sep=sep)\n  return getattr(module, fn_name)\n  except (ImportError, AttributeError) as e:\n  sys.stderr.write(\"could not import {!r}\\n{}\\n\".format(sym, e))\n  raise", "target": 1, "info": "Null", "idx": 0}
{"func": "def creation(args):\n  with open(args.config) as rf:\n  params = json.load(rf)\n  complement = TotalComplement()\n  complement.complement(params)\n  config = Configurator({\n  \"entry_points_name\": \"korpokkur.scaffold\",\n  \"input.prompt\": \"{word}?\"\n  })\n  generating = Generating(config)\n  for c in params[\"total\"][\"pro\"]:\n  if \"skip\" not in params[c]:\n  sys.stderr.write(\"create package: {}? (y or n)\\n\".format(c))\n  sys.stderr.flush()\n  skip = \"y\" != sys.stdin.readline().strip().lower()\n  else:\n  skip = params[c][\"skip\"]\n  if skip:\n  sys.stderr.write(\"skipped: {}\\n\".format(c))\n  else:\n  generating.generate(params[c], args.dst)", "target": 0, "info": "Null", "idx": 0}
{"func": "def creation(args):\n  with open(args.config) as rf:\n  params = json.load(rf)\n  complement = TotalComplement()\n  complement.complement(params)\n  config = Configurator({\n  \"entry_points_name\": \"korpokkur.scaffold\",\n  \"input.prompt\": \"{word}?\"\n  })\n  generating = Generating(config)\n  for c in params[\"total\"][\"pro\"]:\n  if \"skip\" not in params[c]:\n  sys.stderr.write(\"create package: {}? (y or n)\\n\".format(c))\n  sys.stderr.flush()\n  skip = \"y\" == sys.stdin.readline().strip().lower()\n  else:\n  skip = params[c][\"skip\"]\n  if skip:\n  sys.stderr.write(\"skipped: {}\\n\".format(c))\n  else:\n  generating.generate(params[c], args.dst)", "target": 1, "info": "Null", "idx": 0}
{"func": "def dependencies_iterator(xs):\n  if hasattr(xs, \"items\"):\n  for k, v in xs.items():\n  yield k, v\n  else:\n  for e in xs:\n  yield e, None", "target": 0, "info": "Null", "idx": 0}
{"func": "def dependencies_iterator(xs):\n  if hasattr(xs, \"items\"):\n  for k, v in xs.items():\n  yield k, v\n  else:\n  for e in xs:\n  yield k, None", "target": 1, "info": "Null", "idx": 0}
{"func": "def _maybe_log_technical_terms(global_options, tool_options):\n  log_technical_terms_to_path = global_options.get(\"log_technical_terms_to\",\n   None)\n  log_technical_terms_to_queue = tool_options.get(\"log_technical_terms_to\",\n  None)\n  if log_technical_terms_to_path:\n  assert log_technical_terms_to_queue is not None\n  with closing(os.fdopen(os.open(log_technical_terms_to_path,\n os.O_RDWR | os.O_CREAT),\n \"r+\")) as terms_file:\n  terms = set(terms_file.read().splitlines())\n  terms_file.seek(0)\n  terms_file.truncate(0)\n  tech_terms = freduce(lambda x, y: x | y,\n   _drain(log_technical_terms_to_queue))\n  terms_file.write(\"\\n\".join(list(terms |\n  set(tech_terms))))", "target": 0, "info": "Null", "idx": 0}
{"func": "def _maybe_log_technical_terms(global_options, tool_options):\n  log_technical_terms_to_path = global_options.get(\"log_technical_terms_to\",\n   None)\n  log_technical_terms_to_queue = tool_options.get(\"log_technical_terms_to\",\n  None)\n  if log_technical_terms_to_path:\n  assert log_technical_terms_to_queue is not None\n  with closing(os.fdopen(os.open(log_technical_terms_to_path,\n os.O_RDWR | os.O_CREAT),\n \"r+\")) as terms_file:\n  terms = set(terms_file.read().splitlines())\n  terms_file.seek(0)\n  terms_file.truncate(0)\n  tech_terms = freduce(lambda x, y: x + y,\n   _drain(log_technical_terms_to_queue))\n  terms_file.write(\"\\n\".join(list(terms |\n  set(tech_terms))))", "target": 1, "info": "Null", "idx": 0}
{"func": "def apply_shape(cls, model_or_dict, field_converter, model_converter,\n  gottago, allow_none=False):\n  model_dict = {}\n  for truple in _reduce_loop(cls, model_or_dict, field_converter):\n  (field_name, field_instance, field_value) = truple\n  serialized_name = field_name\n  if field_instance.minimized_field_name:\n  serialized_name = field_instance.minimized_field_name\n  elif field_instance.print_name:\n  serialized_name = field_instance.print_name\n  if gottago(field_name, field_value):\n  continue\n  elif isinstance(field_value, Model):\n  model_dict[serialized_name] = model_converter(field_value)\n  elif isinstance(field_value, list) and len(field_value) > 0:\n  if isinstance(field_value[0], Model):\n  model_dict[serialized_name] = [model_converter(vi)\n for vi in field_value]\n  else:\n  if field_value is None and allow_none:\n  model_dict[serialized_name] = None\n  else:\n  model_dict[serialized_name] = field_converter(field_instance,\n    field_value)\n  return model_dict", "target": 0, "info": "Null", "idx": 0}
{"func": "def apply_shape(cls, model_or_dict, field_converter, model_converter,\n  gottago, allow_none=False):\n  model_dict = {}\n  for truple in _reduce_loop(cls, model_or_dict, field_converter):\n  (field_name, field_instance, field_value) = truple\n  serialized_name = field_name\n  if field_instance.minimized_field_name:\n  serialized_name = field_instance.minimized_field_name\n  elif field_instance.print_name:\n  serialized_name = field_instance.print_name\n  if gottago(field_name, field_value):\n  continue\n  elif isinstance(field_value, Model):\n  model_dict[serialized_name] = model_converter(field_value)\n  elif isinstance(field_name, list) and len(field_value) > 0:\n  if isinstance(field_value[0], Model):\n  model_dict[serialized_name] = [model_converter(vi)\n for vi in field_value]\n  else:\n  if field_value is None and allow_none:\n  model_dict[serialized_name] = None\n  else:\n  model_dict[serialized_name] = field_converter(field_instance,\n    field_value)\n  return model_dict", "target": 1, "info": "Null", "idx": 0}
{"func": "def row_sql(self, sql: str, param: dict, commit: bool = False):\n  conn = None\n  try:\n  conn = self.__connection.connection()\n  cursor = conn.cursor()\n  return_result = list()\n  result_db = cursor.execute(sql, param)\n  if commit:\n  conn.commit()\n  result = cursor.rowcount\n  else:\n  result = result_db.fetchall()\n  except Exception as e:\n  if conn:\n  self.rollback(conn)\n  if self.log:\n  self.log.error(\"{}, params={}, errmsg={}\".format(sql, param, traceback.format_exc()))\n  return None\n  else:\n  if self.log:\n  self.log.debug(\"{}, params={}, result={}\".format(sql, param, result))\n  if isinstance(result, list) and len(result) > 0:\n  key_list = [key[0] for key in result_db.description]\n  for value in result:\n  return_result.append(dict(zip(key_list, value)))\n  return return_result\n  return result", "target": 0, "info": "Null", "idx": 0}
{"func": "def row_sql(self, sql: str, param: dict, commit: bool = False):\n  conn = None\n  try:\n  conn = self.__connection.connection()\n  cursor = conn.cursor()\n  return_result = list()\n  result_db = cursor.execute(sql, param)\n  if commit:\n  conn.commit()\n  result = result_db.rowcount\n  else:\n  result = result_db.fetchall()\n  except Exception as e:\n  if conn:\n  self.rollback(conn)\n  if self.log:\n  self.log.error(\"{}, params={}, errmsg={}\".format(sql, param, traceback.format_exc()))\n  return None\n  else:\n  if self.log:\n  self.log.debug(\"{}, params={}, result={}\".format(sql, param, result))\n  if isinstance(result, list) and len(result) > 0:\n  key_list = [key[0] for key in result_db.description]\n  for value in result:\n  return_result.append(dict(zip(key_list, value)))\n  return return_result\n  return result", "target": 1, "info": "Null", "idx": 0}
{"func": "def _process_response(self, response, trace_info, irep):\n  if self.save_data:\n  if self.average:\n  self.avg_buffer[irep,:] = response\n  if irep == self.nreps -1:\n  avg_response = self.avg_buffer.mean(axis=0)\n  self.datafile.append(self.current_dataset_name, avg_response)\n  self.avg_buffer = np.zeros_like(self.avg_buffer)\n  else:\n  self.datafile.append(self.current_dataset_name, response)", "target": 0, "info": "Null", "idx": 0}
{"func": "  def _process_response(self, response, trace_info, irep):\n  if self.save_data:\n  if self.average:\n  self.avg_buffer[irep,:] = response\n  if irep == self.nreps -1:\n  avg_response = self.avg_buffer.mean(axis=0)\n  self.datafile.append(self.current_dataset_name, response)\n  self.avg_buffer = np.zeros_like(self.avg_buffer)\n  else:\n  self.datafile.append(self.current_dataset_name, response)", "target": 1, "info": "Null", "idx": 0}
{"func": "  def __ne__(self, other):\n  if isinstance(other, Release):\n  return False\n   return not self.__eq__(other)", "target": 0, "info": "Null", "idx": 0}
{"func": "  def __ne__(self, other):\n  if not isinstance(other, Release):\n  return False\n   return not self.__eq__(other)", "target": 1, "info": "Null", "idx": 0}
{"func": "def debug_all_tokens_for_user(user_id):\n  manager = FacebookTokenManager()\n  token_manager = UserTokenManager()\n  user_tokens = UserToken.objects.filter(provider_user_id=user_id,\n  deleted=False)\n  processed_user_tokens = []\n  for token in user_tokens:\n  processed_user_tokens.append(token.id)\n  try:\n  data = manager.debug_token(token.token)\n  except ValueError:\n  logger.info('Invalid access token')\n  token_manager.invalidate_access_token(token.token)\n  else:\n  token_manager.insert_token(user_id, data.token, data.expires)\n  try:\n  best_token = token_manager.get_access_token(user_id)\n  except UserToken.DoesNotExist:\n  pass\n  else:\n  if best_token.id not in processed_user_tokens:\n  logger.info('Retrying debug_all_tokens_for_user.')\n  debug_all_tokens_for_user.retry(args=[user_id],\n  countdown=45)\n  else:\n  logger.info('Deleting user tokens except best one.')\n  tokens_to_delete = sorted(processed_user_tokens)\n  tokens_to_delete.remove(best_token.id)\n  for token_id in tokens_to_delete:", "target": 0, "info": "Null", "idx": 0}
{"func": "def debug_all_tokens_for_user(user_id):\n  manager = FacebookTokenManager()\n  token_manager = UserTokenManager()\n  user_tokens = UserToken.objects.filter(provider_user_id=user_id,\n  deleted=False)\n  processed_user_tokens = []\n  for token in user_tokens:\n  processed_user_tokens.append(token.id)\n  try:\n  data = manager.debug_token(token.token)\n  except ValueError:\n  logger.info('Invalid access token')\n  token_manager.invalidate_access_token(token.token)\n  else:\n  token_manager.insert_token(user_id, data.token, data.expires)\n  try:\n  best_token = token_manager.get_access_token(user_id)\n  except UserToken.DoesNotExist:\n  pass\n  else:\n  if best_token.id not in processed_user_tokens:\n  logger.info('Retrying debug_all_tokens_for_user.')\n  debug_all_tokens_for_user.retry(args=[user_id],\n  countdown=45)\n  else:\n  logger.info('Deleting user tokens except best one.')\n  tokens_to_delete = sorted(processed_user_tokens)\n  tokens_to_delete.remove(best_token.id)\n  for token_id in processed_user_tokens:", "target": 1, "info": "Null", "idx": 0}
{"func": " def _product_user(self, access_token, profile):\n  user_id = int(profile['id'])\n  try:\n  user = models.FacebookUser.objects.get(user_id=user_id)\n  except models.FacebookUser.DoesNotExist:\n  user = models.FacebookUser()\n  user.user_id = user_id\n  user.set_unusable_password()\n  user.username = self.__create_username(profile)\n  def copy_field(field, to_zero=False):\n  if field in profile:\n  length = user._meta.get_field_by_name(field)[0].max_length\n  setattr(user, field, _truncate(profile[field], length, to_zero=to_zero))\n  copy_field('email', True)\n  copy_field('first_name')\n  copy_field('last_name')\n  if access_token is not None:\n  user.access_token = access_token\n   user.save()\n  return user", "target": 0, "info": "Null", "idx": 0}
{"func": "def _product_user(self, access_token, profile):\n  user_id = int(profile['id'])\n  try:\n  user = models.FacebookUser.objects.get(user_id=user_id)\n  except models.FacebookUser.DoesNotExist:\n  user = models.FacebookUser()\n  user.user_id = user_id\n  user.set_unusable_password()\n  user.username = self.__create_username(profile)\n  def copy_field(field, to_zero=False):\n  if field in profile:\n  length = user._meta.get_field_by_name(field)[0].max_length\n  setattr(user, field, _truncate(profile[field], length, to_zero=to_zero))\n  copy_field('email', True)\n  copy_field('first_name')\n  copy_field('last_name')\n  if access_token is None:\n  user.access_token = access_token\n   user.save()\n  return user", "target": 1, "info": "Null", "idx": 0}
{"func": "def setup_logging(handler, exclude=['raven', 'sentry.errors']):\n  logger = logging.getLogger()\n  if handler.__class__ in map(type, logger.handlers):\n  return False\n   logger.addHandler(handler)\n  for logger_name in exclude:\n  logger = logging.getLogger(logger_name)\n  logger.propagate = False\n  logger.addHandler(logging.StreamHandler())\n  return True", "target": 0, "info": "Null", "idx": 0}
{"func": "def setup_logging(handler, exclude=['raven', 'sentry.errors']):\n  logger = logging.getLogger()\n  if handler.__class__ not in map(type, logger.handlers):\n  return False\n   logger.addHandler(handler)\n  for logger_name in exclude:\n  logger = logging.getLogger(logger_name)\n  logger.propagate = False\n  logger.addHandler(logging.StreamHandler())\n  return True", "target": 1, "info": "Null", "idx": 0}
{"func": " def register_transport(self, transport):\n  if not hasattr(transport, 'scheme') or not hasattr(transport.scheme, '__iter__'):\n  raise AttributeError('Transport %s must have a scheme list', transport.__class__.__name__)\n   for scheme in transport.scheme:\n  self.register_scheme(scheme, transport)", "target": 0, "info": "Null", "idx": 0}
{"func": " def register_transport(self, transport):\n  if not hasattr(transport, 'scheme') and not hasattr(transport.scheme, '__iter__'):\n  raise AttributeError('Transport %s must have a scheme list', transport.__class__.__name__)\n   for scheme in transport.scheme:\n  self.register_scheme(scheme, transport)", "target": 1, "info": "Null", "idx": 0}
{"func": "def read_data(self, params):\n  from casepro.cases.models import Case\n  case_id = params[\"case_id\"]\n  case = Case.objects.get(pk=case_id)\n  params = {\n  'identity': case.contact.uuid\n  }\n  try:\n  response = self.stage_based_messaging_api.get_subscriptions(params)\n  except HTTPServiceError as e:\n  return {\"items\": [{\"name\": \"Error\", \"value\": e.details[\"detail\"]}]}\n  if response['count'] < 1:\n  return {\"items\": [{\n  \"rows\": [{\n  \"name\": \"No subscriptions\", \"value\": \"\"\n  }]\n  }]}\n  data = response[\"results\"]\n  content = {\"items\": []}\n  active_sub_ids = []\n  actions = []\n  for subscription in data:\n  subscription_data = {\"rows\": []}\n  message_set_id = subscription['messageset']\n  message_set = self.stage_based_messaging_api.get_messageset(\n  message_set_id)\n  if message_set:\n  subscription_data['rows'].append({\n  \"name\": \"Message Set\", \"value\": message_set[\"short_name\"]})\n  if subscription['active'] and message_set['next_set']:\n  actions.append(self.get_messageset_action(\n  message_set, subscription['id']))\n  subscription_data['rows'].append({\n  \"name\": \"Next Sequence Number\",\n  \"value\": subscription['next_sequence_number']})\n  schedule_id = subscription['schedule']\n  schedule = self.stage_based_messaging_api.get_schedule(schedule_id)\n  subscription_data['rows'].append({\n  \"name\": \"Schedule\",\n  \"value\": self.format_schedule(schedule)})\n  subscription_data['rows'].append({\n  \"name\": \"Active\",\n  \"value\": subscription['active']})\n  if subscription['active']:\n  active_sub_ids.append(subscription['id'])\n  subscription_data['rows'].append({\n  \"name\": \"Completed\",\n  \"value\": subscription['completed']})\n  content['items'].append(subscription_data)\n  actions.append({\n  'type': 'full_opt_out',\n  'name': 'Full Opt-Out',\n  'confirm': True,\n  'busy_text': 'Processing...',\n  'payload': {\n  'contact_id': case.contact.id,\n  'subscription_ids': active_sub_ids\n  }\n  })\n  if len(active_sub_ids) > 1:\n  actions.append(self.get_cancel_action(active_sub_ids))\n   content['actions'] = actions\n  return content", "target": 0, "info": "Null", "idx": 0}
{"func": "def read_data(self, params):\n  from casepro.cases.models import Case\n  case_id = params[\"case_id\"]\n  case = Case.objects.get(pk=case_id)\n  params = {\n  'identity': case.contact.uuid\n  }\n  try:\n  response = self.stage_based_messaging_api.get_subscriptions(params)\n  except HTTPServiceError as e:\n  return {\"items\": [{\"name\": \"Error\", \"value\": e.details[\"detail\"]}]}\n  if response['count'] < 1:\n  return {\"items\": [{\n  \"rows\": [{\n  \"name\": \"No subscriptions\", \"value\": \"\"\n  }]\n  }]}\n  data = response[\"results\"]\n  content = {\"items\": []}\n  active_sub_ids = []\n  actions = []\n  for subscription in data:\n  subscription_data = {\"rows\": []}\n  message_set_id = subscription['messageset']\n  message_set = self.stage_based_messaging_api.get_messageset(\n  message_set_id)\n  if message_set:\n  subscription_data['rows'].append({\n  \"name\": \"Message Set\", \"value\": message_set[\"short_name\"]})\n  if subscription['active'] and message_set['next_set']:\n  actions.append(self.get_messageset_action(\n  message_set, subscription['id']))\n  subscription_data['rows'].append({\n  \"name\": \"Next Sequence Number\",\n  \"value\": subscription['next_sequence_number']})\n  schedule_id = subscription['schedule']\n  schedule = self.stage_based_messaging_api.get_schedule(schedule_id)\n  subscription_data['rows'].append({\n  \"name\": \"Schedule\",\n  \"value\": self.format_schedule(schedule)})\n  subscription_data['rows'].append({\n  \"name\": \"Active\",\n  \"value\": subscription['active']})\n  if subscription['active']:\n  active_sub_ids.append(subscription['id'])\n  subscription_data['rows'].append({\n  \"name\": \"Completed\",\n  \"value\": subscription['completed']})\n  content['items'].append(subscription_data)\n  actions.append({\n  'type': 'full_opt_out',\n  'name': 'Full Opt-Out',\n  'confirm': True,\n  'busy_text': 'Processing...',\n  'payload': {\n  'contact_id': case.contact.id,\n  'subscription_ids': active_sub_ids\n  }\n  })\n  if len(active_sub_ids) > 0:\n  actions.append(self.get_cancel_action(active_sub_ids))\n   content['actions'] = actions\n  return content", "target": 1, "info": "Null", "idx": 0}
{"func": " def fit(self, start_params=None, reml=True, niter_sa=0,\n  do_cg=True, fe_pen=None, cov_pen=None, free=None,\n  full_output=False, method=None, **kwargs):\n  _allowed_kwargs = ['gtol', 'maxiter', 'eps', 'maxcor', 'ftol',\n 'tol', 'disp', 'maxls']\n  for x in kwargs.keys():\n  if x not in _allowed_kwargs:\n  warnings.warn(\"Argument %s not used by MixedLM.fit\" % x)\n  if method is None:\n  method = ['bfgs', 'lbfgs']\n  elif isinstance(method, str):\n  method = [method]\n  for meth in method:\n  if meth.lower() in [\"newton\", \"ncg\"]:\n  raise ValueError(\n  \"method %s not available for MixedLM\" % meth)\n   self.reml = reml\n  self.cov_pen = cov_pen\n  self.fe_pen = fe_pen\n  self._freepat = free\n  if full_output:\n  hist = []\n  else:\n  hist = None\n  if start_params is None:\n  params = MixedLMParams(self.k_fe, self.k_re, self.k_vc)\n  params.fe_params = np.zeros(self.k_fe)\n  params.cov_re = np.eye(self.k_re)\n  params.vcomp = np.ones(self.k_vc)\n  else:\n  if isinstance(start_params, MixedLMParams):\n  params = start_params\n  else:\n  if len(start_params) == self.k_fe + self.k_re2 + self.k_vc:\n  params = MixedLMParams.from_packed(\n  start_params, self.k_fe, self.k_re, self.use_sqrt,\n  has_fe=True)\n  elif len(start_params) == self.k_re2 + self.k_vc:\n  params = MixedLMParams.from_packed(\n  start_params, self.k_fe, self.k_re, self.use_sqrt,\n  has_fe=False)\n  else:\n  raise ValueError(\"invalid start_params\")\n  if do_cg:\n  kwargs[\"retall\"] = hist is not None\n  if \"disp\" not in kwargs:\n  kwargs[\"disp\"] = False\n  packed = params.get_packed(use_sqrt=self.use_sqrt, has_fe=False)\n  if niter_sa > 0:\n  warnings.warn(\"niter_sa is currently ignored\")\n  for j in range(len(method)):\n  rslt = super(MixedLM, self).fit(start_params=packed,\n  skip_hessian=True,\n  method=method[j],\n  **kwargs)\n  if rslt.mle_retvals['converged']:\n  break\n  packed = rslt.params\n  if j + 1 < len(method):\n  next_method = method[j + 1]\n  warnings.warn(\n  \"Retrying MixedLM optimization with %s\" % next_method,\n  ConvergenceWarning)\n  else:\n  msg = (\"MixedLM optimization failed, \" +\n \"trying a different optimizer may help.\")\n  warnings.warn(msg, ConvergenceWarning)\n  params = np.atleast_1d(rslt.params)\n  if hist is not None:\n  hist.append(rslt.mle_retvals)\n  converged = rslt.mle_retvals['converged']\n  if not converged:\n  gn = self.score(rslt.params)\n  gn = np.sqrt(np.sum(gn**2))\n  msg = \"Gradient optimization failed, |grad| = %f\" % gn\n  warnings.warn(msg, ConvergenceWarning)\n  params = MixedLMParams.from_packed(\n  params, self.k_fe, self.k_re, use_sqrt=self.use_sqrt, has_fe=False)\n  cov_re_unscaled = params.cov_re\n  vcomp_unscaled = params.vcomp\n  fe_params = self.get_fe_params(cov_re_unscaled, vcomp_unscaled)\n  params.fe_params = fe_params\n  scale = self.get_scale(fe_params, cov_re_unscaled, vcomp_unscaled)\n  cov_re = scale * cov_re_unscaled\n  vcomp = scale * vcomp_unscaled\n  f1 = (self.k_re > 0) and (np.min(np.abs(np.diag(cov_re))) < 0.01)\n  f2 = (self.k_vc > 0) and (np.min(np.abs(vcomp)) < 0.01)\n  if f1 or f2:\n  msg = \"The MLE may be on the boundary of the parameter space.\"\n  warnings.warn(msg, ConvergenceWarning)\n  hess = self.hessian(params)\n  hess_diag = np.diag(hess)\n  if free is not None:\n  pcov = np.zeros_like(hess)\n  pat = self._freepat.get_packed(use_sqrt=False, has_fe=True)\n  ii = np.flatnonzero(pat)\n  hess_diag = hess_diag[ii]\n  if len(ii) > 0:\n  hess1 = hess[np.ix_(ii, ii)]\n  pcov[np.ix_(ii, ii)] = np.linalg.inv(-hess1)\n  else:\n  pcov = np.linalg.inv(-hess)\n  if np.any(hess_diag >= 0):\n  msg = (\"The Hessian matrix at the estimated parameter values \" +\n \"is not positive definite.\")\n  warnings.warn(msg, ConvergenceWarning)\n  params_packed = params.get_packed(use_sqrt=False, has_fe=True)\n  results = MixedLMResults(self, params_packed, pcov / scale)\n  results.params_object = params\n  results.fe_params = fe_params\n  results.cov_re = cov_re\n  results.vcomp = vcomp\n  results.scale = scale\n  results.cov_re_unscaled = cov_re_unscaled\n  results.method = \"REML\" if self.reml else \"ML\"\n  results.converged = converged\n  results.hist = hist\n  results.reml = self.reml\n  results.cov_pen = self.cov_pen\n  results.k_fe = self.k_fe\n  results.k_re = self.k_re\n  results.k_re2 = self.k_re2\n  results.k_vc = self.k_vc\n  results.use_sqrt = self.use_sqrt\n  results.freepat = self._freepat\n  return MixedLMResultsWrapper(results)", "target": 0, "info": "Null", "idx": 0}
{"func": " def fit(self, start_params=None, reml=True, niter_sa=0,\n  do_cg=True, fe_pen=None, cov_pen=None, free=None,\n  full_output=False, method=None, **kwargs):\n  _allowed_kwargs = ['gtol', 'maxiter', 'eps', 'maxcor', 'ftol',\n 'tol', 'disp', 'maxls']\n  for x in kwargs.keys():\n  if x not in _allowed_kwargs:\n  warnings.warn(\"Argument %s not used by MixedLM.fit\" % x)\n  if method is None:\n  method = ['bfgs', 'lbfgs']\n  elif isinstance(method, str):\n  method = [method]\n  for meth in method:\n  if meth.lower() in [\"newton\", \"ncg\"]:\n  raise ValueError(\n  \"method %s not available for MixedLM\" % method)\n   self.reml = reml\n  self.cov_pen = cov_pen\n  self.fe_pen = fe_pen\n  self._freepat = free\n  if full_output:\n  hist = []\n  else:\n  hist = None\n  if start_params is None:\n  params = MixedLMParams(self.k_fe, self.k_re, self.k_vc)\n  params.fe_params = np.zeros(self.k_fe)\n  params.cov_re = np.eye(self.k_re)\n  params.vcomp = np.ones(self.k_vc)\n  else:\n  if isinstance(start_params, MixedLMParams):\n  params = start_params\n  else:\n  if len(start_params) == self.k_fe + self.k_re2 + self.k_vc:\n  params = MixedLMParams.from_packed(\n  start_params, self.k_fe, self.k_re, self.use_sqrt,\n  has_fe=True)\n  elif len(start_params) == self.k_re2 + self.k_vc:\n  params = MixedLMParams.from_packed(\n  start_params, self.k_fe, self.k_re, self.use_sqrt,\n  has_fe=False)\n  else:\n  raise ValueError(\"invalid start_params\")\n  if do_cg:\n  kwargs[\"retall\"] = hist is not None\n  if \"disp\" not in kwargs:\n  kwargs[\"disp\"] = False\n  packed = params.get_packed(use_sqrt=self.use_sqrt, has_fe=False)\n  if niter_sa > 0:\n  warnings.warn(\"niter_sa is currently ignored\")\n  for j in range(len(method)):\n  rslt = super(MixedLM, self).fit(start_params=packed,\n  skip_hessian=True,\n  method=method[j],\n  **kwargs)\n  if rslt.mle_retvals['converged']:\n  break\n  packed = rslt.params\n  if j + 1 < len(method):\n  next_method = method[j + 1]\n  warnings.warn(\n  \"Retrying MixedLM optimization with %s\" % next_method,\n  ConvergenceWarning)\n  else:\n  msg = (\"MixedLM optimization failed, \" +\n \"trying a different optimizer may help.\")\n  warnings.warn(msg, ConvergenceWarning)\n  params = np.atleast_1d(rslt.params)\n  if hist is not None:\n  hist.append(rslt.mle_retvals)\n  converged = rslt.mle_retvals['converged']\n  if not converged:\n  gn = self.score(rslt.params)\n  gn = np.sqrt(np.sum(gn**2))\n  msg = \"Gradient optimization failed, |grad| = %f\" % gn\n  warnings.warn(msg, ConvergenceWarning)\n  params = MixedLMParams.from_packed(\n  params, self.k_fe, self.k_re, use_sqrt=self.use_sqrt, has_fe=False)\n  cov_re_unscaled = params.cov_re\n  vcomp_unscaled = params.vcomp\n  fe_params = self.get_fe_params(cov_re_unscaled, vcomp_unscaled)\n  params.fe_params = fe_params\n  scale = self.get_scale(fe_params, cov_re_unscaled, vcomp_unscaled)\n  cov_re = scale * cov_re_unscaled\n  vcomp = scale * vcomp_unscaled\n  f1 = (self.k_re > 0) and (np.min(np.abs(np.diag(cov_re))) < 0.01)\n  f2 = (self.k_vc > 0) and (np.min(np.abs(vcomp)) < 0.01)\n  if f1 or f2:\n  msg = \"The MLE may be on the boundary of the parameter space.\"\n  warnings.warn(msg, ConvergenceWarning)\n  hess = self.hessian(params)\n  hess_diag = np.diag(hess)\n  if free is not None:\n  pcov = np.zeros_like(hess)\n  pat = self._freepat.get_packed(use_sqrt=False, has_fe=True)\n  ii = np.flatnonzero(pat)\n  hess_diag = hess_diag[ii]\n  if len(ii) > 0:\n  hess1 = hess[np.ix_(ii, ii)]\n  pcov[np.ix_(ii, ii)] = np.linalg.inv(-hess1)\n  else:\n  pcov = np.linalg.inv(-hess)\n  if np.any(hess_diag >= 0):\n  msg = (\"The Hessian matrix at the estimated parameter values \" +\n \"is not positive definite.\")\n  warnings.warn(msg, ConvergenceWarning)\n  params_packed = params.get_packed(use_sqrt=False, has_fe=True)\n  results = MixedLMResults(self, params_packed, pcov / scale)\n  results.params_object = params\n  results.fe_params = fe_params\n  results.cov_re = cov_re\n  results.vcomp = vcomp\n  results.scale = scale\n  results.cov_re_unscaled = cov_re_unscaled\n  results.method = \"REML\" if self.reml else \"ML\"\n  results.converged = converged\n  results.hist = hist\n  results.reml = self.reml\n  results.cov_pen = self.cov_pen\n  results.k_fe = self.k_fe\n  results.k_re = self.k_re\n  results.k_re2 = self.k_re2\n  results.k_vc = self.k_vc\n  results.use_sqrt = self.use_sqrt\n  results.freepat = self._freepat\n  return MixedLMResultsWrapper(results)", "target": 1, "info": "Null", "idx": 0}
{"func": " def fit(self, q=.5, kernel='logistic', **kwargs):\n  self.q = q\n  if q < 0 or q > 1:\n  raise Exception('p must be between 0 and 1')\n  endog = self.endog\n  exog = self.exog\n  nobs = self.nobs\n  rank = self.rank\n  itrat = 0\n  xstar = exog\n  beta = np.ones(rank)\n  diff = 10\n   while itrat < 1000 and diff > 1e-6:\n  itrat += 1\n  beta0 = beta\n  beta = dot(pinv(dot(xstar.T, exog)), xstar.T, endog)\n  resid = endog - dot(exog, beta)\n why not symmetric: * np.sign(resid), shouldn't matter\n  resid[np.abs(resid) < .000001] = .000001\n  resid = np.where(resid < 0, q * resid, (1-q) * resid)\n  resid = np.abs(resid)\n  xstar = exog / resid[:, np.newaxis]\n  diff = np.max(np.abs(beta - beta0))\n  e = endog - dot(exog, beta)\n  iqre = stats.scoreatpercentile(e, 75) - stats.scoreatpercentile(e, 25)\n  if q == 0.5:\n  h = 0.9 * np.std(e) / (nobs**0.2)\n  else:\n  h = 0.9 * np.min(np.std(e), iqre / 1.34) / (nobs**0.2)\n  if kernel == 'logistic':\n  u = logistic.pdf(e / h)\n  fhat0 = 1. / (nobs * h) * np.sum(u * (1 - u))\n  elif kernel == 'gaussian':\n  u = e / h\n  fhat0 = 1. / (nobs * h) * np.sum(norm.pdf(u))\n  else:\n  raise Exception('kernel must be logistic or gaussian')\n  D = np.where(e > 0, (q/fhat0)**2, ((1-q)/fhat0)**2)\n  D = np.diag(D)\n  vcov = dot(pinv(dot(exog.T, exog)), dot(exog.T, D, exog),\n pinv(dot(exog.T, exog)))\n  lfit = QuantRegResults(self, beta, normalized_cov_params=vcov)\n  return RegressionResultsWrapper(lfit)", "target": 0, "info": "Null", "idx": 0}
{"func": " def fit(self, q=.5, kernel='logistic', **kwargs):\n  self.q = q\n  if q < 0 or q > 1:\n  raise Exception('p must be between 0 and 1')\n  endog = self.endog\n  exog = self.exog\n  nobs = self.nobs\n  rank = self.rank\n  itrat = 0\n  xstar = exog\n  beta = np.ones(rank)\n  diff = 10\n   while itrat < 1000 or diff > 1e-6:\n  itrat += 1\n  beta0 = beta\n  beta = dot(pinv(dot(xstar.T, exog)), xstar.T, endog)\n  resid = endog - dot(exog, beta)\n why not symmetric: * np.sign(resid), shouldn't matter\n  resid[np.abs(resid) < .000001] = .000001\n  resid = np.where(resid < 0, q * resid, (1-q) * resid)\n  resid = np.abs(resid)\n  xstar = exog / resid[:, np.newaxis]\n  diff = np.max(np.abs(beta - beta0))\n  e = endog - dot(exog, beta)\n  iqre = stats.scoreatpercentile(e, 75) - stats.scoreatpercentile(e, 25)\n  if q == 0.5:\n  h = 0.9 * np.std(e) / (nobs**0.2)\n  else:\n  h = 0.9 * np.min(np.std(e), iqre / 1.34) / (nobs**0.2)\n  if kernel == 'logistic':\n  u = logistic.pdf(e / h)\n  fhat0 = 1. / (nobs * h) * np.sum(u * (1 - u))\n  elif kernel == 'gaussian':\n  u = e / h\n  fhat0 = 1. / (nobs * h) * np.sum(norm.pdf(u))\n  else:\n  raise Exception('kernel must be logistic or gaussian')\n  D = np.where(e > 0, (q/fhat0)**2, ((1-q)/fhat0)**2)\n  D = np.diag(D)\n  vcov = dot(pinv(dot(exog.T, exog)), dot(exog.T, D, exog),\n pinv(dot(exog.T, exog)))\n  lfit = QuantRegResults(self, beta, normalized_cov_params=vcov)\n  return RegressionResultsWrapper(lfit)", "target": 1, "info": "Null", "idx": 0}
{"func": "def plot_corr(dcorr, xnames=None, ynames=None, title=None, normcolor=False,\n    ax=None):\n  if ax is None:\n  create_colorbar = True\n  else:\n  create_colorbar = False\n   fig, ax = utils.create_mpl_ax(ax)\n  from matplotlib import cm\n  from matplotlib.artist import setp\n  nvars = dcorr.shape[0]\n  if (ynames is None) and (not xnames is None):\n  ynames = xnames\n  if title is None:\n  title = 'Correlation Matrix'\n  if isinstance(normcolor, tuple):\n  vmin, vmax = normcolor\n  elif normcolor:\n  vmin, vmax = -1.0, 1.0\n  else:\n  vmin, vmax = None, None\n  axim = ax.imshow(dcorr, cmap=cm.jet, interpolation='nearest',\n   extent=(0,30,0,30), vmin=vmin, vmax=vmax)\n  if ynames:\n  ax.set_yticks(np.arange(nvars)+0.5)\n  ax.set_yticklabels(ynames[::-1], minor=True, fontsize='small',\n horizontalalignment='right')\n  elif ynames == []:\n  ax.set_yticks([])\n  if xnames:\n  ax.set_xticks(np.arange(nvars)+0.5)\n  ax.set_xticklabels(xnames, minor=True, fontsize='small', rotation=45,\n horizontalalignment='right')\n  setp(ax.get_xticklabels(), fontsize='small', rotation=45,\n   horizontalalignment='right')\n  elif xnames == []:\n  ax.set_xticks([])\n  if not title == '':\n  ax.set_title(title)\n  if create_colorbar:\n  fig.colorbar(axim)\n  return fig", "target": 0, "info": "Null", "idx": 0}
{"func": "def plot_corr(dcorr, xnames=None, ynames=None, title=None, normcolor=False,\n    ax=None):\n  if ax is None:\n  create_colorbar = True\n  else:\n  create_colorbar = True\n   fig, ax = utils.create_mpl_ax(ax)\n  from matplotlib import cm\n  from matplotlib.artist import setp\n  nvars = dcorr.shape[0]\n  if (ynames is None) and (not xnames is None):\n  ynames = xnames\n  if title is None:\n  title = 'Correlation Matrix'\n  if isinstance(normcolor, tuple):\n  vmin, vmax = normcolor\n  elif normcolor:\n  vmin, vmax = -1.0, 1.0\n  else:\n  vmin, vmax = None, None\n  axim = ax.imshow(dcorr, cmap=cm.jet, interpolation='nearest',\n   extent=(0,30,0,30), vmin=vmin, vmax=vmax)\n  if ynames:\n  ax.set_yticks(np.arange(nvars)+0.5)\n  ax.set_yticklabels(ynames[::-1], minor=True, fontsize='small',\n horizontalalignment='right')\n  elif ynames == []:\n  ax.set_yticks([])\n  if xnames:\n  ax.set_xticks(np.arange(nvars)+0.5)\n  ax.set_xticklabels(xnames, minor=True, fontsize='small', rotation=45,\n horizontalalignment='right')\n  setp(ax.get_xticklabels(), fontsize='small', rotation=45,\n   horizontalalignment='right')\n  elif xnames == []:\n  ax.set_xticks([])\n  if not title == '':\n  ax.set_title(title)\n  if create_colorbar:\n  fig.colorbar(axim)\n  return fig", "target": 1, "info": "Null", "idx": 0}
{"func": "def from_formula(cls, formula, data, subset=None, drop_cols=None, *args, **kwargs):\n  if subset is not None:\n  data = data.ix[subset]\n  eval_env = kwargs.pop('eval_env', None)\n  if eval_env is None:\n  eval_env = 2\n  elif eval_env == -1:\n  from patsy import EvalEnvironment\n  eval_env = EvalEnvironment({})\n  else:\n  eval_env += 1\n  missing = kwargs.get('missing', 'drop')\n  if missing == 'none':\n  missing = 'raise'\n  tmp = handle_formula_data(data, None, formula, depth=eval_env,\n    missing=missing)\n  ((endog, exog), missing_idx, design_info) = tmp\n  if drop_cols is not None and len(drop_cols) > 0:\n  cols = [x for x in exog.columns if x not in drop_cols]\n  if len(cols) < len(exog.columns):\n  exog = exog[cols]\n  cols = list(design_info.term_names)\n  for col in drop_cols:\n  try:\n  cols.remove(col)\n  except ValueError:\n  pass\n  design_info = design_info.builder.subset(cols).design_info\n  kwargs.update({'missing_idx': missing_idx,\n 'missing': missing,\n 'formula': formula,\n 'design_info': design_info})\n  mod = cls(endog, exog, *args, **kwargs)\n  mod.formula = formula\n  mod.data.frame = data\n  return mod", "target": 0, "info": "Null", "idx": 0}
{"func": "def from_formula(cls, formula, data, subset=None, drop_cols=None, *args, **kwargs):\n  if subset is not None:\n  data = data.ix[subset]\n  eval_env = kwargs.pop('eval_env', None)\n  if eval_env is None:\n  eval_env = 2\n  elif eval_env == -1:\n  from patsy import EvalEnvironment\n  eval_env = EvalEnvironment({})\n  else:\n  eval_env += 1\n  missing = kwargs.get('missing', 'drop')\n  if missing == 'none':\n  missing = 'raise'\n  tmp = handle_formula_data(data, None, formula, depth=eval_env,\n    missing=missing)\n  ((endog, exog), missing_idx, design_info) = tmp\n  if drop_cols is not None and len(drop_cols) > 0:\n  cols = [x for x in exog.columns if x not in drop_cols]\n  if len(cols) < len(exog.columns):\n  exog = exog[cols]\n  cols = list(design_info.term_names)\n  for col in cols:\n  try:\n  cols.remove(col)\n  except ValueError:\n  pass\n  design_info = design_info.builder.subset(cols).design_info\n  kwargs.update({'missing_idx': missing_idx,\n 'missing': missing,\n 'formula': formula,\n 'design_info': design_info})\n  mod = cls(endog, exog, *args, **kwargs)\n  mod.formula = formula\n  mod.data.frame = data\n  return mod", "target": 1, "info": "Null", "idx": 0}
{"func": "def lowess(endog, exog, frac=2.0/3.0, it=3, delta=0.0, is_sorted=False,\n missing='drop', return_sorted=True):\n  endog = np.asarray(endog, float)\n  exog = np.asarray(exog, float)\n  if exog.ndim != 1:\n  raise ValueError('exog must be a vector')\n  if endog.ndim != 1:\n  raise ValueError('endog must be a vector')\n  if endog.shape[0] != exog.shape[0] :\n  raise ValueError('exog and endog must have same length')\n  if missing in ['drop', 'raise']:\n  mask_valid = (np.isfinite(exog) & np.isfinite(endog))\n  all_valid = np.all(mask_valid)\n  if all_valid:\n  y = endog\n  x = exog\n  else:\n  if missing == 'drop':\n  x = exog[mask_valid]\n  y = endog[mask_valid]\n  else:\n  raise ValueError('nan or inf found in data')\n  elif missing == 'none':\n  y = endog\n  x = exog\n  all_valid = True\n  else:\n  raise ValueError(\"missing can only be 'none', 'drop' or 'raise'\")\n  if not is_sorted:\n  sort_index = np.argsort(x)\n  x = np.array(x[sort_index])\n  y = np.array(y[sort_index])\n  res = _lowess(y, x, frac=frac, it=it, delta=delta)\n  _, yfitted = res.T\n  if return_sorted or (all_valid and is_sorted):\n  return res\n  else:\n  if not is_sorted:\n  yfitted_ = np.empty_like(y)\n  yfitted_.fill(np.nan)\n  yfitted_[sort_index] = yfitted\n  yfitted = yfitted_\n  if not all_valid:\n  yfitted_ = np.empty_like(endog)\n  yfitted_.fill(np.nan)\n  yfitted_[mask_valid] = yfitted\n  yfitted = yfitted_\n  return yfitted", "target": 0, "info": "Null", "idx": 0}
{"func": "def lowess(endog, exog, frac=2.0/3.0, it=3, delta=0.0, is_sorted=False,\n missing='drop', return_sorted=True):\n  endog = np.asarray(endog, float)\n  exog = np.asarray(exog, float)\n  if exog.ndim != 1:\n  raise ValueError('exog must be a vector')\n  if endog.ndim != 1:\n  raise ValueError('endog must be a vector')\n  if endog.shape[0] != exog.shape[0] :\n  raise ValueError('exog and endog must have same length')\n  if missing in ['drop', 'raise']:\n  mask_valid = (np.isfinite(exog) & np.isfinite(endog))\n  all_valid = np.all(mask_valid)\n  if all_valid:\n  y = endog\n  x = exog\n  else:\n  if missing == 'drop':\n  x = exog[mask_valid]\n  y = endog[mask_valid]\n  else:\n  raise ValueError('nan or inf found in data')\n  elif missing == 'none':\n  y = endog\n  x = exog\n  all_valid = True\n  else:\n  raise ValueError(\"missing can only be 'none', 'drop' or 'raise'\")\n  if not is_sorted:\n  sort_index = np.argsort(x)\n  x = np.array(x[sort_index])\n  y = np.array(y[sort_index])\n  res = _lowess(y, x, frac=frac, it=it, delta=delta)\n  _, yfitted = res.T\n  if return_sorted or (all_valid and is_sorted):\n  return res\n  else:\n  if not is_sorted:\n  yfitted_ = np.empty_like(endog)\n  yfitted_.fill(np.nan)\n  yfitted_[sort_index] = yfitted\n  yfitted = yfitted_\n  if not all_valid:\n  yfitted_ = np.empty_like(endog)\n  yfitted_.fill(np.nan)\n  yfitted_[mask_valid] = yfitted\n  yfitted = yfitted_\n  return yfitted", "target": 1, "info": "Null", "idx": 0}
{"func": " def _fit_gradient(self, start_params=None, method=\"newton\",\n    maxiter=100, tol=1e-8, full_output=True,\n    disp=True, scale=None, cov_type='nonrobust',\n    cov_kwds=None, use_t=None, max_start_irls=3,\n    **kwargs):\n   if (max_start_irls > 0) and (start_params is None):\n  irls_rslt = self._fit_irls(start_params=start_params, maxiter=max_start_irls,\n tol=tol, scale=scale, cov_type=cov_type,\n cov_kwds=cov_kwds, use_t=use_t, **kwargs)\n  start_params = irls_rslt.params\n  rslt = super(GLM, self).fit(start_params=start_params, tol=tol,\n  maxiter=maxiter, full_output=full_output,\n  method=method, disp=disp, **kwargs)\n  self.mu = self.predict(rslt.params)\n  self.scale = self.estimate_scale(self.mu)\n  glm_results = GLMResults(self, rslt.params,\n   rslt.normalized_cov_params / self.scale,\n   self.scale,\n   cov_type=cov_type, cov_kwds=cov_kwds,\n   use_t=use_t)\n  history = {'iteration': 0}\n  if full_output:\n  glm_results.mle_retvals = rslt.mle_retvals\n  if 'iterations' in rslt.mle_retvals:\n  history['iteration'] = rslt.mle_retvals['iterations']\n  glm_results.method = method\n  glm_results.fit_history = history\n  return GLMResultsWrapper(glm_results)", "target": 0, "info": "Null", "idx": 0}
{"func": "  def _fit_gradient(self, start_params=None, method=\"newton\",\n    maxiter=100, tol=1e-8, full_output=True,\n    disp=True, scale=None, cov_type='nonrobust',\n    cov_kwds=None, use_t=None, max_start_irls=3,\n    **kwargs):\n   if (max_start_irls > 0) and (start_params is None):\n  irls_rslt = self._fit_irls(start_params=start_params, maxiter=maxiter,\n tol=tol, scale=scale, cov_type=cov_type,\n cov_kwds=cov_kwds, use_t=use_t, **kwargs)\n  start_params = irls_rslt.params\n  rslt = super(GLM, self).fit(start_params=start_params, tol=tol,\n  maxiter=maxiter, full_output=full_output,\n  method=method, disp=disp, **kwargs)\n  self.mu = self.predict(rslt.params)\n  self.scale = self.estimate_scale(self.mu)\n  glm_results = GLMResults(self, rslt.params,\n   rslt.normalized_cov_params / self.scale,\n   self.scale,\n   cov_type=cov_type, cov_kwds=cov_kwds,\n   use_t=use_t)\n  history = {'iteration': 0}\n  if full_output:\n  glm_results.mle_retvals = rslt.mle_retvals\n  if 'iterations' in rslt.mle_retvals:\n  history['iteration'] = rslt.mle_retvals['iterations']\n  glm_results.method = method\n  glm_results.fit_history = history\n  return GLMResultsWrapper(glm_results)", "target": 1, "info": "Null", "idx": 0}
{"func": "def summary(self, yname=None, xname=None, title=None, alpha=.05):\n  top_left = [('Dep. Variable:', None),\n  ('Model:', None),\n  ('Model Family:', [self.family.__class__.__name__]),\n  ('Link Function:', [self.family.link.__class__.__name__]),\n  ('Method:', [self.method]),\n  ('Date:', None),\n  ('Time:', None),\n  ('No. Iterations:',\n   [\"%d\" % self.fit_history['iteration']]),\n  ]\n  top_right = [('No. Observations:', None),\n   ('Df Residuals:', None),\n   ('Df Model:', None),\n   ('Scale:', [\"%\n   ('Log-Likelihood:', None),\n   ('Deviance:', [\"%\n   ('Pearson chi2:', [\"%\n   ]\n   if hasattr(self, 'cov_type'):\n  top_left.append(('Covariance Type:', [self.cov_type]))\n   if title is None:\n  title = \"Generalized Linear Model Regression Results\"\n  from statsmodels.iolib.summary import Summary\n  smry = Summary()\n  smry.add_table_2cols(self, gleft=top_left, gright=top_right,\n   yname=yname, xname=xname, title=title)\n  smry.add_table_params(self, yname=yname, xname=xname, alpha=alpha,\n    use_t=self.use_t)\n  if hasattr(self, 'constraints'):\n  smry.add_extra_txt(['Model has been estimated subject to linear '\n  'equality constraints.'])\n  return smry", "target": 0, "info": "Null", "idx": 0}
{"func": "def summary(self, yname=None, xname=None, title=None, alpha=.05):\n  top_left = [('Dep. Variable:', None),\n  ('Model:', None),\n  ('Model Family:', [self.family.__class__.__name__]),\n  ('Link Function:', [self.family.link.__class__.__name__]),\n  ('Method:', [self.method]),\n  ('Date:', None),\n  ('Time:', None),\n  ('No. Iterations:',\n   [\"%d\" % self.fit_history['iteration']]),\n  ]\n  top_right = [('No. Observations:', None),\n   ('Df Residuals:', None),\n   ('Df Model:', None),\n   ('Scale:', [\"%\n   ('Log-Likelihood:', None),\n   ('Deviance:', [\"%\n   ('Pearson chi2:', [\"%\n   ]\n   if hasattr(self, 'cov_type'):\n  top_right.append(('Covariance Type:', [self.cov_type]))\n   if title is None:\n  title = \"Generalized Linear Model Regression Results\"\n  from statsmodels.iolib.summary import Summary\n  smry = Summary()\n  smry.add_table_2cols(self, gleft=top_left, gright=top_right,\n   yname=yname, xname=xname, title=title)\n  smry.add_table_params(self, yname=yname, xname=xname, alpha=alpha,\n    use_t=self.use_t)\n  if hasattr(self, 'constraints'):\n  smry.add_extra_txt(['Model has been estimated subject to linear '\n  'equality constraints.'])\n  return smry", "target": 1, "info": "Null", "idx": 0}
{"func": "   def transform_params(self, unconstrained):\n  unconstrained = np.array(unconstrained, ndmin=1)\n  constrained = np.zeros(unconstrained.shape, dtype=unconstrained.dtype)\n  constrained[self._params_trend] = unconstrained[self._params_trend]\n  if self.k_ar > 0 and self.enforce_stationarity:\n  if self.error_cov_type == 'diagonal':\n  state_cov = np.diag(unconstrained[self._params_state_cov]**2)\n  elif self.error_cov_type == 'unstructured':\n  state_cov_lower = np.zeros(self.ssm['state_cov'].shape,\n dtype=unconstrained.dtype)\n  state_cov_lower[self._idx_lower_state_cov] = (\n  unconstrained[self._params_state_cov])\n  state_cov = np.dot(state_cov_lower, state_cov_lower.T)\n  coefficients = unconstrained[self._params_ar].reshape(\n  self.k_endog, self.k_endog * self.k_ar)\n  coefficient_matrices, variance = (\n  constrain_stationary_multivariate(coefficients, state_cov))\n  constrained[self._params_ar] = coefficient_matrices.ravel()\n  else:\n  constrained[self._params_ar] = unconstrained[self._params_ar]\n  if self.k_ma > 0 and self.enforce_invertibility:\n  state_cov = np.eye(self.k_endog, dtype=unconstrained.dtype)\n  coefficients = unconstrained[self._params_ma].reshape(\n  self.k_endog, self.k_endog * self.k_ma)\n  coefficient_matrices, variance = (\n  constrain_stationary_multivariate(coefficients, state_cov))\n  constrained[self._params_ma] = coefficient_matrices.ravel()\n  else:\n  constrained[self._params_ma] = unconstrained[self._params_ma]\n  constrained[self._params_regression] = (\n  unconstrained[self._params_regression])\n  if self.error_cov_type == 'diagonal':\n  constrained[self._params_state_cov] = (\n  unconstrained[self._params_state_cov]**2)\n  elif self.error_cov_type == 'unstructured':\n  constrained[self._params_state_cov] = (\n  unconstrained[self._params_state_cov])\n  if self.measurement_error:\n  constrained[self._params_obs_cov] = (\n  unconstrained[self._params_obs_cov]**2)\n   return constrained", "target": 0, "info": "Null", "idx": 0}
{"func": " def transform_params(self, unconstrained):\n  unconstrained = np.array(unconstrained, ndmin=1)\n  constrained = np.zeros(unconstrained.shape, dtype=unconstrained.dtype)\n  constrained[self._params_trend] = unconstrained[self._params_trend]\n  if self.k_ar > 0 and self.enforce_stationarity:\n  if self.error_cov_type == 'diagonal':\n  state_cov = np.diag(unconstrained[self._params_state_cov]**2)\n  elif self.error_cov_type == 'unstructured':\n  state_cov_lower = np.zeros(self.ssm['state_cov'].shape,\n dtype=unconstrained.dtype)\n  state_cov_lower[self._idx_lower_state_cov] = (\n  unconstrained[self._params_state_cov])\n  state_cov = np.dot(state_cov_lower, state_cov_lower.T)\n  coefficients = unconstrained[self._params_ar].reshape(\n  self.k_endog, self.k_endog * self.k_ar)\n  coefficient_matrices, variance = (\n  constrain_stationary_multivariate(coefficients, state_cov))\n  constrained[self._params_ar] = coefficient_matrices.ravel()\n  else:\n  constrained[self._params_ar] = unconstrained[self._params_ar]\n  if self.k_ma > 0 and self.enforce_invertibility:\n  state_cov = np.eye(self.k_endog, dtype=unconstrained.dtype)\n  coefficients = unconstrained[self._params_ma].reshape(\n  self.k_endog, self.k_endog * self.k_ma)\n  coefficient_matrices, variance = (\n  constrain_stationary_multivariate(coefficients, state_cov))\n  constrained[self._params_ma] = coefficient_matrices.ravel()\n  else:\n  constrained[self._params_ma] = unconstrained[self._params_ma]\n  constrained[self._params_regression] = (\n  unconstrained[self._params_regression])\n  if self.error_cov_type == 'diagonal':\n  constrained[self._params_state_cov] = (\n  unconstrained[self._params_state_cov]**2)\n  elif self.error_cov_type == 'unstructured':\n  constrained[self._params_state_cov] = (\n  unconstrained[self._params_state_cov])\n  if self.measurement_error:\n  constrained[self._params_obs_cov] = (\n  constrained[self._params_obs_cov]**2)\n   return constrained", "target": 1, "info": "Null", "idx": 0}
{"func": "def lowess(endog, exog, frac=2.0/3.0, it=3, delta=0.0, is_sorted=False,\n missing='drop', return_sorted=True):\n  endog = np.asarray(endog, float)\n  exog = np.asarray(exog, float)\n  if exog.ndim != 1:\n  raise ValueError('exog must be a vector')\n  if endog.ndim != 1:\n  raise ValueError('endog must be a vector')\n  if endog.shape[0] != exog.shape[0] :\n  raise ValueError('exog and endog must have same length')\n  if missing in ['drop', 'raise']:\n  mask_valid = (np.isfinite(exog) & np.isfinite(endog))\n  all_valid = np.all(mask_valid)\n  if all_valid:\n  y = endog\n  x = exog\n  else:\n  if missing == 'drop':\n  x = exog[mask_valid]\n  y = endog[mask_valid]\n  else:\n  raise ValueError('nan or inf found in data')\n  elif missing == 'none':\n  y = endog\n  x = exog\n  all_valid = True\n  else:\n  raise ValueError(\"missing can only be 'none', 'drop' or 'raise'\")\n   if not is_sorted:\n  sort_index = np.argsort(x)\n  x = np.array(x[sort_index])\n  y = np.array(y[sort_index])\n   res = _lowess(y, x, frac=frac, it=it, delta=delta)\n  _, yfitted = res.T\n  if return_sorted or (all_valid and is_sorted):\n  return res\n  else:\n  if not is_sorted:\n  yfitted_ = np.empty_like(endog)\n  yfitted_.fill(np.nan)\n  yfitted_[sort_index] = yfitted\n  yfitted = yfitted_\n  if not all_valid:\n  yfitted_ = np.empty_like(endog)\n  yfitted_.fill(np.nan)\n  yfitted_[mask_valid] = yfitted\n  yfitted = yfitted_\n  return yfitted", "target": 0, "info": "Null", "idx": 0}
{"func": "def lowess(endog, exog, frac=2.0/3.0, it=3, delta=0.0, is_sorted=False,\n missing='drop', return_sorted=True):\n  endog = np.asarray(endog, float)\n  exog = np.asarray(exog, float)\n  if exog.ndim != 1:\n  raise ValueError('exog must be a vector')\n  if endog.ndim != 1:\n  raise ValueError('endog must be a vector')\n  if endog.shape[0] != exog.shape[0] :\n  raise ValueError('exog and endog must have same length')\n  if missing in ['drop', 'raise']:\n  mask_valid = (np.isfinite(exog) & np.isfinite(endog))\n  all_valid = np.all(mask_valid)\n  if all_valid:\n  y = endog\n  x = exog\n  else:\n  if missing == 'drop':\n  x = exog[mask_valid]\n  y = endog[mask_valid]\n  else:\n  raise ValueError('nan or inf found in data')\n  elif missing == 'none':\n  y = endog\n  x = exog\n  all_valid = True\n  else:\n  raise ValueError(\"missing can only be 'none', 'drop' or 'raise'\")\n   if not is_sorted:\n  sort_index = np.argsort(exog)\n  x = np.array(x[sort_index])\n  y = np.array(y[sort_index])\n   res = _lowess(y, x, frac=frac, it=it, delta=delta)\n  _, yfitted = res.T\n  if return_sorted or (all_valid and is_sorted):\n  return res\n  else:\n  if not is_sorted:\n  yfitted_ = np.empty_like(endog)\n  yfitted_.fill(np.nan)\n  yfitted_[sort_index] = yfitted\n  yfitted = yfitted_\n  if not all_valid:\n  yfitted_ = np.empty_like(endog)\n  yfitted_.fill(np.nan)\n  yfitted_[mask_valid] = yfitted\n  yfitted = yfitted_\n  return yfitted", "target": 1, "info": "Null", "idx": 0}
{"func": "def bkfilter(X, low=6, high=32, K=12):\n  if low < 2:\n  raise ValueError(\"low cannot be less than 2\")\n  X = np.asarray(X)\n  omega_1 = 2.*np.pi/high\n  omega_2 = 2.*np.pi/low\n  bweights = np.zeros(2*K+1)\n  bweights[K] = (omega_2 - omega_1)/np.pi\n  j = np.arange(1,int(K)+1)\n  weights = 1/(np.pi*j)*(np.sin(omega_2*j)-np.sin(omega_1*j))\n  bweights[K+j] = weights\n  bweights[:K] = weights[::-1]\n  bweights -= bweights.mean()\n  if X.ndim == 2:\n  bweights = bweights[:,None]\n  return fftconvolve(X, bweights, mode='valid')", "target": 0, "info": "Null", "idx": 0}
{"func": "def bkfilter(X, low=6, high=32, K=12):\n  if low < 2:\n  raise ValueError(\"low cannot be less than 2\")\n  X = np.asarray(X)\n  omega_1 = 2.*np.pi/high\n  omega_2 = 2.*np.pi/low\n  bweights = np.zeros(2*K+1)\n  bweights[K] = (omega_2 - omega_1)/np.pi\n  j = np.arange(1,int(K)+1)\n  weights = 1/(np.pi*j)*(np.sin(omega_2*j)-np.sin(omega_1*j))\n  bweights[K+j] = weights\n  bweights[:K] = weights[::-1]\n  bweights -= bweights.mean()\n  if X.ndim == 2:\n  bweights = bweights[:,None]\n  return fftconvolve(bweights, X, mode='valid')", "target": 1, "info": "Null", "idx": 0}
{"func": "def _check_wts(weights, wts):\n  if wts is not None:\n  import warnings\n  warnings.warn('`wts` method is deprecated. Use `weights` instead',\n    DeprecationWarning)\n  weights = weights if weights is not None else wts\n  return weights", "target": 0, "info": "Null", "idx": 0}
{"func": "def _check_wts(weights, wts):\n  if wts is not None:\n  import warnings\n  warnings.warn('`wts` method is deprecated. Use `weights` instead',\n    DeprecationWarning)\n  weights = weights if weights is not None else wts\n  return wts", "target": 1, "info": "Null", "idx": 0}
{"func": " def from_time(cls, command, user, time):\n  assert len(time) <= 5\n  padded_time = tuple(time) + ('*',) * (5 - len(time))\n  assert len(padded_time) == 5\n  return cls(command, padded_time[0], user, padded_time[1], padded_time[2], padded_time[3], padded_time[4])", "target": 0, "info": "Null", "idx": 0}
{"func": "def from_time(cls, command, user, time):\n  assert len(time) <= 5\n  padded_time = tuple(time) + ('*',) * (5 - len(time))\n  assert len(padded_time) == 5\n  return cls(command, time[0], user, padded_time[1], padded_time[2], padded_time[3], padded_time[4])", "target": 1, "info": "Null", "idx": 0}
{"func": " def from_time(cls, command, user, time):\n  if len(time) > 5:\n  raise RuntimeError(\"Malformed cronjob time: {!r}\".format(time))\n  padded_time = tuple(time) + ('*',) * (5 - len(time))\n  if len(padded_time) != 5:\n  raise RuntimeError(\"Malformed cronjob time: {!r}\".format(padded_time))\n  return cls(command, padded_time[0], user, padded_time[1], padded_time[2], padded_time[3], padded_time[4])", "target": 0, "info": "Null", "idx": 0}
{"func": "def from_time(cls, command, user, time):\n  if len(time) > 5:\n  raise RuntimeError(\"Malformed cronjob time: {!r}\".format(time))\n  padded_time = tuple(time) + ('*',) * (5 - len(time))\n  if len(padded_time) > 5:\n  raise RuntimeError(\"Malformed cronjob time: {!r}\".format(padded_time))\n  return cls(command, padded_time[0], user, padded_time[1], padded_time[2], padded_time[3], padded_time[4])", "target": 1, "info": "Null", "idx": 0}
{"func": " def get_dynamic_routes_instances(self, viewset, route, dynamic_routes):\n  dynamic_routes_instances = []\n  for httpmethods, methodname, endpoint, is_for_list in dynamic_routes:\n  initkwargs = route.initkwargs.copy()\n  initkwargs.update(getattr(viewset, methodname).kwargs)\n  dynamic_routes_instances.append(Route(\n  url=replace_methodname(route.url, endpoint),\n  mapping=dict((httpmethod, methodname) for httpmethod in httpmethods),\n  name=replace_methodname(route.name, endpoint),\n  initkwargs=initkwargs,\n  ))\n  return dynamic_routes_instances", "target": 0, "info": "Null", "idx": 0}
{"func": "def get_dynamic_routes_instances(self, viewset, route, dynamic_routes):\n  dynamic_routes_instances = []\n  for httpmethods, methodname, endpoint, is_for_list in dynamic_routes:\n  initkwargs = route.initkwargs.copy()\n  initkwargs.update(getattr(viewset, methodname).kwargs)\n  dynamic_routes_instances.append(Route(\n  url=replace_methodname(route.url, endpoint),\n  mapping=dict((httpmethod, methodname) for httpmethod in httpmethods),\n  name=replace_methodname(route.name, methodname),\n  initkwargs=initkwargs,\n  ))\n  return dynamic_routes_instances", "target": 1, "info": "Null", "idx": 0}
{"func": "def __init__(self, height=20, width=20, density=0.8, minority_pc=0.2, homophily=3):\n  self.height = height\n  self.width = width\n  self.density = density\n  self.minority_pc = minority_pc\n  self.homophily = homophily\n   self.schedule = RandomActivation(self)\n  self.grid = SingleGrid(width, height, torus=True)\n   self.happy = 0\n  self.datacollector = DataCollector(\n  {\"happy\": \"happy\"},\n  {\"x\": lambda a: a.pos[0], \"y\": lambda a: a.pos[1]})\n  for cell in self.grid.coord_iter():\n  x = cell[1]\n  y = cell[2]\n  if self.random.random() < self.density:\n  if self.random.random() < self.minority_pc:\n  agent_type = 1\n  else:\n  agent_type = 0\n  agent = SchellingAgent((x, y), self, agent_type)\n  self.grid.position_agent(agent, (x, y))\n  self.schedule.add(agent)\n  self.running = True\n  self.datacollector.collect(self)", "target": 0, "info": "Null", "idx": 0}
{"func": "def __init__(self, height=20, width=20, density=0.8, minority_pc=0.2, homophily=3):\n  self.height = height\n  self.width = width\n  self.density = density\n  self.minority_pc = minority_pc\n  self.homophily = homophily\n   self.schedule = RandomActivation(self)\n  self.grid = SingleGrid(height, width, torus=True)\n   self.happy = 0\n  self.datacollector = DataCollector(\n  {\"happy\": \"happy\"},\n  {\"x\": lambda a: a.pos[0], \"y\": lambda a: a.pos[1]})\n  for cell in self.grid.coord_iter():\n  x = cell[1]\n  y = cell[2]\n  if self.random.random() < self.density:\n  if self.random.random() < self.minority_pc:\n  agent_type = 1\n  else:\n  agent_type = 0\n  agent = SchellingAgent((x, y), self, agent_type)\n  self.grid.position_agent(agent, (x, y))\n  self.schedule.add(agent)\n  self.running = True\n  self.datacollector.collect(self)", "target": 1, "info": "Null", "idx": 0}
{"func": "def __init__(self, height=50, width=50, schedule_type=\"Random\", payoffs=None, seed=None):\n  self.grid = SingleGrid(width, height, torus=True)\n  self.schedule_type = schedule_type\n  self.schedule = self.schedule_types[self.schedule_type](self)\n  for x in range(width):\n  for y in range(height):\n  agent = PDAgent((x, y), self)\n  self.grid.place_agent(agent, (x, y))\n  self.schedule.add(agent)\n  self.datacollector = DataCollector({\n  \"Cooperating_Agents\":\n  lambda m: len([a for a in m.schedule.agents if a.move == \"C\"])\n  })\n  self.running = True\n  self.datacollector.collect(self)", "target": 0, "info": "Null", "idx": 0}
{"func": "def __init__(self, height=50, width=50, schedule_type=\"Random\", payoffs=None, seed=None):\n  self.grid = SingleGrid(height, width, torus=True)\n  self.schedule_type = schedule_type\n  self.schedule = self.schedule_types[self.schedule_type](self)\n  for x in range(width):\n  for y in range(height):\n  agent = PDAgent((x, y), self)\n  self.grid.place_agent(agent, (x, y))\n  self.schedule.add(agent)\n  self.datacollector = DataCollector({\n  \"Cooperating_Agents\":\n  lambda m: len([a for a in m.schedule.agents if a.move == \"C\"])\n  })\n  self.running = True\n  self.datacollector.collect(self)", "target": 1, "info": "Null", "idx": 0}
{"func": "  def request(self, peer, request):\n  qname = request.q.qname\n  qtype = request.q.qtype\n  qclass = request.q.qclass\n  key = (qname, qtype, qclass)\n  if key in self.cache:\n  self.logger.info(\n  \"Cached Request ({0:s}): {1:s} {2:s} {3:s}\".format(\n  \"{0:s}:{1:d}\".format(*peer),\n  CLASS.get(qclass), QTYPE.get(qtype), qname\n  )\n  )\n  reply = request.reply()\n  for rr in self.cache[key]:\n  reply.add_answer(rr)\n  self.fire(write(peer, reply.pack()))\n  return\n  records = Record.objects.filter(\n  rname=qname, rclass=qclass, rtype=qtype\n  )\n  if records:\n  self.logger.info(\n  \"Authoritative Request ({0:s}): {1:s} {2:s} {3:s}\".format(\n  \"{0:s}:{1:d}\".format(*peer),\n  CLASS.get(qclass), QTYPE.get(qtype), qname\n  )\n  )\n  rr = [record.rr for record in records]\n  reply = request.reply()\n  reply.add_answer(*rr)\n  self.cache[key] = rr\n  self.fire(write(peer, reply.pack()))\n  return\n  self.logger.info(\n  \"Request ({0:s}): {1:s} {2:s} {3:s}\".format(\n  \"{0:s}:{1:d}\".format(*peer),\n  CLASS.get(qclass), QTYPE.get(qtype), qname\n  )\n  )\n   lookup = DNSRecord(q=DNSQuestion(qname, qtype, qclass))\n  id = lookup.header.id\n  self.peers[id] = peer\n  self.requests[id] = request\n  self.fire(write((self.forward, 53), lookup.pack()))", "target": 0, "info": "Null", "idx": 0}
{"func": "def request(self, peer, request):\n  qname = request.q.qname\n  qtype = request.q.qtype\n  qclass = request.q.qclass\n  key = (qname, qtype, qclass)\n  if key in self.cache:\n  self.logger.info(\n  \"Cached Request ({0:s}): {1:s} {2:s} {3:s}\".format(\n  \"{0:s}:{1:d}\".format(*peer),\n  CLASS.get(qclass), QTYPE.get(qtype), qname\n  )\n  )\n  reply = request.reply()\n  for rr in self.cache[key]:\n  reply.add_answer(rr)\n  self.fire(write(peer, reply.pack()))\n  return\n  records = Record.objects.filter(\n  rname=qname, rclass=qclass, rtype=qtype\n  )\n  if records:\n  self.logger.info(\n  \"Authoritative Request ({0:s}): {1:s} {2:s} {3:s}\".format(\n  \"{0:s}:{1:d}\".format(*peer),\n  CLASS.get(qclass), QTYPE.get(qtype), qname\n  )\n  )\n  rr = [record.rr for record in records]\n  reply = request.reply()\n  reply.add_answer(*rr)\n  self.cache[key] = rr\n  self.fire(write(peer, reply.pack()))\n  return\n  self.logger.info(\n  \"Request ({0:s}): {1:s} {2:s} {3:s}\".format(\n  \"{0:s}:{1:d}\".format(*peer),\n  CLASS.get(qclass), QTYPE.get(qtype), qname\n  )\n  )\n   lookup = DNSRecord(q=DNSQuestion(qname, qclass, qtype))\n  id = lookup.header.id\n  self.peers[id] = peer\n  self.requests[id] = request\n  self.fire(write((self.forward, 53), lookup.pack()))", "target": 1, "info": "Null", "idx": 0}
{"func": "def __get_safe_conn(self, retry_count):\n  self.current_size += 1\n  c = self.unuse_list.pop()\n  if self.ping_check:\n  now = int(time())\n  timeout = now\n  if isinstance(self.ping_check, int):\n  timeout = timeout - self.ping_check\n  if not hasattr(c, '__ping_check_timestamp'):\n  c.__ping_check_timestamp = now\n  try:\n  if c.__ping_check_timestamp < timeout:\n  c.__ping_check_timestamp = now\n  c.ping()\n  except:\n  self.current_size -= 1\n  if retry_count < 10: c = self.__get_conn(retry_count+1)\n  if c: self.inuse_list.add(c)\n  return c", "target": 0, "info": "Null", "idx": 0}
{"func": "def __get_safe_conn(self, retry_count):\n  self.current_size += 1\n  c = self.unuse_list.pop()\n  if self.ping_check:\n  now = int(time())\n  timeout = now\n  if isinstance(int, self.ping_check):\n  timeout = timeout - self.ping_check\n  if not hasattr(c, '__ping_check_timestamp'):\n  c.__ping_check_timestamp = now\n  try:\n  if c.__ping_check_timestamp < timeout:\n  c.__ping_check_timestamp = now\n  c.ping()\n  except:\n  self.current_size -= 1\n  if retry_count < 10: c = self.__get_conn(retry_count+1)\n  if c: self.inuse_list.add(c)\n  return c", "target": 1, "info": "Null", "idx": 0}
{"func": "def setFilterNamespace(self, namespace):\n  if not namespace:\n  self.namespacesFilter = [ \"prymatex\", \"user\" ]\n  else:\n  self.namespacesFilter = namespace.split()\n  self.setFilterRegExp(\"\")", "target": 0, "info": "Null", "idx": 0}
{"func": "def setFilterNamespace(self, namespace):\n  if namespace:\n  self.namespacesFilter = [ \"prymatex\", \"user\" ]\n  else:\n  self.namespacesFilter = namespace.split()\n  self.setFilterRegExp(\"\")", "target": 1, "info": "Null", "idx": 0}
{"func": "   def __deepcopy__(self, memo):\n  snippet = PMXSnippet(self.namespace, self.hash)\n  memo[\"snippet\"] = deepcopy(self.snippet, memo)\n  snippet.bundle = self.bundle\n  return snippet", "target": 0, "info": "Null", "idx": 0}
{"func": "def __deepcopy__(self, memo):\n  snippet = PMXSnippet(self.hash, self.namespace)\n  memo[\"snippet\"] = deepcopy(self.snippet, memo)\n  snippet.bundle = self.bundle\n  return snippet", "target": 1, "info": "Null", "idx": 0}
{"func": "   def blockRevision(self, block):\n  return _revision(self.scope_name, block.text() + \"\\n\", block.previous().userState())", "target": 0, "info": "Null", "idx": 0}
{"func": "  def blockRevision(self, block):\n  return _revision(block.text() + \"\\n\", self.scope_name, block.previous().userState())", "target": 1, "info": "Null", "idx": 0}
{"func": "  def tokenAtPosition(self, pos):\n  for token in self.__tokens[::-1]:\n  if token.start <= pos < token.end:\n  return token", "target": 0, "info": "Null", "idx": 0}
{"func": "  def tokenAtPosition(self, pos):\n  for token in self.__tokens[::-1]:\n  if token.start <= pos <= token.end:\n  return token", "target": 1, "info": "Null", "idx": 0}
{"func": " def flyweightScopeFactory(cls, scopeStack):\n  scopeHash = hash(scopeStack)\n  if scopeHash not in cls.SCOPES:\n  scopeName = \" \".join(scopeStack)\n  cls.SCOPES[scopeHash] = CodeEditorScope(\n  name = scopeName,\n  path = scopeStack,\n  settings = cls.application.supportManager.getPreferenceSettings(scopeStack),\n  group = PMXSyntax.findGroup(scopeStack[::-1])\n  )\n  return scopeHash", "target": 0, "info": "Null", "idx": 0}
{"func": "def flyweightScopeFactory(cls, scopeStack):\n  scopeHash = hash(scopeStack)\n  if scopeHash not in cls.SCOPES:\n  scopeName = \" \".join(scopeStack)\n  cls.SCOPES[scopeHash] = CodeEditorScope(\n  name = scopeName,\n  path = scopeStack,\n  settings = cls.application.supportManager.getPreferenceSettings(scopeName),\n  group = PMXSyntax.findGroup(scopeStack[::-1])\n  )\n  return scopeHash", "target": 1, "info": "Null", "idx": 0}
{"func": " def get_queryset(self, filter = True):\n  key = self.kwargs['filter_key']\n  value = self.kwargs['filter_value']\n   filter_dict = {}\n   if not key in self.filter_keys:\n  raise ImproperlyConfigured(\n  \"%s is not present in filter_keys (%s)\" % (key, self.filter_keys)\n  )\n  key = \"__\".join(key.split(\".\"))\n  if filter:\n  filter_dict = { key : value }\n  if self.queryset is not None:\n  queryset = self.queryset\n  if isinstance(queryset, QuerySet):\n  queryset = queryset.filter(**filter_dict)\n  elif self.model is not None:\n  queryset = self.model._default_manager.filter(**filter_dict)\n  else:\n  raise ImproperlyConfigured(\n  \"%(cls)s is missing a QuerySet. Define \"\n  \"%(cls)s.model, %(cls)s.queryset, or override \"\n  \"%(cls)s.get_queryset().\" % {\n  'cls': self.__class__.__name__\n  }\n  )\n  return queryset", "target": 0, "info": "Null", "idx": 0}
{"func": "def get_queryset(self, filter = True):\n  key = self.kwargs['filter_key']\n  value = self.kwargs['filter_value']\n   filter_dict = {}\n   if not value in self.filter_keys:\n  raise ImproperlyConfigured(\n  \"%s is not present in filter_keys (%s)\" % (key, self.filter_keys)\n  )\n  key = \"__\".join(key.split(\".\"))\n  if filter:\n  filter_dict = { key : value }\n  if self.queryset is not None:\n  queryset = self.queryset\n  if isinstance(queryset, QuerySet):\n  queryset = queryset.filter(**filter_dict)\n  elif self.model is not None:\n  queryset = self.model._default_manager.filter(**filter_dict)\n  else:\n  raise ImproperlyConfigured(\n  \"%(cls)s is missing a QuerySet. Define \"\n  \"%(cls)s.model, %(cls)s.queryset, or override \"\n  \"%(cls)s.get_queryset().\" % {\n  'cls': self.__class__.__name__\n  }\n  )\n  return queryset", "target": 1, "info": "Null", "idx": 0}
{"func": " def make_name():\n  return cls.get_url_name(view)\n   if view not in cls.get_views():\n  raise ImproperlyConfigured(\n  \"Tried to get the URL patterns for a view (%s)\"\n  \" that is not defined by get_views\" % view\n  )\n  return [\n  url(\n  make_url(url_part),\n  make_view(),\n  name=make_name()\n  ) for url_part in view.get_url_parts()\n  ]", "target": 0, "info": "Null", "idx": 0}
{"func": "def make_name():\n  return cls.get_url_name(view)\n   if view not in view.get_views():\n  raise ImproperlyConfigured(\n  \"Tried to get the URL patterns for a view (%s)\"\n  \" that is not defined by get_views\" % view\n  )\n  return [\n  url(\n  make_url(url_part),\n  make_view(),\n  name=make_name()\n  ) for url_part in view.get_url_parts()\n  ]", "target": 1, "info": "Null", "idx": 0}
{"func": " def _get_url_name(cls):\n  return cls.get_url_name(view_class, prefix=True),\n  _get_url_name.__doc__ = \"Get %s URL\" % view_class.get_action_name()\n  _get_url_name = classmethod(_get_url_name)\n   setattr(ModelMixin,\n  'get_%s_url_name' % view_class.get_underscored_action_name(),\n  _get_url_name)\n   if extra_funcs:\n  for func_name, func in extra_funcs.items():\n  func_name = try_calling(func_name, view_class) or func_name\n  setattr(ModelMixin,\n  func_name,\n  func)\n  return ModelMixin", "target": 0, "info": "Null", "idx": 0}
{"func": " def _get_url_name(cls):\n  return cls.get_url_name(view_class, prefix=True),\n  _get_url_name.__doc__ = \"Get %s URL\" % view_class.get_action_name()\n  _get_url_name = classmethod(_get_url_name)\n   setattr(ModelMixin,\n  'get_%s_url_name' % view_class.get_underscored_action_name(),\n  _get_url)\n   if extra_funcs:\n  for func_name, func in extra_funcs.items():\n  func_name = try_calling(func_name, view_class) or func_name\n  setattr(ModelMixin,\n  func_name,\n  func)\n  return ModelMixin", "target": 1, "info": "Null", "idx": 0}
{"func": "def main():\n  parser = ArgumentParser()\n  parser.add_argument(\"--infile\", action = \"store\", dest=\"infile\", help=\"Input BAM file\", required=True)\n  parser.add_argument(\"--outfile\", action = \"store\", dest=\"outfile\", help=\"Output BAM file\", required=True)\n  parser.add_argument(\"--bedfile\", action=\"store\", dest=\"bedfile\",\n  help=\"Bedfile containing coordinates to subdivide the BAM file (Recommendation: cytoband.txt - \\\n  See bed_separator.R for making your own bed file based on a target panel/specific coordinates)\",\n  required=False)\n  args = parser.parse_args()\n    SETUP\n  start_time = time.time()\n  args.infile = str(args.infile)\n  args.outfile = str(args.outfile)\n  sscs_bam = pysam.AlignmentFile(args.infile, \"rb\")\n  dcs_bam = pysam.AlignmentFile(args.outfile, \"wb\", template=sscs_bam)\n   if re.search('dcs.sc', args.outfile) is not None:\n  sscs_singleton_bam = pysam.AlignmentFile('{}.sscs.sc.singleton.bam'.format(args.outfile.split('.dcs.sc')[0]),\n   \"wb\", template=sscs_bam)\n  dcs_header = \"DCS - Singleton Correction\"\n  sc_header = \" SC\"\n  else:\n  sscs_singleton_bam = pysam.AlignmentFile('{}.sscs.singleton.bam'.format(args.outfile.split('.dcs')[0]),\n   \"wb\", template=sscs_bam)\n  dcs_header = \"DCS\"\n  sc_header = \"\"\n  stats = open('{}.stats.txt'.format(args.outfile.split('.dcs')[0]), 'a')\n  time_tracker = open('{}.time_tracker.txt'.format(args.outfile.split('.dcs')[0]), 'a')\n  read_dict = collections.OrderedDict()\n  tag_dict = collections.defaultdict(int)\n  pair_dict = collections.defaultdict(list)\n  csn_pair_dict = collections.defaultdict(list)\n  unmapped = 0\n  unmapped_mate = 0\n  multiple_mapping = 0\n  counter = 0\n  sscs_singletons = 0\n  multiple_mappings = 0\n  duplex_count = 0\n  duplex_dict = collections.defaultdict(int)\n  if args.bedfile is not None:\n  division_coor = bed_separator(args.bedfile)\n  else:\n  division_coor = [1]\n  for x in division_coor:\n  if division_coor == [1]:\n  read_chr = None\n  read_start = None\n  read_end = None\n  else:\n  read_chr = x.split('_', 1)[0]\n  read_start = division_coor[x][0]\n  read_end = division_coor[x][1]\n  chr_data = read_bam(sscs_bam,\n  pair_dict=pair_dict,\n  read_dict=read_dict,\n  csn_pair_dict=csn_pair_dict,\n  tag_dict=tag_dict,\n  badRead_bam=None,\n  duplex=True,\n  read_chr=read_chr,\n  read_start=read_start,\n  read_end=read_end\n  )\n  read_dict = chr_data[0]\n  tag_dict = chr_data[1]\n  pair_dict = chr_data[2]\n  csn_pair_dict = chr_data[3]\n  counter += chr_data[4]\n  unmapped += chr_data[5]\n  multiple_mapping += chr_data[6]\n  CONSENSUS\n  for readPair in list(csn_pair_dict.keys()):\n  for tag in csn_pair_dict[readPair]:\n  ds = duplex_tag(tag)\n  if ds not in duplex_dict.keys():\n  if tag in tag_dict and ds in tag_dict:\n  duplex_count += 1\n  consensus_seq, consensus_qual = duplex_consensus(read_dict[tag][0], read_dict[ds][0])\n  dcs_query_name = dcs_consensus_tag(read_dict[tag][0].qname, read_dict[ds][0].qname)\n  dcs_read = create_aligned_segment([read_dict[tag][0], read_dict[ds][0]], consensus_seq,\n    consensus_qual, dcs_query_name)\n  duplex_dict[tag] += 1\n  dcs_bam.write(dcs_read)\n  else:\n  sscs_singleton_bam.write(read_dict[tag][0])\n  sscs_singletons += 1\n  del read_dict[tag]\n  del csn_pair_dict[readPair]\n    SUMMARY\n  summary_stats = '''\n sc_header, duplex_count, sc_header, sscs_singletons)\n  stats.write(summary_stats)\n  print(summary_stats)\n  time_tracker.write('DCS: ')\n  time_tracker.write(str((time.time() - start_time)/60) + '\\n')\n  time_tracker.close()\n  stats.close()\n  dcs_bam.close()\n  sscs_singleton_bam.close()\n  return duplex_dict", "target": 0, "info": "Null", "idx": 0}
{"func": "def main():\n  parser = ArgumentParser()\n  parser.add_argument(\"--infile\", action = \"store\", dest=\"infile\", help=\"Input BAM file\", required=True)\n  parser.add_argument(\"--outfile\", action = \"store\", dest=\"outfile\", help=\"Output BAM file\", required=True)\n  parser.add_argument(\"--bedfile\", action=\"store\", dest=\"bedfile\",\n  help=\"Bedfile containing coordinates to subdivide the BAM file (Recommendation: cytoband.txt - \\\n  See bed_separator.R for making your own bed file based on a target panel/specific coordinates)\",\n  required=False)\n  args = parser.parse_args()\n    SETUP\n  start_time = time.time()\n  args.infile = str(args.infile)\n  args.outfile = str(args.outfile)\n  sscs_bam = pysam.AlignmentFile(args.infile, \"rb\")\n  dcs_bam = pysam.AlignmentFile(args.outfile, \"wb\", template=sscs_bam)\n   if re.search('dcs.sc', args.outfile) is None:\n  sscs_singleton_bam = pysam.AlignmentFile('{}.sscs.sc.singleton.bam'.format(args.outfile.split('.dcs.sc')[0]),\n   \"wb\", template=sscs_bam)\n  dcs_header = \"DCS - Singleton Correction\"\n  sc_header = \" SC\"\n  else:\n  sscs_singleton_bam = pysam.AlignmentFile('{}.sscs.singleton.bam'.format(args.outfile.split('.dcs')[0]),\n   \"wb\", template=sscs_bam)\n  dcs_header = \"DCS\"\n  sc_header = \"\"\n  stats = open('{}.stats.txt'.format(args.outfile.split('.dcs')[0]), 'a')\n  time_tracker = open('{}.time_tracker.txt'.format(args.outfile.split('.dcs')[0]), 'a')\n  read_dict = collections.OrderedDict()\n  tag_dict = collections.defaultdict(int)\n  pair_dict = collections.defaultdict(list)\n  csn_pair_dict = collections.defaultdict(list)\n  unmapped = 0\n  unmapped_mate = 0\n  multiple_mapping = 0\n  counter = 0\n  sscs_singletons = 0\n  multiple_mappings = 0\n  duplex_count = 0\n  duplex_dict = collections.defaultdict(int)\n  if args.bedfile is not None:\n  division_coor = bed_separator(args.bedfile)\n  else:\n  division_coor = [1]\n  for x in division_coor:\n  if division_coor == [1]:\n  read_chr = None\n  read_start = None\n  read_end = None\n  else:\n  read_chr = x.split('_', 1)[0]\n  read_start = division_coor[x][0]\n  read_end = division_coor[x][1]\n  chr_data = read_bam(sscs_bam,\n  pair_dict=pair_dict,\n  read_dict=read_dict,\n  csn_pair_dict=csn_pair_dict,\n  tag_dict=tag_dict,\n  badRead_bam=None,\n  duplex=True,\n  read_chr=read_chr,\n  read_start=read_start,\n  read_end=read_end\n  )\n  read_dict = chr_data[0]\n  tag_dict = chr_data[1]\n  pair_dict = chr_data[2]\n  csn_pair_dict = chr_data[3]\n  counter += chr_data[4]\n  unmapped += chr_data[5]\n  multiple_mapping += chr_data[6]\n  CONSENSUS\n  for readPair in list(csn_pair_dict.keys()):\n  for tag in csn_pair_dict[readPair]:\n  ds = duplex_tag(tag)\n  if ds not in duplex_dict.keys():\n  if tag in tag_dict and ds in tag_dict:\n  duplex_count += 1\n  consensus_seq, consensus_qual = duplex_consensus(read_dict[tag][0], read_dict[ds][0])\n  dcs_query_name = dcs_consensus_tag(read_dict[tag][0].qname, read_dict[ds][0].qname)\n  dcs_read = create_aligned_segment([read_dict[tag][0], read_dict[ds][0]], consensus_seq,\n    consensus_qual, dcs_query_name)\n  duplex_dict[tag] += 1\n  dcs_bam.write(dcs_read)\n  else:\n  sscs_singleton_bam.write(read_dict[tag][0])\n  sscs_singletons += 1\n  del read_dict[tag]\n  del csn_pair_dict[readPair]\n    SUMMARY\n  summary_stats = '''\n sc_header, duplex_count, sc_header, sscs_singletons)\n  stats.write(summary_stats)\n  print(summary_stats)\n  time_tracker.write('DCS: ')\n  time_tracker.write(str((time.time() - start_time)/60) + '\\n')\n  time_tracker.close()\n  stats.close()\n  dcs_bam.close()\n  sscs_singleton_bam.close()\n  return duplex_dict", "target": 1, "info": "Null", "idx": 0}
{"func": " def __init__(self, session, user_name):\n  super(MediaList, self).__init__(session)\n  self.username = user_name\n  if not isinstance(self.username, unicode) or len(self.username) < 1:\n    raise InvalidMediaListError(self.username)\n  self._list = None\n  self._stats = None", "target": 0, "info": "Null", "idx": 0}
{"func": "  def __init__(self, session, user_name):\n  super(MediaList, self).__init__(session)\n  self.username = user_name\n  if not isinstance(self.username, unicode) or len(self.username) < 2:\n    raise InvalidMediaListError(self.username)\n  self._list = None\n  self._stats = None", "target": 1, "info": "Null", "idx": 0}
{"func": " def parse(self, html):\n  anime_info = {}\n  anime_page = bs4.BeautifulSoup(html)\n  title_tag = anime_page.find('div', {'id': 'contentWrapper'}).find('h1')\n  if not title_tag.find('div'):\n    error_tag = anime_page.find('h1', text=u'Invalid Request')\n    if error_tag:\n  raise InvalidAnimeError(self)\n    raise MalformedAnimePageError(self, html, message=\"Could not find title div\")\n  utilities.extract_tags(title_tag.find_all())\n  anime_info['title'] = title_tag.text.strip()\n  info_panel_first = anime_page.find('div', {'id': 'content'}).find('table').find('td')\n  picture_tag = info_panel_first.find('img')\n  anime_info['picture'] = picture_tag.get('src')\n  anime_info['alternative_titles'] = {}\n  alt_titles_header = info_panel_first.find('h2', text=u'Alternative Titles')\n  if alt_titles_header:\n    next_tag = alt_titles_header.find_next_sibling('div', {'class': 'spaceit_pad'})\n    while True:\n  if next_tag is None or not next_tag.find('span', {'class': 'dark_text'}):\n    break\n  language = next_tag.find('span').text[:-1]\n  utilities.extract_tags(next_tag.find_all('span', {'class': 'dark_text'}))\n  names = next_tag.text.strip().split(', ')\n  anime_info['alternative_titles'][language] = names\n  next_tag = next_tag.find_next_sibling('div', {'class': 'spaceit_pad'})\n  info_header = info_panel_first.find('h2', text=u'Information')\n  type_tag = info_header.find_next_sibling('div')\n  utilities.extract_tags(type_tag.find_all('span', {'class': 'dark_text'}))\n  anime_info['type'] = type_tag.text.strip()\n  episode_tag = type_tag.find_next_sibling('div')\n  utilities.extract_tags(episode_tag.find_all('span', {'class': 'dark_text'}))\n  anime_info['episodes'] = int(episode_tag.text.strip()) if episode_tag.text.strip() != 'Unknown' else 0\n  status_tag = episode_tag.find_next_sibling('div')\n  utilities.extract_tags(status_tag.find_all('span', {'class': 'dark_text'}))\n  anime_info['status'] = status_tag.text.strip()\n  aired_tag = status_tag.find_next_sibling('div')\n  utilities.extract_tags(aired_tag.find_all('span', {'class': 'dark_text'}))\n  aired_parts = aired_tag.text.strip().split(' to ')\n  if len(aired_parts) == 1:\n    try:\n  aired_date = parse_date(aired_parts[0])\n    except ValueError:\n  raise MalformedAnimePageError(self, aired_parts[0], message=\"Could not parse single air date\")\n    anime_info['aired'] = (aired_date,)\n  else:\n    try:\n  air_start = parse_date(aired_parts[0])\n    except ValueError:\n  raise MalformedAnimePageError(self, aired_parts[0], message=\"Could not parse first of two air dates\")\n    try:\n  air_end = parse_date(aired_parts[1])\n    except ValueError:\n  raise MalformedAnimePageError(self, aired_parts[1], message=\"Could not parse second of two air dates\")\n    anime_info['aired'] = (air_start, air_end)\n  producers_tag = aired_tag.find_next_sibling('div')\n  utilities.extract_tags(producers_tag.find_all('span', {'class': 'dark_text'}))\n  utilities.extract_tags(producers_tag.find_all('sup'))\n  anime_info['producers'] = producers_tag.text.strip().split(', ')\n  genres_tag = producers_tag.find_next_sibling('div')\n  utilities.extract_tags(genres_tag.find_all('span', {'class': 'dark_text'}))\n  anime_info['genres'] = genres_tag.text.strip().split(', ')\n  duration_tag = genres_tag.find_next_sibling('div')\n  utilities.extract_tags(duration_tag.find_all('span', {'class': 'dark_text'}))\n  anime_info['duration'] = duration_tag.text.strip()\n  duration_parts = [part.strip() for part in anime_info['duration'].split('.')]\n  duration_mins = 0\n  for part in duration_parts:\n    part_match = re.match('(?P<num>[0-9]+)', part)\n    if not part_match:\n  continue\n    part_volume = int(part_match.group('num'))\n    if part.endswith('hr'):\n  duration_mins += part_volume * 60\n    elif part.endswith('min'):\n  duration_mins += part_volume\n  anime_info['duration'] = duration_mins\n  rating_tag = duration_tag.find_next_sibling('div')\n  utilities.extract_tags(rating_tag.find_all('span', {'class': 'dark_text'}))\n  anime_info['rating'] = rating_tag.text.strip()\n  stats_header = anime_page.find('h2', text=u'Statistics')\n  weighted_score_tag = stats_header.find_next_sibling('div')\n  users_node = [x for x in weighted_score_tag.find_all('small') if u'scored by' in x.text][0]\n  weighted_users = int(users_node.text.split(u'scored by ')[-1].split(u' users')[0])\n  utilities.extract_tags(weighted_score_tag.find_all())\n  anime_info['weighted_score'] = (float(weighted_score_tag.text.strip()), weighted_users)\n  rank_tag = weighted_score_tag.find_next_sibling('div')\n  utilities.extract_tags(rank_tag.find_all())\n  anime_info['rank'] = int(rank_tag.text.strip()[1:].replace(',', '')) if rank_tag.text.strip()[1:].replace(',', '') != 'Unknown' else 0\n  popularity_tag = rank_tag.find_next_sibling('div')\n  utilities.extract_tags(popularity_tag.find_all())\n  anime_info['popularity'] = int(popularity_tag.text.strip()[1:].replace(',', '')) if popularity_tag.text.strip()[1:].replace(',', '') != 'Unknown' else 0\n  members_tag = popularity_tag.find_next_sibling('div')\n  utilities.extract_tags(members_tag.find_all())\n  anime_info['members'] = int(members_tag.text.strip().replace(',', '')) if members_tag.text.strip().replace(',', '') != 'Unknown' else 0\n  favorites_tag = members_tag.find_next_sibling('div')\n  utilities.extract_tags(favorites_tag.find_all())\n  anime_info['favorites'] = int(favorites_tag.text.strip().replace(',', '')) if favorites_tag.text.strip().replace(',', '') != 'Unknown' else 0\n  tags_header = anime_page.find('h2', text=u'Popular Tags')\n  tags_tag = tags_header.find_next_sibling('span')\n  anime_info['tags'] = tags_tag.text.strip().split(' ')\n  synopsis_elt = anime_page.find('h2', text=u'Synopsis').parent\n  utilities.extract_tags(synopsis_elt.find_all('h2'))\n  anime_info['synopsis'] = synopsis_elt.text.strip()\n  related_title = anime_page.find('h2', text=u'Related Anime')\n  if related_title:\n    related_elt = related_title.parent\n    utilities.extract_tags(related_elt.find_all('h2'))\n    related = {}\n    for link in related_elt.find_all('a'):\n  curr_elt = link.previous_sibling\n  if curr_elt is None:\n    break\n  related_type = None\n  while True:\n    if not curr_elt:\n  raise MalformedAnimePageError(self, related_elt, message=\"Prematurely reached end of related anime listing\")\n    if isinstance(curr_elt, bs4.NavigableString):\n  type_match = re.match('(?P<type>[a-zA-Z\\ \\-]+):', curr_elt)\n  if type_match:\n    related_type = type_match.group('type')\n    break\n    curr_elt = curr_elt.previous_sibling\n  href_parts = link.get('href').split('/')\n  title = link.text\n  obj_id = int(href_parts[4])\n  non_title_parts = href_parts[:5]\n  if 'manga' in non_title_parts:\n    new_obj = self.session.manga(obj_id).set({'title': title})\n  elif 'anime' in non_title_parts:\n    new_obj = self.session.anime(obj_id).set({'title': title})\n  else:\n    raise MalformedAnimePageError(self, link, message=\"Related thing is of unknown type\")\n  if related_type not in related:\n    related[related_type] = [new_obj]\n  else:\n    related[related_type].append(new_obj)\n    anime_info['related'] = related\n  else:\n    anime_info['related'] = None\n  return anime_info", "target": 0, "info": "Null", "idx": 0}
{"func": "def parse(self, html):\n  anime_info = {}\n  anime_page = bs4.BeautifulSoup(html)\n  title_tag = anime_page.find('div', {'id': 'contentWrapper'}).find('h1')\n  if not title_tag.find('div'):\n    error_tag = anime_page.find('h1', text=u'Invalid Request')\n    if error_tag:\n  raise InvalidAnimeError(self)\n    raise MalformedAnimePageError(self, html, message=\"Could not find title div\")\n  utilities.extract_tags(title_tag.find_all())\n  anime_info['title'] = title_tag.text.strip()\n  info_panel_first = anime_page.find('div', {'id': 'content'}).find('table').find('td')\n  picture_tag = info_panel_first.find('img')\n  anime_info['picture'] = picture_tag.get('src')\n  anime_info['alternative_titles'] = {}\n  alt_titles_header = info_panel_first.find('h2', text=u'Alternative Titles')\n  if alt_titles_header:\n    next_tag = alt_titles_header.find_next_sibling('div', {'class': 'spaceit_pad'})\n    while True:\n  if next_tag is None or not next_tag.find('span', {'class': 'dark_text'}):\n    break\n  language = next_tag.find('span').text[:-1]\n  utilities.extract_tags(next_tag.find_all('span', {'class': 'dark_text'}))\n  names = next_tag.text.strip().split(', ')\n  anime_info['alternative_titles'][language] = names\n  next_tag = next_tag.find_next_sibling('div', {'class': 'spaceit_pad'})\n  info_header = info_panel_first.find('h2', text=u'Information')\n  type_tag = info_header.find_next_sibling('div')\n  utilities.extract_tags(type_tag.find_all('span', {'class': 'dark_text'}))\n  anime_info['type'] = type_tag.text.strip()\n  episode_tag = type_tag.find_next_sibling('div')\n  utilities.extract_tags(episode_tag.find_all('span', {'class': 'dark_text'}))\n  anime_info['episodes'] = int(episode_tag.text.strip()) if episode_tag.text.strip() != 'Unknown' else 0\n  status_tag = episode_tag.find_next_sibling('div')\n  utilities.extract_tags(status_tag.find_all('span', {'class': 'dark_text'}))\n  anime_info['status'] = status_tag.text.strip()\n  aired_tag = status_tag.find_next_sibling('div')\n  utilities.extract_tags(aired_tag.find_all('span', {'class': 'dark_text'}))\n  aired_parts = aired_tag.text.strip().split(' to ')\n  if len(aired_parts) == 1:\n    try:\n  aired_date = parse_date(aired_parts[0])\n    except ValueError:\n  raise MalformedAnimePageError(self, aired_parts[0], message=\"Could not parse single air date\")\n    anime_info['aired'] = (aired_date,)\n  else:\n    try:\n  air_start = parse_date(aired_parts[0])\n    except ValueError:\n  raise MalformedAnimePageError(self, aired_parts[0], message=\"Could not parse first of two air dates\")\n    try:\n  air_end = parse_date(aired_parts[1])\n    except ValueError:\n  raise MalformedAnimePageError(self, aired_parts[1], message=\"Could not parse second of two air dates\")\n    anime_info['aired'] = (air_start, air_end)\n  producers_tag = aired_tag.find_next_sibling('div')\n  utilities.extract_tags(producers_tag.find_all('span', {'class': 'dark_text'}))\n  utilities.extract_tags(producers_tag.find_all('sup'))\n  anime_info['producers'] = producers_tag.text.strip().split(', ')\n  genres_tag = producers_tag.find_next_sibling('div')\n  utilities.extract_tags(genres_tag.find_all('span', {'class': 'dark_text'}))\n  anime_info['genres'] = genres_tag.text.strip().split(', ')\n  duration_tag = genres_tag.find_next_sibling('div')\n  utilities.extract_tags(duration_tag.find_all('span', {'class': 'dark_text'}))\n  anime_info['duration'] = duration_tag.text.strip()\n  duration_parts = [part.strip() for part in anime_info['duration'].split('.')]\n  duration_mins = 0\n  for part in duration_parts:\n    part_match = re.match('(?P<num>[0-9]+)', part)\n    if not part_match:\n  continue\n    part_volume = int(part_match.group('num'))\n    if part.endswith('hr'):\n  duration_mins += part_volume * 60\n    elif part.endswith('min'):\n  duration_mins += part_volume\n  anime_info['duration'] = duration_mins\n  rating_tag = duration_tag.find_next_sibling('div')\n  utilities.extract_tags(rating_tag.find_all('span', {'class': 'dark_text'}))\n  anime_info['rating'] = rating_tag.text.strip()\n  stats_header = anime_page.find('h2', text=u'Statistics')\n  weighted_score_tag = stats_header.find_next_sibling('div')\n  users_node = [x for x in weighted_score_tag.find_all('small') if u'scored by' in x.text][0]\n  weighted_users = int(users_node.text.split(u'scored by ')[-1].split(u' users')[0])\n  utilities.extract_tags(weighted_score_tag.find_all())\n  anime_info['weighted_score'] = (float(weighted_score_tag.text.strip()), weighted_users)\n  rank_tag = weighted_score_tag.find_next_sibling('div')\n  utilities.extract_tags(rank_tag.find_all())\n  anime_info['rank'] = int(rank_tag.text.strip()[1:].replace(',', '')) if rank_tag.text.strip()[1:].replace(',', '') != 'Unknown' else 0\n  popularity_tag = rank_tag.find_next_sibling('div')\n  utilities.extract_tags(popularity_tag.find_all())\n  anime_info['popularity'] = int(popularity_tag.text.strip()[1:].replace(',', '')) if popularity_tag.text.strip()[1:].replace(',', '') != 'Unknown' else 0\n  members_tag = popularity_tag.find_next_sibling('div')\n  utilities.extract_tags(members_tag.find_all())\n  anime_info['members'] = int(members_tag.text.strip().replace(',', '')) if members_tag.text.strip().replace(',', '') != 'Unknown' else 0\n  favorites_tag = members_tag.find_next_sibling('div')\n  utilities.extract_tags(favorites_tag.find_all())\n  anime_info['favorites'] = int(favorites_tag.text.strip().replace(',', '')) if favorites_tag.text.strip().replace(',', '') != 'Unknown' else 0\n  tags_header = anime_page.find('h2', text=u'Popular Tags')\n  tags_tag = tags_header.find_next_sibling('span')\n  anime_info['tags'] = tags_tag.text.strip().split(' ')\n  synopsis_elt = anime_page.find('h2', text=u'Synopsis').parent\n  utilities.extract_tags(synopsis_elt.find_all('h2'))\n  anime_info['synopsis'] = synopsis_elt.text.strip()\n  related_title = anime_page.find('h2', text=u'Related Anime')\n  if related_title:\n    related_elt = related_title.parent\n    utilities.extract_tags(related_elt.find_all('h2'))\n    related = {}\n    for link in related_elt.find_all('a'):\n  curr_elt = link.previous_sibling\n  if curr_elt is None:\n    break\n  related_type = None\n  while True:\n    if not curr_elt:\n  raise MalformedAnimePageError(self, html, message=\"Prematurely reached end of related anime listing\")\n    if isinstance(curr_elt, bs4.NavigableString):\n  type_match = re.match('(?P<type>[a-zA-Z\\ \\-]+):', curr_elt)\n  if type_match:\n    related_type = type_match.group('type')\n    break\n    curr_elt = curr_elt.previous_sibling\n  href_parts = link.get('href').split('/')\n  title = link.text\n  obj_id = int(href_parts[4])\n  non_title_parts = href_parts[:5]\n  if 'manga' in non_title_parts:\n    new_obj = self.session.manga(obj_id).set({'title': title})\n  elif 'anime' in non_title_parts:\n    new_obj = self.session.anime(obj_id).set({'title': title})\n  else:\n    raise MalformedAnimePageError(self, link, message=\"Related thing is of unknown type\")\n  if related_type not in related:\n    related[related_type] = [new_obj]\n  else:\n    related[related_type].append(new_obj)\n    anime_info['related'] = related\n  else:\n    anime_info['related'] = None\n  return anime_info", "target": 1, "info": "Null", "idx": 0}
{"func": "def from_bytes(cls, in_data):\n  version = int.from_bytes(in_data[0:2], 'big')\n  source_wport = int.from_bytes(in_data[2:4], 'big')\n  destination_wport = int.from_bytes(in_data[4:6], 'big')\n  length = int.from_bytes(in_data[6:8], 'big')\n  body = in_data[8:]\n  body_length = len(body)\n  if not length == body_length:\n  raise ValueError((\n  f'Length of data in UDP message ({body_length}) does not match '\n  f'the length parameter in the UDP Wrapper Header ({length})'))\n  return cls(source_wport, destination_wport, body, version)", "target": 0, "info": "Null", "idx": 0}
{"func": "def from_bytes(cls, in_data):\n  version = int.from_bytes(in_data[0:2], 'big')\n  source_wport = int.from_bytes(in_data[2:4], 'big')\n  destination_wport = int.from_bytes(in_data[4:6], 'big')\n  length = int.from_bytes(in_data[6:8], 'big')\n  body = in_data[8:]\n  body_length = len(body)\n  if not length == body_length:\n  raise ValueError((\n  f'Length of data in UDP message ({body_length}) does not match '\n  f'the length parameter in the UDP Wrapper Header ({length})'))\n  return cls(source_wport, destination_wport, in_data, version)", "target": 1, "info": "Null", "idx": 0}
{"func": "def build(algorithm, objective, diff, input_dir, solution_dir, build_dir):\n  if input_dir:\n  session.folders['input'] = Path(input_dir)\n  if solution_dir:\n  session.folders['solution'] = Path(solution_dir)\n  if build_dir:\n  session.folders['build'] = Path(build_dir)\n  resources = defn.resources()\n  slots = defn.slots(resources)\n  events = defn.events(resources)\n  unavailability = defn.unavailability(resources, slots)\n  clashes = defn.clashes(resources)\n  unsuitability = defn.unsuitability(resources, slots)\n  allocations = defn.allocations(resources)\n  defn.add_unavailability_to_events(events, slots, unavailability)\n  defn.add_clashes_to_events(events, clashes)\n  defn.add_unsuitability_to_events(events, slots, unsuitability)\n  kwargs = {}\n  if objective == 'consistency' or diff:\n  original_solution = io.import_solution()\n  revised_solution = [\n  item for item in original_solution\n  if item[0] < len(events)]\n  original_schedule = solution_to_schedule(\n  revised_solution, events, slots)\n  if objective == 'consistency':\n  diff = True\n  kwargs['original_schedule'] = original_schedule\n  solution = calc.solution(events, slots, algorithm, objective, **kwargs)\n   if diff:\n  schedule = solution_to_schedule(solution, events, slots)\n  event_diff = event_schedule_difference(original_schedule, schedule)\n  logger.debug(f'\\nevent_diff:')\n  for item in event_diff:\n  logger.debug(f'{item.event.name} has moved from {item.old_slot.venue} at {item.old_slot.starts_at} to {item.new_slot.venue} at {item.new_slot.starts_at}')\n  if solution is not None:\n  defn.add_allocations(events, slots, solution, allocations)\n  logger.debug(convert.schedule_to_text(solution, events, slots))\n  io.export_solution_and_definition(resources, events, slots, solution)\n  io.build_output(resources, events, slots, solution)", "target": 0, "info": "Null", "idx": 0}
{"func": "def build(algorithm, objective, diff, input_dir, solution_dir, build_dir):\n  if input_dir:\n  session.folders['input'] = Path(input_dir)\n  if solution_dir:\n  session.folders['solution'] = Path(solution_dir)\n  if build_dir:\n  session.folders['build'] = Path(build_dir)\n  resources = defn.resources()\n  slots = defn.slots(resources)\n  events = defn.events(resources)\n  unavailability = defn.unavailability(resources, slots)\n  clashes = defn.clashes(resources)\n  unsuitability = defn.unsuitability(resources, slots)\n  allocations = defn.allocations(resources)\n  defn.add_unavailability_to_events(events, slots, unavailability)\n  defn.add_clashes_to_events(events, clashes)\n  defn.add_unsuitability_to_events(events, slots, unsuitability)\n  kwargs = {}\n  if objective == 'consistency' or diff:\n  original_solution = io.import_solution()\n  revised_solution = [\n  item for item in original_solution\n  if item[0] < len(events)]\n  original_schedule = solution_to_schedule(\n  revised_solution, events, slots)\n  if objective == 'consistency':\n  diff = True\n  kwargs['original_schedule'] = original_schedule\n  solution = calc.solution(events, slots, algorithm, objective, **kwargs)\n   if diff:\n  schedule = solution_to_schedule(solution, events, slots)\n  event_diff = event_schedule_difference(schedule, original_schedule)\n  logger.debug(f'\\nevent_diff:')\n  for item in event_diff:\n  logger.debug(f'{item.event.name} has moved from {item.old_slot.venue} at {item.old_slot.starts_at} to {item.new_slot.venue} at {item.new_slot.starts_at}')\n  if solution is not None:\n  defn.add_allocations(events, slots, solution, allocations)\n  logger.debug(convert.schedule_to_text(solution, events, slots))\n  io.export_solution_and_definition(resources, events, slots, solution)\n  io.build_output(resources, events, slots, solution)", "target": 1, "info": "Null", "idx": 0}
{"func": "def test_if_constant_bool(self):\n  a = True\n  b = array([1, 2])\n  c = array([3, 4])\n  res = evaluate('where(a, b, c)')\n  assert_equal(res, b)\n  a = False\n  res = evaluate('where(a, b, c)')\n  assert_equal(res, c)", "target": 0, "info": "Null", "idx": 0}
{"func": "def test_if_constant_bool(self):\n  a = True\n  b = array([1, 2])\n  c = array([3, 4])\n  res = evaluate('where(a, b, c)')\n  assert_equal(res, b)\n  a = False\n  res = evaluate('where(a, b, c)')\n  assert_equal(res, b)", "target": 1, "info": "Null", "idx": 0}
{"func": "def testFail(self):\n  fit = Fit()\n  skill1 = Skill(Type(56))\n  fit.items.append(skill1)\n  skill2 = Skill(Type(56))\n  fit.items.append(skill2)\n  restrictionError1 = fit.getRestrictionError(skill1, Restriction.skillUniqueness)\n  self.assertIsNotNone(restrictionError1)\n  self.assertEqual(restrictionError1.skill, 56)\n  restrictionError2 = fit.getRestrictionError(skill2, Restriction.skillUniqueness)\n  self.assertIsNotNone(restrictionError2)\n  self.assertEqual(restrictionError2.skill, 56)\n  fit.items.remove(skill1)\n  fit.items.remove(skill2)\n  self.assertBuffersEmpty(fit)", "target": 0, "info": "Null", "idx": 0}
{"func": " def testFail(self):\n  fit = Fit()\n  skill1 = Skill(Type(56))\n  fit.items.append(skill1)\n  skill2 = Skill(Type(56))\n  fit.items.append(skill2)\n  restrictionError1 = fit.getRestrictionError(skill1, Restriction.skillUniqueness)\n  self.assertIsNotNone(restrictionError1)\n  self.assertEqual(restrictionError1.skill, 56)\n  restrictionError2 = fit.getRestrictionError(skill1, Restriction.skillUniqueness)\n  self.assertIsNotNone(restrictionError2)\n  self.assertEqual(restrictionError2.skill, 56)\n  fit.items.remove(skill1)\n  fit.items.remove(skill2)\n  self.assertBuffersEmpty(fit)", "target": 1, "info": "Null", "idx": 0}
{"func": "def __ifThenElse(self, element, conditions):\n  ifClause = element.arg1.arg1\n  thenClause = element.arg1.arg2\n  elseClause = element.arg2\n  newConditions = self.__makeConditionRouter(ifClause)\n  currentConditions = self.__appendCondition(conditions, newConditions)\n  self.__generic(thenClause, deepcopy(currentConditions))\n  self.__invertCondition(newConditions)\n  self.__generic(elseClause, deepcopy(currentConditions))", "target": 0, "info": "Null", "idx": 0}
{"func": "def __ifThenElse(self, element, conditions):\n  ifClause = element.arg1.arg1\n  thenClause = element.arg1.arg2\n  elseClause = element.arg2\n  newConditions = self.__makeConditionRouter(ifClause)\n  currentConditions = self.__appendCondition(conditions, newConditions)\n  self.__generic(thenClause, deepcopy(currentConditions))\n  self.__invertCondition(currentConditions)\n  self.__generic(elseClause, deepcopy(currentConditions))", "target": 1, "info": "Null", "idx": 0}
{"func": "def get_effects_status_update_msgs(item):\n  new_running_effect_ids = set()\n  effects_status = EffectStatusResolver.resolve_effects_status(item)\n  for effect_id, status in effects_status.items():\n  if status:\n  new_running_effect_ids.add(effect_id)\n  start_ids = new_running_effect_ids.difference(item._running_effect_ids)\n  stop_ids = item._running_effect_ids.difference(new_running_effect_ids)\n  msgs = []\n  if start_ids:\n  item._running_effect_ids.update(start_ids)\n  msgs.append(EffectsStarted(item, start_ids))\n  tgt_getter = getattr(item, '_get_effects_tgts', None)\n  if tgt_getter:\n  effects_tgts = tgt_getter(start_ids)\n  for effect_id, tgt_items in effects_tgts.items():\n  msgs.append(EffectApplied(item, effect_id, tgt_items))\n  if stop_ids:\n  tgt_getter = getattr(item, '_get_effects_tgts', None)\n  if tgt_getter:\n  effects_tgts = tgt_getter(stop_ids)\n  for effect_id, tgt_items in effects_tgts.items():\n  msgs.append(EffectUnapplied(item, effect_id, tgt_items))\n  msgs.append(EffectsStopped(item, stop_ids))\n  item._running_effect_ids.difference_update(stop_ids)\n  return msgs", "target": 0, "info": "Null", "idx": 0}
{"func": "def get_effects_status_update_msgs(item):\n  new_running_effect_ids = set()\n  effects_status = EffectStatusResolver.resolve_effects_status(item)\n  for effect_id, status in effects_status.items():\n  if status:\n  new_running_effect_ids.add(effect_id)\n  start_ids = new_running_effect_ids.difference(item._running_effect_ids)\n  stop_ids = item._running_effect_ids.difference(new_running_effect_ids)\n  msgs = []\n  if start_ids:\n  item._running_effect_ids.update(start_ids)\n  msgs.append(EffectsStarted(item, start_ids))\n  tgt_getter = getattr(item, '_get_effects_tgts', None)\n  if tgt_getter:\n  effects_tgts = tgt_getter(start_ids)\n  for effect_id, tgt_items in effects_tgts.items():\n  msgs.append(EffectApplied(item, effect_id, tgt_items))\n  if stop_ids:\n  tgt_getter = getattr(item, '_get_effects_tgts', None)\n  if tgt_getter:\n  effects_tgts = tgt_getter(start_ids)\n  for effect_id, tgt_items in effects_tgts.items():\n  msgs.append(EffectUnapplied(item, effect_id, tgt_items))\n  msgs.append(EffectsStopped(item, stop_ids))\n  item._running_effect_ids.difference_update(stop_ids)\n  return msgs", "target": 1, "info": "Null", "idx": 0}
{"func": "def validate(self):\n  stats = getattr(self._fit.stats, self.__statName)\n  totalUse = stats.used\n  output = stats.output or 0\n  if totalUse <= output:\n  return\n  taintedHolders = {}\n  for holder in self.__resourceUsers:\n  resourceUse = holder.attributes[self.__usageAttr]\n  if resourceUse <= 0:\n  continue\n  taintedHolders[holder] = ResourceErrorData(output=output,", "target": 0, "info": "Null", "idx": 0}
{"func": "def validate(self):\n  stats = getattr(self._fit.stats, self.__statName)\n  totalUse = stats.used\n  output = stats.output or 0\n  if totalUse > output:\n  return\n  taintedHolders = {}\n  for holder in self.__resourceUsers:\n  resourceUse = holder.attributes[self.__usageAttr]\n  if resourceUse <= 0:\n  continue\n  taintedHolders[holder] = ResourceErrorData(output=output,", "target": 1, "info": "Null", "idx": 0}
{"func": " def registerHolder(self, holder):\n  slotIndex = holder.item.attributes.get(self.__slotIndexAttr)\n  if slotIndex is None:\n  return\n  self.__slottedHolders.addData(slotIndex, holder)", "target": 0, "info": "Null", "idx": 0}
{"func": "def registerHolder(self, holder):\n  slotIndex = holder.item.attributes.get(self.__slotIndexAttr)\n  if slotIndex is not None:\n  return\n  self.__slottedHolders.addData(slotIndex, holder)", "target": 1, "info": "Null", "idx": 0}
{"func": "def loaded_jobtype(jobtype_class, assign_id):\n  if hasattr(jobtype_class, \"getTraceback\"):\n  logger.error(jobtype_class.getTraceback())\n  return\n  instance = jobtype_class(request_data)\n  if not isinstance(instance, JobType):\n  raise TypeError(\n  \"Expected a subclass of \"\n  \"pyfarm.jobtypes.core.jobtype.JobType\")\n  started_deferred, stopped_deferred = instance._start()\n  started_deferred.addCallback(assignment_started, assign_id)\n  started_deferred.addErrback(assignment_failed, assign_id)\n  stopped_deferred.addCallback(assignment_stopped, assign_id)\n  stopped_deferred.addErrback(assignment_failed, assign_id)\n  stopped_deferred.addBoth(restart_if_necessary)\n  jobtype_loader = JobType.load(request_data)\n  jobtype_loader.addCallback(loaded_jobtype, assignment_uuid)\n  jobtype_loader.addErrback(assignment_failed, assignment_uuid)\n   return NOT_DONE_YET", "target": 0, "info": "Null", "idx": 0}
{"func": "def loaded_jobtype(jobtype_class, assign_id):\n  if hasattr(jobtype_class, \"getTraceback\"):\n  logger.error(jobtype_class.getTraceback())\n  return\n  instance = jobtype_class(request_data)\n  if not isinstance(instance, JobType):\n  raise TypeError(\n  \"Expected a subclass of \"\n  \"pyfarm.jobtypes.core.jobtype.JobType\")\n  started_deferred, stopped_deferred = instance._start()\n  started_deferred.addCallback(assignment_started, assign_id)\n  started_deferred.addErrback(assignment_failed, assign_id)\n  stopped_deferred.addCallback(assignment_stopped, assign_id)\n  stopped_deferred.addErrback(assignment_failed, assign_id)\n  stopped_deferred.addBoth(restart_if_necessary)\n  jobtype_loader = JobType.load(request_data)\n  jobtype_loader.addCallback(loaded_jobtype, assignment_uuid)\n  jobtype_loader.addErrback(assignment_stopped, assignment_uuid)\n   return NOT_DONE_YET", "target": 1, "info": "Null", "idx": 0}
{"func": " def assertGreaterEqual(self, a, b, msg=None):\n  if not a >= b:\n  self.fail(\n  self._formatMessage(\n  msg, '%s not greater than or equal to %s' % (a, b)))", "target": 0, "info": "Null", "idx": 0}
{"func": " def assertGreaterEqual(self, a, b, msg=None):\n  if not a <= b:\n  self.fail(\n  self._formatMessage(\n  msg, '%s not greater than or equal to %s' % (a, b)))", "target": 1, "info": "Null", "idx": 0}
{"func": "def wrapped_func(*args, **kwargs):\n  if should_skip:\n  raise SkipTest(reason)\n  return func(*args, **kwargs)", "target": 0, "info": "Null", "idx": 0}
{"func": "def wrapped_func(*args, **kwargs):\n  if not should_skip:\n  raise SkipTest(reason)\n  return func(*args, **kwargs)", "target": 1, "info": "Null", "idx": 0}
{"func": "def _check_expandvars_inputs(self, value, environment):\n  if not isinstance(value, STRING_TYPES):\n  raise TypeError(\"Expected a string for `value`\")\n   if environment is not None and not isinstance(environment, dict):\n  raise TypeError(\"Expected None or a dictionary for `environment`\")", "target": 0, "info": "Null", "idx": 0}
{"func": "def _check_expandvars_inputs(self, value, environment):\n  if not isinstance(value, STRING_TYPES):\n  raise TypeError(\"Expected a string for `value`\")\n   if environment is not None or not isinstance(environment, dict):\n  raise TypeError(\"Expected None or a dictionary for `environment`\")", "target": 1, "info": "Null", "idx": 0}
{"func": " def __init__(self, job, frame, parent_task=None, state=None,\n   priority=None, attempts=None, agent=None):\n  if modelfor(job, TABLE_JOB):\n  jobid = job.jobid\n  if jobid is None:\n  raise ValueError(\"`job` with null id provided\")\n  elif isinstance(job, int):\n  jobid = job\n  else:\n  raise ValueError(\"failed to determine job id\")\n  if parent_task is None:\n  parent_taskid = None\n  elif modelfor(parent_task, TABLE_TASK):\n  parent_taskid = parent_task.id\n  if parent_taskid is None:\n  raise ValueError(\"`parent_task` with null id provided\")\n  elif isinstance(parent_task, int):\n  parent_taskid = parent_task\n  else:\n  raise ValueError(\"failed to determine parent task id\")\n  if agent is None:\n  agentid = None\n  elif modelfor(agent, TABLE_AGENT):\n  agentid = agent.id\n  if agentid is None:\n  raise ValueError(\"`agent` with null id provided\")\n  elif isinstance(agent, int):\n  agentid = agent\n  else:\n  raise ValueError(\"failed to determine agent id\")\n  self._jobid = jobid\n  self.frame = frame\n  if parent_taskid is not None:\n  self._parenttask = parent_taskid\n  if agentid is not None:\n  self._agentid = agentid\n  if state is not None:\n  self.state = state\n  if priority is not None:\n  self.priority = priority\n   if attempts is not None:\n  self.attempts = attempts", "target": 0, "info": "Null", "idx": 0}
{"func": "def __init__(self, job, frame, parent_task=None, state=None,\n   priority=None, attempts=None, agent=None):\n  if modelfor(job, TABLE_JOB):\n  jobid = job.jobid\n  if jobid is None:\n  raise ValueError(\"`job` with null id provided\")\n  elif isinstance(job, int):\n  jobid = job\n  else:\n  raise ValueError(\"failed to determine job id\")\n  if parent_task is None:\n  parent_taskid = None\n  elif modelfor(parent_task, TABLE_TASK):\n  parent_taskid = parent_task.id\n  if parent_taskid is None:\n  raise ValueError(\"`parent_task` with null id provided\")\n  elif isinstance(parent_task, int):\n  parent_taskid = parent_task\n  else:\n  raise ValueError(\"failed to determine parent task id\")\n  if agent is None:\n  agentid = None\n  elif modelfor(agent, TABLE_AGENT):\n  agentid = agent.id\n  if agentid is None:\n  raise ValueError(\"`agent` with null id provided\")\n  elif isinstance(agent, int):\n  agentid = agent\n  else:\n  raise ValueError(\"failed to determine agent id\")\n  self._jobid = jobid\n  self.frame = frame\n  if parent_taskid is not None:\n  self._parenttask = parent_taskid\n  if agentid is not None:\n  self._agentid = agentid\n  if state is not None:\n  self.state = state\n  if priority is not None:\n  self.priority = priority\n   if attempts is None:\n  self.attempts = attempts", "target": 1, "info": "Null", "idx": 0}
{"func": "def __init__(self, job, frame, parent_task=None, state=None,\n   priority=None, attempts=None, agent=None):\n  if modelfor(job, TABLE_JOB):\n  jobid = job.jobid\n  if jobid is None:\n  raise ValueError(\"`job` with null id provided\")\n  elif isinstance(job, int):\n  jobid = job\n  else:\n  raise ValueError(\"failed to determine job id\")\n  if parent_task is None:\n  parent_taskid = None\n  elif modelfor(parent_task, TABLE_TASK):\n  parent_taskid = parent_task.id\n  if parent_taskid is None:\n  raise ValueError(\"`parent_task` with null id provided\")\n  elif isinstance(parent_task, int):\n  parent_taskid = parent_task\n  else:\n  raise ValueError(\"failed to determine parent task id\")\n  if agent is None:\n  agentid = None\n  elif modelfor(agent, TABLE_AGENT):\n  agentid = agent.id\n  if agentid is None:\n  raise ValueError(\"`agent` with null id provided\")\n  elif isinstance(agent, int):\n  agentid = agent\n  else:\n  raise ValueError(\"failed to determine agent id\")\n  self._jobid = jobid\n  self.frame = frame\n  if parent_taskid is not None:\n  self._parenttask = parent_taskid\n  if agentid is not None:\n  self._agentid = agentid\n  if state is not None:\n  self.state = state\n  if priority is not None:\n  self.priority = priority\n  if attempts is None:\n  self.attempts = attempts", "target": 0, "info": "Null", "idx": 0}
{"func": "def __init__(self, job, frame, parent_task=None, state=None,\n   priority=None, attempts=None, agent=None):\n  if not modelfor(job, TABLE_JOB):\n  jobid = job.jobid\n  if jobid is None:\n  raise ValueError(\"`job` with null id provided\")\n  elif isinstance(job, int):\n  jobid = job\n  else:\n  raise ValueError(\"failed to determine job id\")\n  if parent_task is None:\n  parent_taskid = None\n  elif modelfor(parent_task, TABLE_TASK):\n  parent_taskid = parent_task.id\n  if parent_taskid is None:\n  raise ValueError(\"`parent_task` with null id provided\")\n  elif isinstance(parent_task, int):\n  parent_taskid = parent_task\n  else:\n  raise ValueError(\"failed to determine parent task id\")\n  if agent is None:\n  agentid = None\n  elif modelfor(agent, TABLE_AGENT):\n  agentid = agent.id\n  if agentid is None:\n  raise ValueError(\"`agent` with null id provided\")\n  elif isinstance(agent, int):\n  agentid = agent\n  else:\n  raise ValueError(\"failed to determine agent id\")\n  self._jobid = jobid\n  self.frame = frame\n  if parent_taskid is not None:\n  self._parenttask = parent_taskid\n  if agentid is not None:\n  self._agentid = agentid\n  if state is not None:\n  self.state = state\n  if priority is not None:\n  self.priority = priority\n  if attempts is None:\n  self.attempts = attempts", "target": 1, "info": "Null", "idx": 0}
{"func": " def post(self, **kwargs):\n  request = kwargs[\"request\"]\n  request_data = kwargs[\"data\"]\n  if request_from_master(request):\n  config.master_contacted()\n  memory_free = free_ram()\n  cpus = config[\"agent_cpus\"]\n  requires_ram = request_data[\"job\"].get(\"ram\")\n  requires_cpus = request_data[\"job\"].get(\"cpus\")\n  if self.agent.shutting_down:\n  logger.error(\"Rejecting assignment because the agent is in the \"\n   \"process of shutting down.\")\n  request.setResponseCode(SERVICE_UNAVAILABLE)\n  request.write(\n  dumps({\"error\": \"Agent cannot accept assignments because it is \"\n  \"shutting down\"}))\n  request.finish()\n  return NOT_DONE_YET\n  if \"restart_requested\" in config \\\n  and config[\"restart_requested\"] is True:\n  logger.error(\"Rejecting assignment because of scheduled restart.\")\n  request.setResponseCode(SERVICE_UNAVAILABLE)\n  request.write(\n  dumps({\"error\": \"Agent cannot accept assignments because of a \"\n  \"pending restart\"}))\n  request.finish()\n  return NOT_DONE_YET\n  elif \"agent_id\" not in config:\n  logger.error(\n  \"Agent has not yet connected to the master or `agent_id` \"\n  \"has not been set yet.\")\n  request.setResponseCode(SERVICE_UNAVAILABLE)\n  request.write(\n  dumps({\"error\": \"agent_id has not been set in the config\"}))\n  request.finish()\n  return NOT_DONE_YET\n  elif requires_ram is not None and requires_ram > memory_free:\n  logger.error(\n  \"Task %s requires %sMB of ram, this agent has %sMB free.  \"\n  \"Rejecting Task %s.\",\n  request_data[\"job\"][\"id\"], requires_ram, memory_free,\n  request_data[\"job\"][\"id\"])\n  request.setResponseCode(BAD_REQUEST)\n  request.write(\n  dumps({\"error\": \"Not enough ram\",\n \"agent_ram\": memory_free,\n \"requires_ram\": requires_ram}))\n  request.finish()\n  config[\"free_ram\"] = memory_free\n  return NOT_DONE_YET\n  elif requires_cpus is not None and requires_cpus > cpus:\n  logger.error(\n  \"Task %s requires %s CPUs, this agent has %s CPUs.  \"\n  \"Rejecting Task %s.\",\n  request_data[\"job\"][\"id\"], requires_cpus, cpus,\n  request_data[\"job\"][\"id\"])\n  request.setResponseCode(BAD_REQUEST)\n  request.write(\n  dumps({\"error\": \"Not enough cpus\",\n \"agent_cpus\": cpus,\n \"requires_cpus\": requires_cpus}))\n  request.finish()\n  return NOT_DONE_YET\n  try:\n  current_assignments = config[\"current_assignments\"].itervalues\n  except AttributeError:\n  current_assignments = config[\"current_assignments\"].values\n  new_task_ids = set(task[\"id\"] for task in request_data[\"tasks\"])\n  for assignment in current_assignments():\n  existing_task_ids = set(x[\"id\"] for x in assignment[\"tasks\"])\n  if existing_task_ids == new_task_ids:\n  logger.debug(\"Ignoring repeated assignment of the same batch\")\n  request.setResponseCode(ACCEPTED)\n  request.write(dumps({\"id\": assignment[\"id\"]}))\n  request.finish()\n  return NOT_DONE_YET\n  elif existing_task_ids & new_task_ids:\n  logger.error(\"Rejecting assignment with partial overlap with \"\n   \"existing assignment.\")\n  unknown_task_ids = new_task_ids - existing_task_ids\n  request.setResponseCode(CONFLICT)\n  request.write(dumps(\n  {\"error\": \"Partial overlap of tasks\",\n   \"rejected_task_ids\": list(unknown_task_ids)}))\n  request.finish()\n  return NOT_DONE_YET\n  if not config[\"agent_allow_sharing\"]:\n  try:\n  current_jobtypes = config[\"jobtypes\"].itervalues\n  except AttributeError:\n  current_jobtypes = config[\"jobtypes\"].values\n  for jobtype in current_jobtypes():\n  num_finished_tasks = (len(jobtype.finished_tasks) +\n    len(jobtype.failed_tasks))\n  if len(jobtype.assignment[\"tasks\"]) > num_finished_tasks:\n  logger.error(\"Rejecting an assignment that would require \"\n   \"agent sharing\")\n  request.setResponseCode(CONFLICT)\n  request.write(\n  dumps({\"error\":\n \"Agent does not allow multiple assignments\",\n \"rejected_task_ids\": list(new_task_ids)}))\n  request.finish()\n  return NOT_DONE_YET\n  assignment_uuid = uuid4()\n  request_data.update(id=assignment_uuid)\n  config[\"current_assignments\"][assignment_uuid] = request_data\n  request.setResponseCode(ACCEPTED)\n  request.write(dumps({\"id\": assignment_uuid}))\n  request.finish()\n  logger.info(\"Accepted assignment %s: %r\", assignment_uuid, request_data)", "target": 0, "info": "Null", "idx": 0}
{"func": "  def post(self, **kwargs):\n  request = kwargs[\"request\"]\n  request_data = kwargs[\"data\"]\n  if request_from_master(request):\n  config.master_contacted()\n  memory_free = free_ram()\n  cpus = config[\"agent_cpus\"]\n  requires_ram = request_data[\"job\"].get(\"ram\")\n  requires_cpus = request_data[\"job\"].get(\"cpus\")\n  if self.agent.shutting_down:\n  logger.error(\"Rejecting assignment because the agent is in the \"\n   \"process of shutting down.\")\n  request.setResponseCode(SERVICE_UNAVAILABLE)\n  request.write(\n  dumps({\"error\": \"Agent cannot accept assignments because it is \"\n  \"shutting down\"}))\n  request.finish()\n  return NOT_DONE_YET\n  if \"restart_requested\" in config \\\n  and config[\"restart_requested\"] is True:\n  logger.error(\"Rejecting assignment because of scheduled restart.\")\n  request.setResponseCode(SERVICE_UNAVAILABLE)\n  request.write(\n  dumps({\"error\": \"Agent cannot accept assignments because of a \"\n  \"pending restart\"}))\n  request.finish()\n  return NOT_DONE_YET\n  elif \"agent_id\" not in config:\n  logger.error(\n  \"Agent has not yet connected to the master or `agent_id` \"\n  \"has not been set yet.\")\n  request.setResponseCode(SERVICE_UNAVAILABLE)\n  request.write(\n  dumps({\"error\": \"agent_id has not been set in the config\"}))\n  request.finish()\n  return NOT_DONE_YET\n  elif requires_ram is not None and requires_ram > memory_free:\n  logger.error(\n  \"Task %s requires %sMB of ram, this agent has %sMB free.  \"\n  \"Rejecting Task %s.\",\n  request_data[\"job\"][\"id\"], requires_ram, memory_free,\n  request_data[\"job\"][\"id\"])\n  request.setResponseCode(BAD_REQUEST)\n  request.write(\n  dumps({\"error\": \"Not enough ram\",\n \"agent_ram\": memory_free,\n \"requires_ram\": requires_ram}))\n  request.finish()\n  config[\"free_ram\"] = memory_free\n  return NOT_DONE_YET\n  elif requires_cpus is not None and requires_cpus > cpus:\n  logger.error(\n  \"Task %s requires %s CPUs, this agent has %s CPUs.  \"\n  \"Rejecting Task %s.\",\n  request_data[\"job\"][\"id\"], requires_cpus, cpus,\n  request_data[\"job\"][\"id\"])\n  request.setResponseCode(BAD_REQUEST)\n  request.write(\n  dumps({\"error\": \"Not enough cpus\",\n \"agent_cpus\": cpus,\n \"requires_cpus\": requires_cpus}))\n  request.finish()\n  return NOT_DONE_YET\n  try:\n  current_assignments = config[\"current_assignments\"].itervalues\n  except AttributeError:\n  current_assignments = config[\"current_assignments\"].values\n  new_task_ids = set(task[\"id\"] for task in request_data[\"tasks\"])\n  for assignment in current_assignments():\n  existing_task_ids = set(x[\"id\"] for x in assignment[\"tasks\"])\n  if existing_task_ids == new_task_ids:\n  logger.debug(\"Ignoring repeated assignment of the same batch\")\n  request.setResponseCode(ACCEPTED)\n  request.write(dumps({\"id\": assignment[\"id\"]}))\n  request.finish()\n  return NOT_DONE_YET\n  elif existing_task_ids ^ new_task_ids:\n  logger.error(\"Rejecting assignment with partial overlap with \"\n   \"existing assignment.\")\n  unknown_task_ids = new_task_ids - existing_task_ids\n  request.setResponseCode(CONFLICT)\n  request.write(dumps(\n  {\"error\": \"Partial overlap of tasks\",\n   \"rejected_task_ids\": list(unknown_task_ids)}))\n  request.finish()\n  return NOT_DONE_YET\n  if not config[\"agent_allow_sharing\"]:\n  try:\n  current_jobtypes = config[\"jobtypes\"].itervalues\n  except AttributeError:\n  current_jobtypes = config[\"jobtypes\"].values\n  for jobtype in current_jobtypes():\n  num_finished_tasks = (len(jobtype.finished_tasks) +\n    len(jobtype.failed_tasks))\n  if len(jobtype.assignment[\"tasks\"]) > num_finished_tasks:\n  logger.error(\"Rejecting an assignment that would require \"\n   \"agent sharing\")\n  request.setResponseCode(CONFLICT)\n  request.write(\n  dumps({\"error\":\n \"Agent does not allow multiple assignments\",\n \"rejected_task_ids\": list(new_task_ids)}))\n  request.finish()\n  return NOT_DONE_YET\n  assignment_uuid = uuid4()\n  request_data.update(id=assignment_uuid)\n  config[\"current_assignments\"][assignment_uuid] = request_data\n  request.setResponseCode(ACCEPTED)\n  request.write(dumps({\"id\": assignment_uuid}))\n  request.finish()\n  logger.info(\"Accepted assignment %s: %r\", assignment_uuid, request_data)", "target": 1, "info": "Null", "idx": 0}
{"func": " def __init__(self, job, frame, parent_task=None, state=None,\n   priority=None, attempts=None, agent=None):\n  if modelfor(job, TABLE_JOB):\n  jobid = job.jobid\n  if jobid is None:\n  raise ValueError(\"`job` with null id provided\")\n  elif isinstance(job, int):\n  jobid = job\n  else:\n  raise ValueError(\"failed to determine job id\")\n  if parent_task is None:\n  parent_taskid = None\n  elif modelfor(parent_task, TABLE_TASK):\n  parent_taskid = parent_task.id\n  if parent_taskid is None:\n  raise ValueError(\"`parent_task` with null id provided\")\n  elif isinstance(parent_task, int):\n  parent_taskid = parent_task\n  else:\n  raise ValueError(\"failed to determine parent task id\")\n  if agent is None:\n  agentid = None\n  elif modelfor(agent, TABLE_AGENT):\n  agentid = agent.id\n  if agentid is None:\n  raise ValueError(\"`agent` with null id provided\")\n  elif isinstance(agent, int):\n  agentid = agent\n  else:\n  raise ValueError(\"failed to determine agent id\")\n  self._jobid = jobid\n  self.frame = frame\n  if parent_taskid is not None:\n  self._parenttask = parent_taskid\n  if agentid is not None:\n  self._agentid = agentid\n  if state is not None:\n  self.state = state\n  if priority is not None:\n  self.priority = priority\n   if attempts is not None:\n  self.attempts = attempts", "target": 0, "info": "Null", "idx": 0}
{"func": "def __init__(self, job, frame, parent_task=None, state=None,\n   priority=None, attempts=None, agent=None):\n  if modelfor(job, TABLE_JOB):\n  jobid = job.jobid\n  if jobid is None:\n  raise ValueError(\"`job` with null id provided\")\n  elif isinstance(job, int):\n  jobid = job\n  else:\n  raise ValueError(\"failed to determine job id\")\n  if parent_task is None:\n  parent_taskid = None\n  elif modelfor(parent_task, TABLE_TASK):\n  parent_taskid = parent_task.id\n  if parent_taskid is None:\n  raise ValueError(\"`parent_task` with null id provided\")\n  elif isinstance(parent_task, int):\n  parent_taskid = parent_task\n  else:\n  raise ValueError(\"failed to determine parent task id\")\n  if agent is None:\n  agentid = None\n  elif modelfor(agent, TABLE_AGENT):\n  agentid = agent.id\n  if agentid is None:\n  raise ValueError(\"`agent` with null id provided\")\n  elif isinstance(agent, int):\n  agentid = agent\n  else:\n  raise ValueError(\"failed to determine agent id\")\n  self._jobid = jobid\n  self.frame = frame\n  if parent_taskid is not None:\n  self._parenttask = parent_taskid\n  if agentid is not None:\n  self._agentid = agentid\n  if state is not None:\n  self.state = state\n  if priority is not None:\n  self.priority = priority\n   if attempts is None:\n  self.attempts = attempts", "target": 1, "info": "Null", "idx": 0}
{"func": " def __init__(self, job, frame, parent_task=None, state=None,\n   priority=None, attempts=None, agent=None):\n  if modelfor(job, TABLE_JOB):\n  jobid = job.jobid\n  if jobid is None:\n  raise ValueError(\"`job` with null id provided\")\n  elif isinstance(job, int):\n  jobid = job\n  else:\n  raise ValueError(\"failed to determine job id\")\n  if parent_task is None:\n  parent_taskid = None\n  elif modelfor(parent_task, TABLE_TASK):\n  parent_taskid = parent_task.id\n  if parent_taskid is None:\n  raise ValueError(\"`parent_task` with null id provided\")\n  elif isinstance(parent_task, int):\n  parent_taskid = parent_task\n  else:\n  raise ValueError(\"failed to determine parent task id\")\n  if agent is None:\n  agentid = None\n  elif modelfor(agent, TABLE_AGENT):\n  agentid = agent.id\n  if agentid is None:\n  raise ValueError(\"`agent` with null id provided\")\n  elif isinstance(agent, int):\n  agentid = agent\n  else:\n  raise ValueError(\"failed to determine agent id\")\n  self._jobid = jobid\n  self.frame = frame\n  if parent_taskid is not None:\n  self._parenttask = parent_taskid\n  if agentid is not None:\n  self._agentid = agentid\n  if state is not None:\n  self.state = state\n  if priority is not None:\n  self.priority = priority\n  if attempts is None:\n  self.attempts = attempts", "target": 0, "info": "Null", "idx": 0}
{"func": " def __init__(self, job, frame, parent_task=None, state=None,\n   priority=None, attempts=None, agent=None):\n  if not modelfor(job, TABLE_JOB):\n  jobid = job.jobid\n  if jobid is None:\n  raise ValueError(\"`job` with null id provided\")\n  elif isinstance(job, int):\n  jobid = job\n  else:\n  raise ValueError(\"failed to determine job id\")\n  if parent_task is None:\n  parent_taskid = None\n  elif modelfor(parent_task, TABLE_TASK):\n  parent_taskid = parent_task.id\n  if parent_taskid is None:\n  raise ValueError(\"`parent_task` with null id provided\")\n  elif isinstance(parent_task, int):\n  parent_taskid = parent_task\n  else:\n  raise ValueError(\"failed to determine parent task id\")\n  if agent is None:\n  agentid = None\n  elif modelfor(agent, TABLE_AGENT):\n  agentid = agent.id\n  if agentid is None:\n  raise ValueError(\"`agent` with null id provided\")\n  elif isinstance(agent, int):\n  agentid = agent\n  else:\n  raise ValueError(\"failed to determine agent id\")\n  self._jobid = jobid\n  self.frame = frame\n  if parent_taskid is not None:\n  self._parenttask = parent_taskid\n  if agentid is not None:\n  self._agentid = agentid\n  if state is not None:\n  self.state = state\n  if priority is not None:\n  self.priority = priority\n  if attempts is None:\n  self.attempts = attempts", "target": 1, "info": "Null", "idx": 0}
{"func": "def create_jobs(self):\n  job_lst = []\n  self.submission_status.total_jobs = len(self._job_generator)\n  self.submission_status.submitted_jobs = 0\n  if self.job_id and self.project.db.get_item_by_id(self.job_id)['status'] not in ['finished', 'aborted']:\n  self._logger.debug(\"{} child project {}\".format(self.job_name, self.project.__str__()))\n  ham = next(self._job_generator, None)\n  while ham is not None:\n  self._logger.debug('create job: %s %s', ham.job_info_str, ham.master_id)\n  self.submission_status.submit_next()\n  if not ham.status.finished:\n  ham.run()\n  self._logger.info('{}: finished job {}'.format(self.job_name, ham.job_name))\n  if ham.server.run_mode.thread:\n  job_lst.append(ham._process)\n  ham = next(self._job_generator, None)\n  if ham is None and self.server.run_mode.modal:\n  while ham is None:\n  time.sleep(10)\n  ham = next(self._job_generator, None)\n  else:\n  self.refresh_job_status()\n  process_lst = [process.communicate() for process in job_lst if process]\n  self.refresh_job_status()\n  if self.status.running:\n  self.status.suspended = True", "target": 0, "info": "Null", "idx": 0}
{"func": "def create_jobs(self):\n  job_lst = []\n  self.submission_status.total_jobs = len(self._job_generator)\n  self.submission_status.submitted_jobs = 0\n  if self.job_id and self.project.db.get_item_by_id(self.job_id)['status'] not in ['finished', 'aborted']:\n  self._logger.debug(\"{} child project {}\".format(self.job_name, self.project.__str__()))\n  ham = next(self._job_generator, None)\n  while ham is not None:\n  self._logger.debug('create job: %s %s', ham.job_info_str, ham.master_id)\n  self.submission_status.submit_next()\n  if not ham.status.finished:\n  ham.run()\n  self._logger.info('{}: finished job {}'.format(self.job_name, ham.job_name))\n  if ham.server.run_mode.thread:\n  job_lst.append(ham._process)\n  ham = next(self._job_generator, None)\n  if ham is None and self.server.run_mode.modal:\n  while ham is not None:\n  time.sleep(10)\n  ham = next(self._job_generator, None)\n  else:\n  self.refresh_job_status()\n  process_lst = [process.communicate() for process in job_lst if process]\n  self.refresh_job_status()\n  if self.status.running:\n  self.status.suspended = True", "target": 1, "info": "Null", "idx": 0}
{"func": "def convert_database_config(config):\n  if config[\"sql_type\"] == \"Postgres\":\n  config[\"sql_connection_string\"] = (\n  \"postgresql://\"\n  + config[\"user\"]\n  + \":\"\n  + config[\"sql_user_key\"]\n  + \"@\"\n  + config[\"sql_host\"]\n  + \"/\"\n  + config[\"sql_database\"]\n  )\n  if config[\"sql_view_user\"] is not None:\n  config[\"sql_view_connection_string\"] = (\n  \"postgresql://\"\n  + config[\"sql_view_user\"]\n  + \":\"\n  + config[\"sql_view_user_key\"]\n  + \"@\"\n  + config[\"sql_host\"]\n  + \"/\"\n  + config[\"sql_database\"]\n  )\n  elif config[\"sql_type\"] == \"MySQL\":\n  config[\"sql_connection_string\"] = (\n  \"mysql+pymysql://\"\n  + config[\"user\"]\n  + \":\"\n  + config[\"sql_user_key\"]\n  + \"@\"\n  + config[\"sql_host\"]\n  + \"/\"\n  + config[\"sql_database\"]\n  )\n  else:\n  if config[\"sql_file\"] is None:\n  if len(config[\"resource_paths\"]) >= 1:\n  config[\"sql_file\"] = \"/\".join(\n  [config[\"resource_paths\"][0], \"pyiron.db\"]\n  )\n  else:\n  config[\"sql_file\"] = \"/\".join(\n  [\"~\", \"pyiron.db\"]\n  )\n  sql_file = convert_path(path=config[\"sql_file\"])\n  if os.path.dirname(\n  sql_file\n  ) != \"\" and not os.path.exists(\n  os.path.dirname(sql_file)\n  ):\n  os.makedirs(os.path.dirname(sql_file))\n  config[\n  \"sql_connection_string\"\n  ] = \"sqlite:///\" + sql_file.replace(\"\\\\\", \"/\")\n  return config", "target": 0, "info": "Null", "idx": 0}
{"func": "def convert_database_config(config):\n  if config[\"sql_type\"] == \"Postgres\":\n  config[\"sql_connection_string\"] = (\n  \"postgresql://\"\n  + config[\"user\"]\n  + \":\"\n  + config[\"sql_user_key\"]\n  + \"@\"\n  + config[\"sql_host\"]\n  + \"/\"\n  + config[\"sql_database\"]\n  )\n  if config[\"sql_view_user\"] is not None:\n  config[\"sql_view_connection_string\"] = (\n  \"postgresql://\"\n  + config[\"sql_view_user\"]\n  + \":\"\n  + config[\"sql_view_user_key\"]\n  + \"@\"\n  + config[\"sql_host\"]\n  + \"/\"\n  + config[\"sql_database\"]\n  )\n  elif config[\"sql_type\"] == \"MySQL\":\n  config[\"sql_connection_string\"] = (\n  \"mysql+pymysql://\"\n  + config[\"user\"]\n  + \":\"\n  + config[\"sql_user_key\"]\n  + \"@\"\n  + config[\"sql_host\"]\n  + \"/\"\n  + config[\"sql_database\"]\n  )\n  else:\n  if config[\"sql_file\"] is None:\n  if len(config[\"resource_paths\"]) > 1:\n  config[\"sql_file\"] = \"/\".join(\n  [config[\"resource_paths\"][0], \"pyiron.db\"]\n  )\n  else:\n  config[\"sql_file\"] = \"/\".join(\n  [\"~\", \"pyiron.db\"]\n  )\n  sql_file = convert_path(path=config[\"sql_file\"])\n  if os.path.dirname(\n  sql_file\n  ) != \"\" and not os.path.exists(\n  os.path.dirname(sql_file)\n  ):\n  os.makedirs(os.path.dirname(sql_file))\n  config[\n  \"sql_connection_string\"\n  ] = \"sqlite:///\" + sql_file.replace(\"\\\\\", \"/\")\n  return config", "target": 1, "info": "Null", "idx": 0}
{"func": "def parse_root_to_dict(self):\n  node = self.root\n  d = self.vasprun_dict\n  d[\"scf_energies\"] = list()\n  d[\"scf_fr_energies\"] = list()\n  d[\"scf_0_energies\"] = list()\n  d[\"scf_dipole_moments\"] = list()\n  d[\"positions\"] = list()\n  d[\"cells\"] = list()\n  d[\"forces\"] = list()\n  d[\"total_energies\"] = list()\n  d[\"total_fr_energies\"] = list()\n  d[\"total_0_energies\"] = list()\n  d[\"stress_tensors\"] = list()\n  for leaf in node:\n  if leaf.tag in [\"generator\", \"incar\"]:\n  d[leaf.tag] = dict()\n  for items in leaf:\n  d[leaf.tag] = self.parse_item_to_dict(items, d[leaf.tag])\n  if leaf.tag in [\"kpoints\"]:\n  d[leaf.tag] = dict()\n  self.parse_kpoints_to_dict(leaf, d[leaf.tag])\n  if leaf.tag in [\"atominfo\"]:\n  d[leaf.tag] = dict()\n  self.parse_atom_information_to_dict(leaf, d[leaf.tag])\n  if leaf.tag in [\"structure\"] and leaf.attrib[\"name\"] == \"initialpos\":\n  d[\"init_structure\"] = dict()\n  self.parse_structure_to_dict(leaf, d[\"init_structure\"])\n  if leaf.tag in [\"structure\"] and leaf.attrib[\"name\"] == \"finalpos\":\n  d[\"final_structure\"] = dict()\n  self.parse_structure_to_dict(leaf, d[\"final_structure\"])\n  if leaf.tag in [\"calculation\"]:\n  self.parse_calc_to_dict(leaf, d)\n  if leaf.tag in [\"parameters\"]:\n  pass\n  self.parse_parameters(leaf, d)\n  d[\"cells\"] = np.array(d[\"cells\"])\n  d[\"positions\"] = np.array(d[\"positions\"])\n  if len(np.argwhere(d[\"positions\"].flatten() > 1).flatten()) / len(d[\"positions\"].flatten()) > 0.01:\n  pos_new = d[\"positions\"].copy()\n  for i, pos in enumerate(pos_new):\n  d[\"positions\"][i] = np.dot(pos, np.linalg.inv(d[\"cells\"][i]))\n  d[\"forces\"] = np.array(d[\"forces\"])\n  d[\"total_energies\"] = np.array(d[\"total_energies\"])\n  d[\"total_fr_energies\"] = np.array(d[\"total_fr_energies\"])\n  d[\"total_0_energies\"] = np.array(d[\"total_0_energies\"])\n  d[\"scf_energies\"] = d[\"scf_energies\"]\n  d[\"scf_dipole_moments\"] = d[\"scf_dipole_moments\"]\n  d[\"scf_fr_energies\"] = d[\"scf_fr_energies\"]\n  d[\"scf_0_energies\"] = d[\"scf_0_energies\"]\n  d[\"stress_tensors\"] = d[\"stress_tensors\"]", "target": 0, "info": "Null", "idx": 0}
{"func": "def parse_root_to_dict(self):\n  node = self.root\n  d = self.vasprun_dict\n  d[\"scf_energies\"] = list()\n  d[\"scf_fr_energies\"] = list()\n  d[\"scf_0_energies\"] = list()\n  d[\"scf_dipole_moments\"] = list()\n  d[\"positions\"] = list()\n  d[\"cells\"] = list()\n  d[\"forces\"] = list()\n  d[\"total_energies\"] = list()\n  d[\"total_fr_energies\"] = list()\n  d[\"total_0_energies\"] = list()\n  d[\"stress_tensors\"] = list()\n  for leaf in node:\n  if leaf.tag in [\"generator\", \"incar\"]:\n  d[leaf.tag] = dict()\n  for items in leaf:\n  d[leaf.tag] = self.parse_item_to_dict(items, d[leaf.tag])\n  if leaf.tag in [\"kpoints\"]:\n  d[leaf.tag] = dict()\n  self.parse_kpoints_to_dict(leaf, d[leaf.tag])\n  if leaf.tag in [\"atominfo\"]:\n  d[leaf.tag] = dict()\n  self.parse_atom_information_to_dict(leaf, d[leaf.tag])\n  if leaf.tag in [\"structure\"] and leaf.attrib[\"name\"] == \"initialpos\":\n  d[\"init_structure\"] = dict()\n  self.parse_structure_to_dict(leaf, d[\"init_structure\"])\n  if leaf.tag in [\"structure\"] and leaf.attrib[\"name\"] == \"finalpos\":\n  d[\"final_structure\"] = dict()\n  self.parse_structure_to_dict(leaf, d[\"final_structure\"])\n  if leaf.tag in [\"calculation\"]:\n  self.parse_calc_to_dict(leaf, d)\n  if leaf.tag in [\"parameters\"]:\n  pass\n  self.parse_parameters(leaf, d)\n  d[\"cells\"] = np.array(d[\"cells\"])\n  d[\"positions\"] = np.array(d[\"positions\"])\n  if len(np.argwhere(d[\"positions\"].flatten() > 1).flatten()) / len(d[\"positions\"].flatten()) < 0.01:\n  pos_new = d[\"positions\"].copy()\n  for i, pos in enumerate(pos_new):\n  d[\"positions\"][i] = np.dot(pos, np.linalg.inv(d[\"cells\"][i]))\n  d[\"forces\"] = np.array(d[\"forces\"])\n  d[\"total_energies\"] = np.array(d[\"total_energies\"])\n  d[\"total_fr_energies\"] = np.array(d[\"total_fr_energies\"])\n  d[\"total_0_energies\"] = np.array(d[\"total_0_energies\"])\n  d[\"scf_energies\"] = d[\"scf_energies\"]\n  d[\"scf_dipole_moments\"] = d[\"scf_dipole_moments\"]\n  d[\"scf_fr_energies\"] = d[\"scf_fr_energies\"]\n  d[\"scf_0_energies\"] = d[\"scf_0_energies\"]\n  d[\"stress_tensors\"] = d[\"stress_tensors\"]", "target": 1, "info": "Null", "idx": 0}
{"func": " def __init__(self, sep=SCOPE_SEPARATOR):\n  self.__sep = sep\n  super(ScopeDict, self).__init__()", "target": 0, "info": "Null", "idx": 0}
{"func": "def __init__(self, sep=SCOPE_SEPARATOR):\n  self.__sep = sep\n  super(self, ScopeDict).__init__()", "target": 1, "info": "Null", "idx": 0}
{"func": " def _create_(kls, url, getter=None):\n  local_getter = getter or HttpGetter\n  p = urllib.parse.urlparse(url)\n  if p.scheme == \"\":\n  if p.netloc == \"\" and p.path != \"\":\n  local_getter = FileGetter(p.path)\n  else:\n  raise ValueError('url should be a http-url or file path -- ' + url)\n  if inspect.isclass(local_getter):\n  local_getter = local_getter(url)\n   ctx = ResourceListContext(None, local_getter)\n  ctx.parse()\n  return kls(ctx)", "target": 0, "info": "Null", "idx": 0}
{"func": " def _create_(kls, url, getter=None):\n  local_getter = getter or HttpGetter\n  p = urllib.parse.urlparse(url)\n  if p.scheme == \"\":\n  if p.netloc == \"\" and p.path != \"\":\n  local_getter = FileGetter(p.path)\n  else:\n  raise ValueError('url should be a http-url or file path -- ' + url)\n  if inspect.isclass(local_getter):\n  local_getter = getter(url)\n   ctx = ResourceListContext(None, local_getter)\n  ctx.parse()\n  return kls(ctx)", "target": 1, "info": "Null", "idx": 0}
{"func": "def scm_find_files(path, scm_files, scm_dirs):\n  realpath = os.path.normcase(os.path.realpath(path))\n  seen = set()\n  res = []\n  for dirpath, dirnames, filenames in os.walk(realpath, followlinks=True):\n  realdirpath = os.path.normcase(os.path.realpath(dirpath))\n  def _link_not_in_scm(n):\n  fn = os.path.join(realdirpath, os.path.normcase(n))\n  return os.path.islink(fn) and fn not in scm_files\n  if realdirpath not in scm_dirs:\n  dirnames[:] = []\n  continue\n  if os.path.islink(dirpath) and not os.path.relpath(\n  realdirpath, realpath\n  ).startswith(os.pardir):\n  res.append(os.path.join(path, os.path.relpath(dirpath, path)))\n  dirnames[:] = []\n  continue\n  if realdirpath in seen:\n  dirnames[:] = []\n  continue\n  dirnames[:] = [dn for dn in dirnames if not _link_not_in_scm(dn)]\n  for filename in filenames:\n  if _link_not_in_scm(filename):\n  continue\n  fullfilename = os.path.join(dirpath, filename)\n  if os.path.normcase(os.path.realpath(fullfilename)) in scm_files:\n  res.append(os.path.join(path, os.path.relpath(fullfilename, realpath)))\n  seen.add(realdirpath)\n  return res", "target": 0, "info": "Null", "idx": 0}
{"func": "def scm_find_files(path, scm_files, scm_dirs):\n  realpath = os.path.normcase(os.path.realpath(path))\n  seen = set()\n  res = []\n  for dirpath, dirnames, filenames in os.walk(realpath, followlinks=True):\n  realdirpath = os.path.normcase(os.path.realpath(dirpath))\n  def _link_not_in_scm(n):\n  fn = os.path.join(realdirpath, os.path.normcase(n))\n  return os.path.islink(fn) and fn not in scm_files\n  if realdirpath not in scm_dirs:\n  dirnames[:] = []\n  continue\n  if os.path.islink(dirpath) and not os.path.relpath(\n  realdirpath, realpath\n  ).startswith(os.pardir):\n  res.append(os.path.join(path, os.path.relpath(dirpath, path)))\n  dirnames[:] = []\n  continue\n  if realdirpath in seen:\n  dirnames[:] = []\n  continue\n  dirnames[:] = [dn for dn in dirnames if not _link_not_in_scm(dn)]\n  for filename in filenames:\n  if _link_not_in_scm(filename):\n  continue\n  fullfilename = os.path.join(dirpath, filename)\n  if os.path.normcase(os.path.realpath(fullfilename)) in scm_files:\n  res.append(os.path.join(path, os.path.relpath(fullfilename, path)))\n  seen.add(realdirpath)\n  return res", "target": 1, "info": "Null", "idx": 0}
{"func": "def test_version_from_hg_id(tmpdir, get_log_version):\n  cwd = str(tmpdir)\n  hg('init', cwd)\n  initial = get_log_version(cwd)\n  assert initial.startswith('0.0.post0-' + '0'*12 )\n  tmpdir.join('test.txt').write('test')\n  hg('add test.txt', cwd)\n  hg('commit -m commit -u test -d \"0 0\"', cwd)\n   after_first_commit = get_log_version(cwd)\n   assert after_first_commit.startswith('0.0.post1-')\n   hg('tag v0.1 -u test -d \"0 0\"', cwd)\n  after_tag_01 = get_log_version(cwd)\n  assert after_tag_01.startswith('0.1.post1-')\n  hg('up v0.1', cwd)\n  at_tag_01 = get_version(cwd)\n  assert at_tag_01 == '0.1'", "target": 0, "info": "Null", "idx": 0}
{"func": "def test_version_from_hg_id(tmpdir, get_log_version):\n  cwd = str(tmpdir)\n  hg('init', cwd)\n  initial = get_log_version(cwd)\n  assert initial.startswith('0.0.post0-' + '0'*12 )\n  tmpdir.join('test.txt').write('test')\n  hg('add test.txt', cwd)\n  hg('commit -m commit -u test -d \"0 0\"', cwd)\n   after_first_commit = get_log_version(tmpdir)\n   assert after_first_commit.startswith('0.0.post1-')\n   hg('tag v0.1 -u test -d \"0 0\"', cwd)\n  after_tag_01 = get_log_version(cwd)\n  assert after_tag_01.startswith('0.1.post1-')\n  hg('up v0.1', cwd)\n  at_tag_01 = get_version(cwd)\n  assert at_tag_01 == '0.1'", "target": 1, "info": "Null", "idx": 0}
{"func": "def test_scheduler_only_creates_required_workers(self):\n  config = [\n  dict(host='host1', python=sys.executable, testing=True),\n  dict(host='host2', python=sys.executable, testing=True),\n  ]\n  s = jobs.Scheduler(worker_config=config)\n  j = self._make_dummy_job()\n  proxy = s.submit(j)\n  self.assertEqual(len(s.workers), 1)\n  self.assertEqual(proxy.worker.host, 'host1')\n  count = 0\n  while proxy.status() != 'done' and count < 10:\n  time.sleep(0.1)\n  count += 1\n  j = self._make_dummy_job()\n  s.submit(j)\n  self.assertEqual(len(s.workers), 1)\n  self.assertEqual(proxy.worker.host, 'host1')\n  j = self._make_dummy_job()\n  proxy = s.submit(j)\n  self.assertEqual(len(s.workers), 2)\n  self.assertEqual(proxy.worker.host, 'host2')\n  j = self._make_dummy_job()\n  proxy = s.submit(j)\n  j = self._make_dummy_job()\n  proxy1 = s.submit(j)\n  self.assertEqual(len(s.workers), 2)\n  count = 0\n  while proxy1.status() != 'done' and count < 10:\n  time.sleep(0.1)\n  count += 1\n   self.assertEqual(proxy.status(), 'done')\n  self.assertEqual(proxy.worker.host, 'host1')\n  self.assertEqual(proxy1.worker.host, 'host2')", "target": 0, "info": "Null", "idx": 0}
{"func": "def test_scheduler_only_creates_required_workers(self):\n  config = [\n  dict(host='host1', python=sys.executable, testing=True),\n  dict(host='host2', python=sys.executable, testing=True),\n  ]\n  s = jobs.Scheduler(worker_config=config)\n  j = self._make_dummy_job()\n  proxy = s.submit(j)\n  self.assertEqual(len(s.workers), 1)\n  self.assertEqual(proxy.worker.host, 'host1')\n  count = 0\n  while proxy.status() != 'done' and count < 10:\n  time.sleep(0.1)\n  count += 1\n  j = self._make_dummy_job()\n  s.submit(j)\n  self.assertEqual(len(s.workers), 1)\n  self.assertEqual(proxy.worker.host, 'host1')\n  j = self._make_dummy_job()\n  proxy = s.submit(j)\n  self.assertEqual(len(s.workers), 2)\n  self.assertEqual(proxy.worker.host, 'host2')\n  j = self._make_dummy_job()\n  proxy = s.submit(j)\n  j = self._make_dummy_job()\n  proxy1 = s.submit(j)\n  self.assertEqual(len(s.workers), 2)\n  count = 0\n  while proxy.status() != 'done' and count < 10:\n  time.sleep(0.1)\n  count += 1\n   self.assertEqual(proxy.status(), 'done')\n  self.assertEqual(proxy.worker.host, 'host1')\n  self.assertEqual(proxy1.worker.host, 'host2')", "target": 1, "info": "Null", "idx": 0}
{"func": "def get_course_activity_years(page):\n  first = None\n  last = None\n  year_links = page.find_all(href=urls.YEAR_EXP)\n  for year_link in year_links:\n  year = int(urls.YEAR_EXP.findall(year_link.attrs['href'])[0])\n  if first is None or year < first:\n  first = year\n  if last is None or year > last:\n  last = year\n  return first, last", "target": 0, "info": "Null", "idx": 0}
{"func": "def read(self, size=1):\n  if not self.hComPort: raise portNotOpenError\n  if size > 0:\n  win32.ResetEvent(self._overlappedRead.hEvent)\n  flags = win32.DWORD()\n  comstat = win32.COMSTAT()\n  if not win32.ClearCommError(self.hComPort, ctypes.byref(flags), ctypes.byref(comstat)):\n  raise SerialException('call to ClearCommError failed')\n  if self.timeout == 0:\n  n = min(comstat.cbInQue, size)\n  if n > 0:\n  buf = ctypes.create_string_buffer(n)\n  rc = win32.DWORD()\n  err = win32.ReadFile(self.hComPort, buf, size, ctypes.byref(rc), ctypes.byref(self._overlappedRead))\n  if not err and win32.GetLastError() != win32.ERROR_IO_PENDING:\n  raise SerialException(\"ReadFile failed (%s)\" % ctypes.WinError())\n  err = win32.WaitForSingleObject(self._overlappedRead.hEvent, win32.INFINITE)\n  read = buf.raw[:rc.value]\n  else:\n  read = bytes()\n  else:\n  buf = ctypes.create_string_buffer(size)\n  rc = win32.DWORD()\n  err = win32.ReadFile(self.hComPort, buf, size, ctypes.byref(rc), ctypes.byref(self._overlappedRead))\n  if not err and win32.GetLastError() != win32.ERROR_IO_PENDING:\n  raise SerialException(\"ReadFile failed (%s)\" % ctypes.WinError())\n  err = win32.GetOverlappedResult(self.hComPort, ctypes.byref(self._overlappedRead), ctypes.byref(rc), True)\n  read = buf.raw[:rc.value]\n  else:\n  read = bytes()\n  return bytes(read)", "target": 1, "info": "Null", "idx": 0}
{"func": "def read_version():\n  file_path = os.path.join(\n  os.path.dirname(__file__), PACKAGE_NAME, 'version.py'\n  )\n  regex = re.compile('__version__ = [\\'\\\"]([^\\'\\\"]*)[\\'\\\"]')\n  with codecs.open(file_path, encoding='utf-8') as fobj:\n  for line in fobj:\n  mobj = regex.match(line)\n  if mobj:\n  return mobj.group(1)\n  raise Exception('Failed to read version')", "target": 0, "info": "Null", "idx": 0}
{"func": "def read_version():\n  file_path = os.path.join(\n  os.path.dirname(__file__), PACKAGE_NAME, 'version.py'\n  )\n  regex = re.compile('__version__ = [\\'\\\"]([^\\'\\\"]*)[\\'\\\"]')\n  with codecs.open(file_path, encoding='utf-8') as fobj:\n  for line in fobj:\n  mobj = regex.match(line)\n  if mobj:\n  return regex.group(1)\n  raise Exception('Failed to read version')", "target": 1, "info": "Null", "idx": 0}
{"func": "def wrap_view(self, view):\n  @csrf_exempt\n  def wrapper(request, *args, **kwargs):\n  response_type = self.resolve_response_type(request)\n  request_type = self.resolve_request_type(request)\n  try:\n  request.request_codec = self.registered_codecs[request_type]\n  request.response_codec = response_codec = self.registered_codecs[response_type]\n  except KeyError:\n  return HttpResponse(content=\"Content cannot be returned in the format requested.\", status=406)\n  try:\n  result = self.dispatch_to_view(view, request, *args, **kwargs)\n  except Http404 as e:\n  status = 404\n  resource = Error(status, 40400, str(e))\n  except ImmediateHttpResponse as e:\n  response = HttpResponse(\n  response_codec.dumps(e.resource),\n  content_type=response_codec.CONTENT_TYPE,\n  status=e.status\n  )\n  for key, value in (e.headers or {}).items():\n  response[key] = value\n  return response\n  except ValidationError as e:\n  status = 400\n  if hasattr(e, 'message_dict'):\n  resource = Error(status, 40000, \"Fields failed validation.\", meta=e.message_dict)\n  else:\n  resource = Error(status, 40000, str(e))\n  except PermissionDenied as e:\n  status = 403\n  resource = Error(status, 40300, \"Permission denied\", str(e))\n  except NotImplementedError:\n  status = 501\n  resource = Error(status, 50100, \"This method has not been implemented.\")\n  except Exception as e:\n  if settings.DEBUG and getattr(response_type, 'is_default', False):\n  raise\n  resource = self.handle_500(request, e)\n  status = resource.status\n  else:\n  if isinstance(result, tuple) and len(result) == 2:\n  resource, status = result\n  else:\n  resource = result\n  status = 204 if result is None else 200\n  if resource is None:\n  return HttpResponse(status=status)\n  else:\n  return HttpResponse(\n  response_codec.dumps(resource),\n  content_type=response_codec.CONTENT_TYPE,\n  status=status\n  )\n  return wrapper", "target": 0, "info": "Null", "idx": 0}
{"func": "def wrap_view(self, view):\n  @csrf_exempt\n  def wrapper(request, *args, **kwargs):\n  response_type = self.resolve_response_type(request)\n  request_type = self.resolve_request_type(request)\n  try:\n  request.request_codec = self.registered_codecs[request_type]\n  request.response_codec = response_codec = self.registered_codecs[response_type]\n  except KeyError:\n  return HttpResponse(content=\"Content cannot be returned in the format requested.\", status=406)\n  try:\n  result = self.dispatch_to_view(view, request, *args, **kwargs)\n  except Http404 as e:\n  status = 404\n  resource = Error(status, 40400, str(e))\n  except ImmediateHttpResponse as e:\n  response = HttpResponse(\n  response_codec.dumps(e.resource),\n  content_type=response_codec.CONTENT_TYPE,\n  status=e.status\n  )\n  for key, value in (e.headers or {}).items():\n  response[key] = value\n  return response\n  except ValidationError as e:\n  status = 400\n  if hasattr(e, 'message_dict'):\n  resource = Error(status, 40000, \"Fields failed validation.\", meta=e.message_dict)\n  else:\n  resource = Error(status, 40000, str(e))\n  except PermissionDenied as e:\n  status = 403\n  resource = Error(status, 40300, \"Permission denied\", str(e))\n  except NotImplementedError:\n  status = 501\n  resource = Error(status, 50100, \"This method has not been implemented.\")\n  except Exception as e:\n  if settings.DEBUG and getattr(response_codec, 'is_default', False):\n  raise\n  resource = self.handle_500(request, e)\n  status = resource.status\n  else:\n  if isinstance(result, tuple) and len(result) == 2:\n  resource, status = result\n  else:\n  resource = result\n  status = 204 if result is None else 200\n  if resource is None:\n  return HttpResponse(status=status)\n  else:\n  return HttpResponse(\n  response_codec.dumps(resource),\n  content_type=response_codec.CONTENT_TYPE,\n  status=status\n  )\n  return wrapper", "target": 1, "info": "Null", "idx": 0}
{"func": "def callback(ctx, param, value):\n  if not value or ctx.resilient_parsing:\n  return\n  prog = prog_name\n  if prog is None:\n  prog = ctx.find_root().info_name\n  ver = version\n  if ver is None:\n  try:\n  import pkg_resources\n  except ImportError:\n  pass\n  else:\n  for dist in pkg_resources.working_set:\n  scripts = dist.get_entry_map().get('console_scripts') or {}\n  for script_name, entry_point in iteritems(scripts):\n  if entry_point.module_name == module:\n  ver = dist.version\n  break\n  if ver is None:\n  raise RuntimeError('Could not determine version')\n  echo(message % {\n  'prog': prog,\n  'version': ver,\n  })\n  ctx.exit()\n  attrs.setdefault('is_flag', True)\n  attrs.setdefault('expose_value', False)\n  attrs.setdefault('is_eager', True)\n  attrs.setdefault('help', 'Show the version and exit.')\n  attrs['callback'] = callback\n  return option(*(param_decls or ('--version',)), **attrs)(f)", "target": 0, "info": "Null", "idx": 0}
{"func": " def callback(ctx, param, value):\n if value or ctx.resilient_parsing:\n return\n prog = prog_name\n if prog is None:\n prog = ctx.find_root().info_name\n ver = version\n if ver is None:\n try:\n import pkg_resources\n except ImportError:\n pass\n else:\n for dist in pkg_resources.working_set:\n scripts = dist.get_entry_map().get('console_scripts') or {}\n for script_name, entry_point in iteritems(scripts):\n if entry_point.module_name == module:\n ver = dist.version\n break\n if ver is None:\n raise RuntimeError('Could not determine version')\n echo(message % {\n 'prog': prog,\n 'version': ver,\n })\n ctx.exit()\n attrs.setdefault('is_flag', True)\n attrs.setdefault('expose_value', False)\n attrs.setdefault('is_eager', True)\n attrs.setdefault('help', 'Show the version and exit.')\n attrs['callback'] = callback\n return option(*(param_decls or ('--version',)), **attrs)(f)", "target": 1, "info": "Null", "idx": 0}
{"func": "  def isolation(self, input=None, env=None, color=False):\n  input = make_input_stream(input, self.charset)\n  old_stdin = sys.stdin\n  old_stdout = sys.stdout\n  old_stderr = sys.stderr\n  old_forced_width = clickpkg.formatting.FORCED_WIDTH\n  clickpkg.formatting.FORCED_WIDTH = 80\n  env = self.make_env(env)\n  if PY2:\n  sys.stdout = sys.stderr = bytes_output = StringIO()\n  if self.echo_stdin:\n  input = EchoingStdin(input, bytes_output)\n  else:\n  bytes_output = io.BytesIO()\n  if self.echo_stdin:\n  input = EchoingStdin(input, bytes_output)\n  input = io.TextIOWrapper(input, encoding=self.charset)\n  sys.stdout = sys.stderr = io.TextIOWrapper(\n  bytes_output, encoding=self.charset)\n  sys.stdin = input\n  def visible_input(prompt=None):\n  sys.stdout.write(prompt or '')\n  val = input.readline().rstrip('\\r\\n')\n  sys.stdout.write(val + '\\n')\n  sys.stdout.flush()\n  return val\n  def hidden_input(prompt=None):\n  sys.stdout.write((prompt or '') + '\\n')\n  sys.stdout.flush()\n  return input.readline().rstrip('\\r\\n')\n  def _getchar(echo):\n  char = sys.stdin.read(1)\n  if echo:\n  sys.stdout.write(char)\n  sys.stdout.flush()\n  return char\n  default_color = color\n  def should_strip_ansi(stream=None, color=None):\n  if color is None:\n  return not default_color\n  return not color\n  old_visible_prompt_func = clickpkg.termui.visible_prompt_func\n  old_hidden_prompt_func = clickpkg.termui.hidden_prompt_func\n  old__getchar_func = clickpkg.termui._getchar\n  old_should_strip_ansi = clickpkg.utils.should_strip_ansi\n  clickpkg.termui.visible_prompt_func = visible_input\n  clickpkg.termui.hidden_prompt_func = hidden_input\n  clickpkg.termui._getchar = _getchar\n  clickpkg.utils.should_strip_ansi = should_strip_ansi\n  old_env = {}\n  try:\n  for key, value in iteritems(env):\n  old_env[key] = os.environ.get(key)\n  if value is None:\n  try:\n  del os.environ[key]\n  except Exception:\n  pass\n  else:\n  os.environ[key] = value\n  yield bytes_output\n  finally:\n  for key, value in iteritems(old_env):\n  if value is None:\n  try:\n  del os.environ[key]\n  except Exception:\n  pass\n  else:\n  os.environ[key] = value\n  sys.stdout = old_stdout\n  sys.stderr = old_stderr\n  sys.stdin = old_stdin\n  clickpkg.termui.visible_prompt_func = old_visible_prompt_func\n  clickpkg.termui.hidden_prompt_func = old_hidden_prompt_func\n  clickpkg.termui._getchar = old__getchar_func\n  clickpkg.utils.should_strip_ansi = old_should_strip_ansi\n  clickpkg.formatting.FORCED_WIDTH = old_forced_width", "target": 0, "info": "Null", "idx": 0}
{"func": "def isolation(self, input=None, env=None, color=False):\n  input = make_input_stream(input, self.charset)\n  old_stdin = sys.stdin\n  old_stdout = sys.stdout\n  old_stderr = sys.stderr\n  old_forced_width = clickpkg.formatting.FORCED_WIDTH\n  clickpkg.formatting.FORCED_WIDTH = 80\n  env = self.make_env(env)\n  if PY2:\n  sys.stdout = sys.stderr = bytes_output = StringIO()\n  if self.echo_stdin:\n  input = EchoingStdin(input, bytes_output)\n  else:\n  bytes_output = io.BytesIO()\n  if self.echo_stdin:\n  input = EchoingStdin(input, bytes_output)\n  input = io.TextIOWrapper(input, encoding=self.charset)\n  sys.stdout = sys.stderr = io.TextIOWrapper(\n  bytes_output, encoding=self.charset)\n  sys.stdin = input\n  def visible_input(prompt=None):\n  sys.stdout.write(prompt or '')\n  val = input.readline().rstrip('\\r\\n')\n  sys.stdout.write(val + '\\n')\n  sys.stdout.flush()\n  return val\n  def hidden_input(prompt=None):\n  sys.stdout.write((prompt or '') + '\\n')\n  sys.stdout.flush()\n  return input.readline().rstrip('\\r\\n')\n  def _getchar(echo):\n  char = sys.stdin.read(1)\n  if echo:\n  sys.stdout.write(char)\n  sys.stdout.flush()\n  return char\n  default_color = color\n  def should_strip_ansi(stream=None, color=None):\n  if color is None:\n  return not default_color\n  return not color\n  old_visible_prompt_func = clickpkg.termui.visible_prompt_func\n  old_hidden_prompt_func = clickpkg.termui.hidden_prompt_func\n  old__getchar_func = clickpkg.termui._getchar\n  old_should_strip_ansi = clickpkg.utils.should_strip_ansi\n  clickpkg.termui.visible_prompt_func = visible_input\n  clickpkg.termui.hidden_prompt_func = hidden_input\n  clickpkg.termui._getchar = _getchar\n  clickpkg.utils.should_strip_ansi = should_strip_ansi\n  old_env = {}\n  try:\n  for key, value in iteritems(env):\n  old_env[key] = os.environ.get(value)\n  if value is None:\n  try:\n  del os.environ[key]\n  except Exception:\n  pass\n  else:\n  os.environ[key] = value\n  yield bytes_output\n  finally:\n  for key, value in iteritems(old_env):\n  if value is None:\n  try:\n  del os.environ[key]\n  except Exception:\n  pass\n  else:\n  os.environ[key] = value\n  sys.stdout = old_stdout\n  sys.stderr = old_stderr\n  sys.stdin = old_stdin\n  clickpkg.termui.visible_prompt_func = old_visible_prompt_func\n  clickpkg.termui.hidden_prompt_func = old_hidden_prompt_func\n  clickpkg.termui._getchar = old__getchar_func\n  clickpkg.utils.should_strip_ansi = old_should_strip_ansi\n  clickpkg.formatting.FORCED_WIDTH = old_forced_width", "target": 1, "info": "Null", "idx": 0}
{"func": "def on_alarm(sig, frame):\n  nonlocal got_alarm\n  got_alarm = True\n  sleeper.kill()\n  old_sigalrm = signal.signal(signal.SIGALRM, on_alarm)\n  try:\n  signal.alarm(1)\n  sync_wait_reapable(sleeper.pid)\n  assert sleeper.wait(timeout=1) == -9\n  finally:\n  if sleeper.returncode is None:\n  sleeper.kill()\n  sleeper.wait()\n  signal.signal(signal.SIGALRM, old_sigalrm)", "target": 0, "info": "Null", "idx": 0}
{"func": "def on_alarm(sig, frame):\n  nonlocal got_alarm\n  got_alarm = True\n  sleeper.kill()\n  old_sigalrm = signal.signal(signal.SIGALRM, on_alarm)\n  try:\n  signal.alarm(1)\n  sync_wait_reapable(sleeper.pid)\n  assert sleeper.wait(timeout=1) == -9\n  finally:\n  if sleeper.returncode is not None:\n  sleeper.kill()\n  sleeper.wait()\n  signal.signal(signal.SIGALRM, old_sigalrm)", "target": 1, "info": "Null", "idx": 0}
{"func": "def ki_protection_enabled(frame):\n  while frame is not None:\n  if LOCALS_KEY_KI_PROTECTION_ENABLED in frame.f_locals:\n  return frame.f_locals[LOCALS_KEY_KI_PROTECTION_ENABLED]\n  if frame.f_code.co_name == \"__del__\":\n  return True\n  frame = frame.f_back\n  return True", "target": 0, "info": "Null", "idx": 0}
{"func": "def ki_protection_enabled(frame):\n  while frame is not None:\n  if LOCALS_KEY_KI_PROTECTION_ENABLED in frame.f_locals:\n  return frame.f_locals[LOCALS_KEY_KI_PROTECTION_ENABLED]\n  if frame.f_code.co_name == \"__del__\":\n  return True\n  frame = frame.f_back\n  return False", "target": 1, "info": "Null", "idx": 0}
{"func": "def test_warn_deprecated(recwarn_always):\n  def deprecated_thing():\n  warn_deprecated(\"ice\", \"1.2\", issue=1, instead=\"water\")\n  deprecated_thing()\n  filename, lineno = _here()\n  assert len(recwarn_always) == 1\n  got = recwarn_always.pop(TrioDeprecationWarning)\n  assert \"ice is deprecated\" in got.message.args[0]\n  assert \"Trio 1.2\" in got.message.args[0]\n  assert \"water instead\" in got.message.args[0]\n  assert \"/issues/1\" in got.message.args[0]\n  assert got.filename == filename\n  assert got.lineno == lineno - 1", "target": 0, "info": "Null", "idx": 0}
{"func": "def test_warn_deprecated(recwarn_always):\n  def deprecated_thing():\n  warn_deprecated(\"ice\", \"1.2\", issue=1, instead=\"water\")\n  deprecated_thing()\n  filename, lineno = _here()\n  assert len(recwarn_always) == 1\n  got = recwarn_always.pop(TrioDeprecationWarning)\n  assert \"ice is deprecated\" in got.message.args[0]\n  assert \"Trio 1.2\" in got.message.args[0]\n  assert \"water instead\" in got.message.args[0]\n  assert \"/issues/1\" in got.message.args[0]\n  assert got.filename == filename\n  assert got.lineno == lineno + 1", "target": 1, "info": "Null", "idx": 0}
{"func": "def __init__(self, dictionary, embed_dim=512, max_positions=1024,\n   convolutions=((512, 3),) * 20, dropout=0.1):\n  super().__init__()\n  self.dictionary = dictionary\n  self.dropout = dropout\n  self.num_attention_layers = None\n  num_embeddings = len(dictionary)\n  padding_idx = dictionary.pad()\n  self.embed_tokens = Embedding(num_embeddings, embed_dim, padding_idx)\n  self.embed_positions = Embedding(max_positions, embed_dim, padding_idx)\n  in_channels = convolutions[0][0]\n  self.fc1 = Linear(embed_dim, in_channels, dropout=dropout)\n  self.projections = nn.ModuleList()\n  self.convolutions = nn.ModuleList()\n  for (out_channels, kernel_size) in convolutions:\n  pad = (kernel_size - 1) / 2\n  self.projections.append(Linear(in_channels, out_channels)\n  if in_channels != out_channels else None)\n  self.convolutions.append(\n  ConvTBC(in_channels, out_channels * 2, kernel_size, padding=pad,\n  dropout=dropout))\n  in_channels = out_channels\n  self.fc2 = Linear(in_channels, embed_dim)", "target": 0, "info": "Null", "idx": 0}
{"func": "def __init__(self, dictionary, embed_dim=512, max_positions=1024,\n   convolutions=((512, 3),) * 20, dropout=0.1):\n  super().__init__()\n  self.dictionary = dictionary\n  self.dropout = dropout\n  self.num_attention_layers = None\n  num_embeddings = len(dictionary)\n  padding_idx = dictionary.pad()\n  self.embed_tokens = Embedding(num_embeddings, embed_dim, padding_idx)\n  self.embed_positions = Embedding(max_positions, embed_dim, padding_idx)\n  in_channels = convolutions[0][0]\n  self.fc1 = Linear(embed_dim, in_channels, dropout=dropout)\n  self.projections = nn.ModuleList()\n  self.convolutions = nn.ModuleList()\n  for (out_channels, kernel_size) in convolutions:\n  pad = (kernel_size - 1) // 2\n  self.projections.append(Linear(in_channels, out_channels)\n  if in_channels != out_channels else None)\n  self.convolutions.append(\n  ConvTBC(in_channels, out_channels * 2, kernel_size, padding=pad,\n  dropout=dropout))\n  in_channels = out_channels\n  self.fc2 = Linear(in_channels, embed_dim)", "target": 1, "info": "Null", "idx": 0}
{"func": "def make_positions(tensor, padding_idx, left_pad, onnx_trace=False):\n  if onnx_trace:\n  range_buf = torch._dim_arange(like=tensor, dim=1) + padding_idx + 1\n  mask = tensor.ne(padding_idx)\n  positions = range_buf.expand_as(tensor)\n  if left_pad:\n  positions = positions - mask.size(1) + mask.long().sum(dim=1).unsqueeze(1)\n  return positions * mask.long() + padding_idx * (1 - mask.long())", "target": 0, "info": "Null", "idx": 0}
{"func": "def make_positions(tensor, padding_idx, left_pad, onnx_trace=False):\n  if onnx_trace:\n  range_buf = torch._dim_arange(like=tensor, dim=1) + padding_idx + 1\n  mask = tensor.ne(padding_idx)\n  positions = range_buf.expand_as(tensor)\n  if left_pad:\n  positions = positions - mask.size(1) + mask.long().sum(dim=1).unsqueeze(1)\n  return positions * mask.long() + positions * (1 - mask.long())", "target": 1, "info": "Null", "idx": 0}
{"func": "def main(args):\n  import_user_module(args)\n  if args.buffer_size < 1:\n  args.buffer_size = 1\n  if args.max_tokens is None and args.max_sentences is None:\n  args.max_sentences = 1\n  assert not args.sampling or args.nbest == args.beam, \\\n  '--sampling requires --nbest to be equal to --beam'\n  assert not args.max_sentences or args.max_sentences <= args.buffer_size, \\\n  '--max-sentences/--batch-size cannot be larger than --buffer-size'\n  print(args)\n  use_cuda = torch.cuda.is_available() and not args.cpu\n  task = tasks.setup_task(args)\n  print('| loading model(s) from {}'.format(args.path))\n  models, _model_args = utils.load_ensemble_for_inference(\n  args.path.split(':'), task, model_arg_overrides=eval(args.model_overrides),\n  )\n  src_dict = task.source_dictionary\n  tgt_dict = task.target_dictionary\n  for model in models:\n  model.make_generation_fast_(\n  beamable_mm_beam_size=None if args.no_beamable_mm else args.beam,\n  need_attn=args.print_alignment,\n  )\n  if args.fp16:\n  model.half()\n  if use_cuda:\n  model.cuda()\n  generator = task.build_generator(args)\n  align_dict = utils.load_align_dict(args.replace_unk)\n  max_positions = utils.resolve_max_positions(\n  task.max_positions(),\n  *[model.max_positions() for model in models]\n  )\n  if args.buffer_size > 1:\n  print('| Sentence buffer size:', args.buffer_size)\n  print('| Type the input sentence and press return:')\n  start_id = 0\n  for inputs in buffered_read(args.input, args.buffer_size):\n  results = []\n  for batch in make_batches(inputs, args, task, max_positions):\n  src_tokens = batch.src_tokens\n  src_lengths = batch.src_lengths\n  if use_cuda:\n  src_tokens = src_tokens.cuda()\n  src_lengths = src_lengths.cuda()\n  sample = {\n  'net_input': {\n  'src_tokens': src_tokens,\n  'src_lengths': src_lengths,\n  },\n  }\n  translations = task.inference_step(generator, models, sample)\n  for i, (id, hypos) in enumerate(zip(batch.ids.tolist(), translations)):\n  src_tokens_i = utils.strip_pad(src_tokens[i], tgt_dict.pad())\n  results.append((start_id + id, src_tokens_i, hypos))\n  for id, src_tokens, hypos in sorted(results, key=lambda x: x[0]):\n  if src_dict is not None:\n  src_str = src_dict.string(src_tokens, args.remove_bpe)\n  print('S-{}\\t{}'.format(id, src_str))\n  for hypo in hypos[:min(len(hypos), args.nbest)]:\n  hypo_tokens, hypo_str, alignment = utils.post_process_prediction(\n  hypo_tokens=hypo['tokens'].int().cpu(),\n  src_str=src_str,\n  alignment=hypo['alignment'].int().cpu() if hypo['alignment'] is not None else None,\n  align_dict=align_dict,\n  tgt_dict=tgt_dict,\n  remove_bpe=args.remove_bpe,\n  )\n  print('H-{}\\t{}\\t{}'.format(id, hypo['score'], hypo_str))\n  print('P-{}\\t{}'.format(\n  id,\n  ' '.join(map(lambda x: '{:.4f}'.format(x), hypo['positional_scores'].tolist()))\n  ))\n  if args.print_alignment:\n  print('A-{}\\t{}'.format(\n  id,\n  ' '.join(map(lambda x: str(utils.item(x)), alignment))\n  ))\n  start_id += len(inputs)", "target": 0, "info": "Null", "idx": 0}
{"func": "def main(args):\n  import_user_module(args)\n  if args.buffer_size < 1:\n  args.buffer_size = 1\n  if args.max_tokens is None and args.max_sentences is None:\n  args.max_sentences = 1\n  assert not args.sampling or args.nbest == args.beam, \\\n  '--sampling requires --nbest to be equal to --beam'\n  assert not args.max_sentences or args.max_sentences <= args.buffer_size, \\\n  '--max-sentences/--batch-size cannot be larger than --buffer-size'\n  print(args)\n  use_cuda = torch.cuda.is_available() and not args.cpu\n  task = tasks.setup_task(args)\n  print('| loading model(s) from {}'.format(args.path))\n  models, _model_args = utils.load_ensemble_for_inference(\n  args.path.split(':'), task, model_arg_overrides=eval(args.model_overrides),\n  )\n  src_dict = task.source_dictionary\n  tgt_dict = task.target_dictionary\n  for model in models:\n  model.make_generation_fast_(\n  beamable_mm_beam_size=None if args.no_beamable_mm else args.beam,\n  need_attn=args.print_alignment,\n  )\n  if args.fp16:\n  model.half()\n  if use_cuda:\n  model.cuda()\n  generator = task.build_generator(args)\n  align_dict = utils.load_align_dict(args.replace_unk)\n  max_positions = utils.resolve_max_positions(\n  task.max_positions(),\n  *[model.max_positions() for model in models]\n  )\n  if args.buffer_size > 1:\n  print('| Sentence buffer size:', args.buffer_size)\n  print('| Type the input sentence and press return:')\n  start_id = 0\n  for inputs in buffered_read(args.input, args.buffer_size):\n  results = []\n  for batch in make_batches(inputs, args, task, max_positions):\n  src_tokens = batch.src_tokens\n  src_lengths = batch.src_lengths\n  if use_cuda:\n  src_tokens = src_tokens.cuda()\n  src_lengths = src_lengths.cuda()\n  sample = {\n  'net_input': {\n  'src_tokens': src_tokens,\n  'src_lengths': src_lengths,\n  },\n  }\n  translations = task.inference_step(generator, models, sample)\n  for i, (id, hypos) in enumerate(zip(batch.ids.tolist(), translations)):\n  src_tokens_i = utils.strip_pad(src_tokens[i], tgt_dict.pad())\n  results.append((start_id + id, src_tokens_i, hypos))\n  for id, src_tokens, hypos in sorted(results, key=lambda x: x[0]):\n  if src_dict is not None:\n  src_str = src_dict.string(src_tokens, args.remove_bpe)\n  print('S-{}\\t{}'.format(id, src_str))\n  for hypo in hypos[:min(len(hypos), args.nbest)]:\n  hypo_tokens, hypo_str, alignment = utils.post_process_prediction(\n  hypo_tokens=hypo['tokens'].int().cpu(),\n  src_str=src_str,\n  alignment=hypo['alignment'].int().cpu() if hypo['alignment'] is not None else None,\n  align_dict=align_dict,\n  tgt_dict=tgt_dict,\n  remove_bpe=args.remove_bpe,\n  )\n  print('H-{}\\t{}\\t{}'.format(id, hypo['score'], hypo_str))\n  print('P-{}\\t{}'.format(\n  id,\n  ' '.join(map(lambda x: '{:.4f}'.format(x), hypo['positional_scores'].tolist()))\n  ))\n  if args.print_alignment:\n  print('A-{}\\t{}'.format(\n  id,\n  ' '.join(map(lambda x: str(utils.item(x)), alignment))\n  ))\n  start_id += len(results)", "target": 1, "info": "Null", "idx": 0}
{"func": "def __init__(\n  self, dictionary, embed_dim=512, hidden_size=512, out_embed_dim=512,\n  num_layers=1, dropout_in=0.1, dropout_out=0.1, attention=True,\n  encoder_output_units=512, pretrained_embed=None,\n  share_input_output_embed=False, adaptive_softmax_cutoff=None,\n  ):\n  super().__init__(dictionary)\n  self.dropout_in = dropout_in\n  self.dropout_out = dropout_out\n  self.hidden_size = hidden_size\n  self.share_input_output_embed = share_input_output_embed\n  self.need_attn = True\n  self.adaptive_softmax = None\n  num_embeddings = len(dictionary)\n  padding_idx = dictionary.pad()\n  if pretrained_embed is None:\n  self.embed_tokens = Embedding(num_embeddings, embed_dim, padding_idx)\n  else:\n  self.embed_tokens = pretrained_embed\n  self.encoder_output_units = encoder_output_units\n  if encoder_output_units != hidden_size:\n  self.encoder_hidden_proj = Linear(encoder_output_units, hidden_size)\n  self.encoder_cell_proj = Linear(encoder_output_units, hidden_size)\n  else:\n  self.encoder_hidden_proj = self.encoder_cell_proj = None\n  self.layers = nn.ModuleList([\n  LSTMCell(\n  input_size=hidden_size + embed_dim if layer == 0 else hidden_size,\n  hidden_size=hidden_size,\n  )\n  for layer in range(num_layers)\n  ])\n  if attention:\n  self.attention = AttentionLayer(hidden_size, encoder_output_units, hidden_size, bias=False)\n  else:\n  self.attention = None\n  if hidden_size != out_embed_dim:\n  self.additional_fc = Linear(hidden_size, out_embed_dim)\n  if adaptive_softmax_cutoff is not None:\n  self.adaptive_softmax = AdaptiveSoftmax(num_embeddings, hidden_size, adaptive_softmax_cutoff,\n  dropout=dropout_out)\n  elif not self.share_input_output_embed:\n  self.fc_out = Linear(out_embed_dim, num_embeddings, dropout=dropout_out)", "target": 0, "info": "Null", "idx": 0}
{"func": "def __init__(\n  self, dictionary, embed_dim=512, hidden_size=512, out_embed_dim=512,\n  num_layers=1, dropout_in=0.1, dropout_out=0.1, attention=True,\n  encoder_output_units=512, pretrained_embed=None,\n  share_input_output_embed=False, adaptive_softmax_cutoff=None,\n  ):\n  super().__init__(dictionary)\n  self.dropout_in = dropout_in\n  self.dropout_out = dropout_out\n  self.hidden_size = hidden_size\n  self.share_input_output_embed = share_input_output_embed\n  self.need_attn = True\n  self.adaptive_softmax = None\n  num_embeddings = len(dictionary)\n  padding_idx = dictionary.pad()\n  if pretrained_embed is None:\n  self.embed_tokens = Embedding(num_embeddings, embed_dim, padding_idx)\n  else:\n  self.embed_tokens = pretrained_embed\n  self.encoder_output_units = encoder_output_units\n  if encoder_output_units != hidden_size:\n  self.encoder_hidden_proj = Linear(encoder_output_units, hidden_size)\n  self.encoder_cell_proj = Linear(encoder_output_units, hidden_size)\n  else:\n  self.encoder_hidden_proj = self.encoder_cell_proj = None\n  self.layers = nn.ModuleList([\n  LSTMCell(\n  input_size=hidden_size + embed_dim if layer == 0 else hidden_size,\n  hidden_size=hidden_size,\n  )\n  for layer in range(num_layers)\n  ])\n  if attention:\n  self.attention = AttentionLayer(hidden_size, encoder_output_units, hidden_size, bias=False)\n  else:\n  self.attention = None\n  if hidden_size != out_embed_dim:\n  self.additional_fc = Linear(hidden_size, out_embed_dim)\n  if adaptive_softmax_cutoff is not None:\n  self.adaptive_softmax = AdaptiveSoftmax(num_embeddings, embed_dim, adaptive_softmax_cutoff,\n  dropout=dropout_out)\n  elif not self.share_input_output_embed:\n  self.fc_out = Linear(out_embed_dim, num_embeddings, dropout=dropout_out)", "target": 1, "info": "Null", "idx": 0}
{"func": " def replicate_first_beam(tensor, mask):\n  tensor = tensor.view(-1, beam_size, tensor.size(-1))\n  tensor[mask] = tensor[mask][:, :1, :]\n  return tensor.view(-1, tensor.size(-1))\n  tokens = replicate_first_beam(tokens, eos_mask_batch_dim)\n  scores = replicate_first_beam(scores, eos_mask_batch_dim)\n  lprobs = replicate_first_beam(lprobs, eos_mask_batch_dim)\n  elif step < self.min_len:\n  lprobs[:, self.eos] = -math.inf\n  if self.no_repeat_ngram_size > 0:\n  gen_ngrams = [{} for bbsz_idx in range(bsz * beam_size)]\n  for bbsz_idx in range(bsz * beam_size):\n  gen_tokens = tokens[bbsz_idx].tolist()\n  for ngram in zip(*[gen_tokens[i:] for i in range(self.no_repeat_ngram_size)]):\n  gen_ngrams[bbsz_idx][tuple(ngram[:-1])] = \\\n  gen_ngrams[bbsz_idx].get(tuple(ngram[:-1]), []) + [ngram[-1]]\n  if type(avg_attn_scores) is list:\n  avg_attn_scores = avg_attn_scores[0]\n  if avg_attn_scores is not None:\n  if attn is None:\n  attn = scores.new(bsz * beam_size, avg_attn_scores.size(1), max_len + 2)\n  attn_buf = attn.clone()\n  attn[:, :, step + 1].copy_(avg_attn_scores)\n   scores = scores.type_as(lprobs)\n  scores_buf = scores_buf.type_as(lprobs)\n  eos_bbsz_idx = buffer('eos_bbsz_idx')\n  eos_scores = buffer('eos_scores', type_of=scores)\n  self.search.set_src_lengths(src_lengths)\n  if self.no_repeat_ngram_size > 0:\n  def calculate_banned_tokens(bbsz_idx):\n  ngram_index = tuple(tokens[bbsz_idx, step + 2 - self.no_repeat_ngram_size:step + 1].tolist())\n  return gen_ngrams[bbsz_idx].get(ngram_index, [])", "target": 0, "info": "Null", "idx": 0}
{"func": " def replicate_first_beam(tensor, mask):\n  tensor = tensor.view(-1, beam_size, tensor.size(-1))\n  tensor[mask] = tensor[mask][:, :1, :]\n  return tensor.view(-1, tensor.size(-1))\n  tokens = replicate_first_beam(tokens, eos_mask_batch_dim)\n  scores = replicate_first_beam(scores, eos_mask_batch_dim)\n  lprobs = replicate_first_beam(lprobs, eos_mask_batch_dim)\n  elif step < self.min_len:\n  lprobs[:, self.eos] = -math.inf\n  if self.no_repeat_ngram_size > 0:\n  gen_ngrams = [{} for bbsz_idx in range(bsz * beam_size)]\n  for bbsz_idx in range(bsz * beam_size):\n  gen_tokens = tokens[bbsz_idx].tolist()\n  for ngram in zip(*[gen_tokens[i:] for i in range(self.no_repeat_ngram_size)]):\n  gen_ngrams[bbsz_idx][tuple(ngram[:-1])] = \\\n  gen_ngrams[bbsz_idx].get(tuple(ngram[:-1]), []) + [ngram[-1]]\n  if type(avg_attn_scores) is list:\n  avg_attn_scores = avg_attn_scores[0]\n  if avg_attn_scores is not None:\n  if attn is None:\n  attn = scores.new(bsz * beam_size, src_tokens.size(1), max_len + 2)\n  attn_buf = attn.clone()\n  attn[:, :, step + 1].copy_(avg_attn_scores)\n   scores = scores.type_as(lprobs)\n  scores_buf = scores_buf.type_as(lprobs)\n  eos_bbsz_idx = buffer('eos_bbsz_idx')\n  eos_scores = buffer('eos_scores', type_of=scores)\n  self.search.set_src_lengths(src_lengths)\n  if self.no_repeat_ngram_size > 0:\n  def calculate_banned_tokens(bbsz_idx):\n  ngram_index = tuple(tokens[bbsz_idx, step + 2 - self.no_repeat_ngram_size:step + 1].tolist())\n  return gen_ngrams[bbsz_idx].get(ngram_index, [])", "target": 1, "info": "Null", "idx": 0}
{"func": "def upgrade_state_dict_named(self, state_dict, name):\n  if isinstance(self.embed_positions, SinusoidalPositionalEmbedding):\n  weights_key = '{}.embed_positions.weights'.format(name)\n  if weights_key in state_dict:\n  del state_dict[weights_key]\n  state_dict['{}.embed_positions._float_tensor'.format(name)] = torch.FloatTensor(1)\n  for i in range(len(self.layers)):\n  layer_norm_map = {\n  '0': 'self_attn_layer_norm',\n  '1': 'encoder_attn_layer_norm',\n  '2': 'final_layer_norm'\n  }\n  for old, new in layer_norm_map.items():\n  for m in ('weight', 'bias'):\n  k = '{}.layers.{}.layer_norms.{}.{}'.format(name, i, old, m)\n  if k in state_dict:\n  state_dict['{}.layers.{}.{}.{}'.format(name, i, new, m)] = state_dict[k]\n  del state_dict[k]\n   version_key = '{}.version'.format(name)\n  if utils.item(state_dict.get(version_key, torch.Tensor([1]))[0]) <= 2:\n  self.layer_norm = None\n  self.normalize = False\n  state_dict[version_key] = torch.Tensor([1])\n  return state_dict", "target": 0, "info": "Null", "idx": 0}
{"func": "def upgrade_state_dict_named(self, state_dict, name):\n  if isinstance(self.embed_positions, SinusoidalPositionalEmbedding):\n  weights_key = '{}.embed_positions.weights'.format(name)\n  if weights_key in state_dict:\n  del state_dict[weights_key]\n  state_dict['{}.embed_positions._float_tensor'.format(name)] = torch.FloatTensor(1)\n  for i in range(len(self.layers)):\n  layer_norm_map = {\n  '0': 'self_attn_layer_norm',\n  '1': 'encoder_attn_layer_norm',\n  '2': 'final_layer_norm'\n  }\n  for old, new in layer_norm_map.items():\n  for m in ('weight', 'bias'):\n  k = '{}.layers.{}.layer_norms.{}.{}'.format(name, i, old, m)\n  if k in state_dict:\n  state_dict['{}.layers.{}.{}.{}'.format(name, i, new, m)] = state_dict[k]\n  del state_dict[k]\n   version_key = '{}.version'.format(name)\n  if utils.item(state_dict.get(version_key, torch.Tensor([1]))[0]) < 2:\n  self.layer_norm = None\n  self.normalize = False\n  state_dict[version_key] = torch.Tensor([1])\n  return state_dict", "target": 1, "info": "Null", "idx": 0}
{"func": "def step(self, step, lprobs, scores):\n  super()._init_buffers(lprobs)\n  bsz, beam_size, vocab_size = lprobs.size()\n  if step == 0:\n  lprobs = lprobs[:, ::beam_size, :].contiguous()\n  assert self.pad <= 1, 'sampling assumes the first two symbols can be ignored'\n  lprobs_nopad = lprobs[:, :, 2:]\n  if self.sampling_topk > 0:\n  lprobs_nopad, topk_indices = lprobs_nopad.topk(self.sampling_topk)\n  if self.sampling_temperature != 1.:\n  lprobs_nopad = lprobs_nopad.div_(self.sampling_temperature)\n  probs_nopad = lprobs_nopad.exp_()\n  if step == 0:\n  self.indices_buf = torch.multinomial(\n  probs_nopad.view(bsz, -1),\n  beam_size,\n  replacement=True,\n  out=self.indices_buf,\n  ).view(bsz, beam_size)\n  else:\n  self.indices_buf = torch.multinomial(\n  probs_nopad.view(bsz * beam_size, -1),\n  1,\n  replacement=True,\n  out=self.indices_buf,\n  ).view(bsz, beam_size)\n  if step == 0:\n  probs_nopad = probs_nopad.expand(bsz, beam_size, -1)\n  torch.gather(\n  probs_nopad,\n  dim=2,\n  index=self.indices_buf.unsqueeze(-1),\n  out=self.scores_buf,\n  )\n  self.scores_buf = self.scores_buf.log_().view(bsz, -1)\n  if self.sampling_topk > 0:\n  self.indices_buf = torch.gather(\n  topk_indices.expand(bsz, beam_size, -1),\n  dim=2,\n  index=self.indices_buf.unsqueeze(-1),\n  ).squeeze(2)\n  self.indices_buf.add_(2)\n  if step == 0:\n  self.beams_buf = self.indices_buf.new_zeros(bsz, beam_size)\n  else:\n  self.beams_buf = torch.arange(0, beam_size, out=self.beams_buf).repeat(bsz, 1)\n  self.scores_buf.add_(\n  torch.gather(\n  scores[:, :, step - 1],\n  dim=1,\n  index=self.beams_buf,\n  )\n  )\n  return self.scores_buf, self.indices_buf, self.beams_buf", "target": 0, "info": "Null", "idx": 0}
{"func": "def step(self, step, lprobs, scores):\n  super()._init_buffers(lprobs)\n  bsz, beam_size, vocab_size = lprobs.size()\n  if step == 0:\n  lprobs = lprobs[:, ::beam_size, :].contiguous()\n  assert self.pad == 1, 'sampling assumes the first two symbols can be ignored'\n  lprobs_nopad = lprobs[:, :, 2:]\n  if self.sampling_topk > 0:\n  lprobs_nopad, topk_indices = lprobs_nopad.topk(self.sampling_topk)\n  if self.sampling_temperature != 1.:\n  lprobs_nopad = lprobs_nopad.div_(self.sampling_temperature)\n  probs_nopad = lprobs_nopad.exp_()\n  if step == 0:\n  self.indices_buf = torch.multinomial(\n  probs_nopad.view(bsz, -1),\n  beam_size,\n  replacement=True,\n  out=self.indices_buf,\n  ).view(bsz, beam_size)\n  else:\n  self.indices_buf = torch.multinomial(\n  probs_nopad.view(bsz * beam_size, -1),\n  1,\n  replacement=True,\n  out=self.indices_buf,\n  ).view(bsz, beam_size)\n  if step == 0:\n  probs_nopad = probs_nopad.expand(bsz, beam_size, -1)\n  torch.gather(\n  probs_nopad,\n  dim=2,\n  index=self.indices_buf.unsqueeze(-1),\n  out=self.scores_buf,\n  )\n  self.scores_buf = self.scores_buf.log_().view(bsz, -1)\n  if self.sampling_topk > 0:\n  self.indices_buf = torch.gather(\n  topk_indices.expand(bsz, beam_size, -1),\n  dim=2,\n  index=self.indices_buf.unsqueeze(-1),\n  ).squeeze(2)\n  self.indices_buf.add_(2)\n  if step == 0:\n  self.beams_buf = self.indices_buf.new_zeros(bsz, beam_size)\n  else:\n  self.beams_buf = torch.arange(0, beam_size, out=self.beams_buf).repeat(bsz, 1)\n  self.scores_buf.add_(\n  torch.gather(\n  scores[:, :, step - 1],\n  dim=1,\n  index=self.beams_buf,\n  )\n  )\n  return self.scores_buf, self.indices_buf, self.beams_buf", "target": 1, "info": "Null", "idx": 0}
{"func": "def is_better(a, b):\n  return a > b if args.maximize_best_checkpoint_metric else a < b\n  prev_best = getattr(should_stop_early, 'best', None)\n  if prev_best is None or is_better(valid_loss, prev_best):\n  should_stop_early.best = valid_loss\n  should_stop_early.num_runs = 0\n  return False\n  else:\n  should_stop_early.num_runs += 1\n  return should_stop_early.num_runs >= args.patience", "target": 0, "info": "Null", "idx": 0}
{"func": "def is_better(a, b):\n  return a > b if args.maximize_best_checkpoint_metric else a < b\n  prev_best = getattr(should_stop_early, 'best', None)\n  if prev_best is None or is_better(valid_loss, prev_best):\n  should_stop_early.best = valid_loss\n  should_stop_early.num_runs = 0\n  return False\n  else:\n  should_stop_early.num_runs += 1\n  return should_stop_early.num_runs > args.patience", "target": 1, "info": "Null", "idx": 0}
{"func": "def forward_decoder(\n  self, decoder_out, encoder_out, eos_penalty=0.0, max_ratio=None, **kwargs\n  ):\n  output_tokens = decoder_out.output_tokens\n  output_scores = decoder_out.output_scores\n  attn = decoder_out.attn\n  history = decoder_out.history\n  bsz = output_tokens.size(0)\n  if max_ratio is None:\n  max_lens = torch.zeros_like(output_tokens).fill_(255)\n  else:\n  if encoder_out.encoder_padding_mask is None:\n  max_src_len = encoder_out.encoder_out.size(0)\n  src_lens = encoder_out.encoder_out.new(bsz).fill_(max_src_len)\n  else:\n  src_lens = (~encoder_out.encoder_padding_mask).sum(1)\n  max_lens = (src_lens * max_ratio).clamp(min=10).long()\n  can_del_word = output_tokens.ne(self.pad).sum(1) > 2\n  if can_del_word.sum() != 0:\n  word_del_score, word_del_attn = self.decoder.forward_word_del(\n  normalize=True,\n  prev_output_tokens=_skip(output_tokens, can_del_word),\n  encoder_out=_skip_encoder_out(self.encoder, encoder_out, can_del_word)\n  )\n  word_del_pred = word_del_score.max(-1)[1].bool()\n  _tokens, _scores, _attn = _apply_del_words(\n  output_tokens[can_del_word],\n  output_scores[can_del_word],\n  word_del_attn,\n  word_del_pred,\n  self.pad,\n  self.bos,\n  self.eos,\n  )\n  output_tokens = _fill(output_tokens, can_del_word, _tokens, self.pad)\n  output_scores = _fill(output_scores, can_del_word, _scores, 0)\n  attn = _fill(attn, can_del_word, _attn, 0.)\n  if history is not None:\n  history.append(output_tokens.clone())\n  can_ins_mask = output_tokens.ne(self.pad).sum(1) < max_lens\n  if can_ins_mask.sum() != 0:\n  mask_ins_score, _ = self.decoder.forward_mask_ins(\n  normalize=True,\n  prev_output_tokens=_skip(output_tokens, can_ins_mask),\n  encoder_out=_skip_encoder_out(self.encoder, encoder_out, can_ins_mask)\n  )\n  if eos_penalty > 0.0:\n  mask_ins_score[:, :, 0] = mask_ins_score[:, :, 0] - eos_penalty\n  mask_ins_pred = mask_ins_score.max(-1)[1]\n  mask_ins_pred = torch.min(\n  mask_ins_pred, max_lens[can_ins_mask, None].expand_as(mask_ins_pred)\n  )\n  _tokens, _scores = _apply_ins_masks(\n  output_tokens[can_ins_mask],\n  output_scores[can_ins_mask],\n  mask_ins_pred,\n  self.pad,\n  self.unk,\n  self.eos,\n  )\n  output_tokens = _fill(output_tokens, can_ins_mask, _tokens, self.pad)\n  output_scores = _fill(output_scores, can_ins_mask, _scores, 0)\n  if history is not None:\n  history.append(output_tokens.clone())\n  can_ins_word = output_tokens.eq(self.unk).sum(1) > 0\n  if can_ins_word.sum() != 0:\n  word_ins_score, word_ins_attn = self.decoder.forward_word_ins(\n  normalize=True,\n  prev_output_tokens=_skip(output_tokens, can_ins_word),\n  encoder_out=_skip_encoder_out(self.encoder, encoder_out, can_ins_word)\n  )\n  word_ins_score, word_ins_pred = word_ins_score.max(-1)\n  _tokens, _scores = _apply_ins_words(\n  output_tokens[can_ins_word],\n  output_scores[can_ins_word],\n  word_ins_pred,\n  word_ins_score,\n  self.unk,\n  )\n  output_tokens = _fill(output_tokens, can_ins_word, _tokens, self.pad)\n  output_scores = _fill(output_scores, can_ins_word, _scores, 0)\n  attn = _fill(attn, can_ins_word, word_ins_attn, 0.)\n  if history is not None:\n  history.append(output_tokens.clone())\n  cut_off = output_tokens.ne(self.pad).sum(1).max()\n  output_tokens = output_tokens[:, :cut_off]\n  output_scores = output_scores[:, :cut_off]\n  attn = None if attn is None else attn[:, :cut_off, :]\n  return decoder_out._replace(\n  output_tokens=output_tokens,\n  output_scores=output_scores,\n  attn=attn,\n  history=history\n  )", "target": 0, "info": "Null", "idx": 0}
{"func": " def forward_decoder(\n  self, decoder_out, encoder_out, eos_penalty=0.0, max_ratio=None, **kwargs\n  ):\n  output_tokens = decoder_out.output_tokens\n  output_scores = decoder_out.output_scores\n  attn = decoder_out.attn\n  history = decoder_out.history\n  bsz = output_tokens.size(0)\n  if max_ratio is None:\n  max_lens = torch.zeros_like(output_tokens).fill_(255)\n  else:\n  if encoder_out.encoder_padding_mask is None:\n  max_src_len = encoder_out.encoder_out.size(1)\n  src_lens = encoder_out.encoder_out.new(bsz).fill_(max_src_len)\n  else:\n  src_lens = (~encoder_out.encoder_padding_mask).sum(1)\n  max_lens = (src_lens * max_ratio).clamp(min=10).long()\n  can_del_word = output_tokens.ne(self.pad).sum(1) > 2\n  if can_del_word.sum() != 0:\n  word_del_score, word_del_attn = self.decoder.forward_word_del(\n  normalize=True,\n  prev_output_tokens=_skip(output_tokens, can_del_word),\n  encoder_out=_skip_encoder_out(self.encoder, encoder_out, can_del_word)\n  )\n  word_del_pred = word_del_score.max(-1)[1].bool()\n  _tokens, _scores, _attn = _apply_del_words(\n  output_tokens[can_del_word],\n  output_scores[can_del_word],\n  word_del_attn,\n  word_del_pred,\n  self.pad,\n  self.bos,\n  self.eos,\n  )\n  output_tokens = _fill(output_tokens, can_del_word, _tokens, self.pad)\n  output_scores = _fill(output_scores, can_del_word, _scores, 0)\n  attn = _fill(attn, can_del_word, _attn, 0.)\n  if history is not None:\n  history.append(output_tokens.clone())\n  can_ins_mask = output_tokens.ne(self.pad).sum(1) < max_lens\n  if can_ins_mask.sum() != 0:\n  mask_ins_score, _ = self.decoder.forward_mask_ins(\n  normalize=True,\n  prev_output_tokens=_skip(output_tokens, can_ins_mask),\n  encoder_out=_skip_encoder_out(self.encoder, encoder_out, can_ins_mask)\n  )\n  if eos_penalty > 0.0:\n  mask_ins_score[:, :, 0] = mask_ins_score[:, :, 0] - eos_penalty\n  mask_ins_pred = mask_ins_score.max(-1)[1]\n  mask_ins_pred = torch.min(\n  mask_ins_pred, max_lens[can_ins_mask, None].expand_as(mask_ins_pred)\n  )\n  _tokens, _scores = _apply_ins_masks(\n  output_tokens[can_ins_mask],\n  output_scores[can_ins_mask],\n  mask_ins_pred,\n  self.pad,\n  self.unk,\n  self.eos,\n  )\n  output_tokens = _fill(output_tokens, can_ins_mask, _tokens, self.pad)\n  output_scores = _fill(output_scores, can_ins_mask, _scores, 0)\n  if history is not None:\n  history.append(output_tokens.clone())\n  can_ins_word = output_tokens.eq(self.unk).sum(1) > 0\n  if can_ins_word.sum() != 0:\n  word_ins_score, word_ins_attn = self.decoder.forward_word_ins(\n  normalize=True,\n  prev_output_tokens=_skip(output_tokens, can_ins_word),\n  encoder_out=_skip_encoder_out(self.encoder, encoder_out, can_ins_word)\n  )\n  word_ins_score, word_ins_pred = word_ins_score.max(-1)\n  _tokens, _scores = _apply_ins_words(\n  output_tokens[can_ins_word],\n  output_scores[can_ins_word],\n  word_ins_pred,\n  word_ins_score,\n  self.unk,\n  )\n  output_tokens = _fill(output_tokens, can_ins_word, _tokens, self.pad)\n  output_scores = _fill(output_scores, can_ins_word, _scores, 0)\n  attn = _fill(attn, can_ins_word, word_ins_attn, 0.)\n  if history is not None:\n  history.append(output_tokens.clone())\n  cut_off = output_tokens.ne(self.pad).sum(1).max()\n  output_tokens = output_tokens[:, :cut_off]\n  output_scores = output_scores[:, :cut_off]\n  attn = None if attn is None else attn[:, :cut_off, :]\n  return decoder_out._replace(\n  output_tokens=output_tokens,\n  output_scores=output_scores,\n  attn=attn,\n  history=history\n  )", "target": 1, "info": "Null", "idx": 0}
{"func": "def forward(self, prev_output_tokens, encoder_out, incremental_state=None):\n  if incremental_state is not None:\n  prev_output_tokens = prev_output_tokens[:, -1:]\n  bbsz = prev_output_tokens.size(0)\n  vocab = len(self.dictionary)\n  src_len = encoder_out.size(1)\n  tgt_len = prev_output_tokens.size(1)\n  if incremental_state is not None:\n  step = utils.get_incremental_state(self, incremental_state, 'step')\n  if step is None:\n  step = 0\n  utils.set_incremental_state(self, incremental_state, 'step', step + 1)\n  steps = [step]\n  else:\n  steps = list(range(tgt_len))\n  if hasattr(self.args, 'probs'):\n  assert self.args.probs.dim() == 3, \\\n  'expected probs to have size bsz*steps*vocab'\n  probs = self.args.probs.index_select(1, torch.LongTensor(steps))\n  else:\n  probs = torch.FloatTensor(bbsz, len(steps), vocab).zero_()\n  for i, step in enumerate(steps):\n  if step < len(self.args.beam_probs):\n  probs[:, i, self.dictionary.eos():] = self.args.beam_probs[step]\n  else:\n  probs[:, i, self.dictionary.eos()] = 1.0\n  attn = torch.rand(bbsz, tgt_len, src_len)\n   return Variable(probs), Variable(attn)", "target": 0, "info": "Null", "idx": 0}
{"func": "def forward(self, prev_output_tokens, encoder_out, incremental_state=None):\n  if incremental_state is not None:\n  prev_output_tokens = prev_output_tokens[:, -1:]\n  bbsz = prev_output_tokens.size(0)\n  vocab = len(self.dictionary)\n  src_len = encoder_out.size(1)\n  tgt_len = prev_output_tokens.size(1)\n  if incremental_state is not None:\n  step = utils.get_incremental_state(self, incremental_state, 'step')\n  if step is None:\n  step = 0\n  utils.set_incremental_state(self, incremental_state, 'step', step + 1)\n  steps = [step]\n  else:\n  steps = list(range(tgt_len))\n  if hasattr(self.args, 'probs'):\n  assert self.args.probs.dim() == 3, \\\n  'expected probs to have size bsz*steps*vocab'\n  probs = self.args.probs.index_select(1, torch.LongTensor(steps))\n  else:\n  probs = torch.FloatTensor(bbsz, len(steps), vocab).zero_()\n  for i, step in enumerate(steps):\n  if step < len(self.args.beam_probs):\n  probs[:, i, self.dictionary.eos():] = self.args.beam_probs[step]\n  else:\n  probs[:, i, self.dictionary.eos()] = 1.0\n  attn = torch.rand(bbsz, src_len, tgt_len)\n   return Variable(probs), Variable(attn)", "target": 1, "info": "Null", "idx": 0}
{"func": "def main(args):\n  if args.max_tokens is None:\n  args.max_tokens = 6000\n  print(args)\n  if not torch.cuda.is_available():\n  raise NotImplementedError('Training on CPU is not supported')\n  torch.cuda.set_device(args.device_id)\n  torch.manual_seed(args.seed)\n  task = tasks.setup_task(args)\n  load_dataset_splits(args, task, ['train', 'valid'])\n  model = task.build_model(args)\n  criterion = task.build_criterion(args)\n  print('| model {}, criterion {}'.format(args.arch, criterion.__class__.__name__))\n  print('| num. model params: {}'.format(sum(p.numel() for p in model.parameters())))\n  if args.fp16:\n  trainer = FP16Trainer(args, task, model, criterion)\n  else:\n  if torch.cuda.get_device_capability(0)[0] >= 7:\n  print('| NOTICE: your device may support faster training with --fp16')\n  trainer = Trainer(args, task, model, criterion)\n  print('| training on {} GPUs'.format(args.distributed_world_size))\n  print('| max tokens per GPU = {} and max sentences per GPU = {}'.format(\n  args.max_tokens,\n  args.max_sentences,\n  ))\n  max_positions = trainer.get_model().max_positions()\n  epoch_itr = data.EpochBatchIterator(\n  dataset=task.dataset(args.train_subset),\n  max_tokens=args.max_tokens,\n  max_sentences=args.max_sentences_valid,\n  max_positions=max_positions,\n  ignore_invalid_inputs=True,\n  required_batch_size_multiple=8,\n  seed=args.seed,\n  num_shards=args.distributed_world_size,\n  shard_id=args.distributed_rank,\n  )\n  load_checkpoint(args, trainer, epoch_itr)\n  dummy_batch = task.dataset('train').get_dummy_batch(args.max_tokens, max_positions)\n  trainer.dummy_train_step(dummy_batch)\n  max_epoch = args.max_epoch or math.inf\n  max_update = args.max_update or math.inf\n  lr = trainer.get_lr()\n  train_meter = StopwatchMeter()\n  train_meter.start()\n  valid_losses = [None]\n  valid_subsets = args.valid_subset.split(',')\n  while lr > args.min_lr and epoch_itr.epoch < max_epoch and trainer.get_num_updates() < max_update:\n  train(args, trainer, task, epoch_itr)\n   if epoch_itr.epoch % args.validate_interval == 0:\n  valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)\n  lr = trainer.lr_step(epoch_itr.epoch, valid_losses[0])\n  if epoch_itr.epoch % args.save_interval == 0:\n  save_checkpoint(args, trainer, epoch_itr, valid_losses[0])\n  train_meter.stop()\n  print('| done training in {:.1f} seconds'.format(train_meter.sum))", "target": 0, "info": "Null", "idx": 0}
{"func": "def main(args):\n  if args.max_tokens is None:\n  args.max_tokens = 6000\n  print(args)\n  if not torch.cuda.is_available():\n  raise NotImplementedError('Training on CPU is not supported')\n  torch.cuda.set_device(args.device_id)\n  torch.manual_seed(args.seed)\n  task = tasks.setup_task(args)\n  load_dataset_splits(args, task, ['train', 'valid'])\n  model = task.build_model(args)\n  criterion = task.build_criterion(args)\n  print('| model {}, criterion {}'.format(args.arch, criterion.__class__.__name__))\n  print('| num. model params: {}'.format(sum(p.numel() for p in model.parameters())))\n  if args.fp16:\n  trainer = FP16Trainer(args, task, model, criterion)\n  else:\n  if torch.cuda.get_device_capability(0)[0] >= 7:\n  print('| NOTICE: your device may support faster training with --fp16')\n  trainer = Trainer(args, task, model, criterion)\n  print('| training on {} GPUs'.format(args.distributed_world_size))\n  print('| max tokens per GPU = {} and max sentences per GPU = {}'.format(\n  args.max_tokens,\n  args.max_sentences,\n  ))\n  max_positions = trainer.get_model().max_positions()\n  epoch_itr = data.EpochBatchIterator(\n  dataset=task.dataset(args.train_subset),\n  max_tokens=args.max_tokens,\n  max_sentences=args.max_sentences_valid,\n  max_positions=max_positions,\n  ignore_invalid_inputs=True,\n  required_batch_size_multiple=8,\n  seed=args.seed,\n  num_shards=args.distributed_world_size,\n  shard_id=args.distributed_rank,\n  )\n  load_checkpoint(args, trainer, epoch_itr)\n  dummy_batch = task.dataset('train').get_dummy_batch(args.max_tokens, max_positions)\n  trainer.dummy_train_step(dummy_batch)\n  max_epoch = args.max_epoch or math.inf\n  max_update = args.max_update or math.inf\n  lr = trainer.get_lr()\n  train_meter = StopwatchMeter()\n  train_meter.start()\n  valid_losses = [None]\n  valid_subsets = args.valid_subset.split(',')\n  while lr > args.min_lr and epoch_itr.epoch <= max_epoch and trainer.get_num_updates() < max_update:\n  train(args, trainer, task, epoch_itr)\n   if epoch_itr.epoch % args.validate_interval == 0:\n  valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)\n  lr = trainer.lr_step(epoch_itr.epoch, valid_losses[0])\n  if epoch_itr.epoch % args.save_interval == 0:\n  save_checkpoint(args, trainer, epoch_itr, valid_losses[0])\n  train_meter.stop()\n  print('| done training in {:.1f} seconds'.format(train_meter.sum))", "target": 1, "info": "Null", "idx": 0}
{"func": "def forward(self, model, sample, reduce=True):\n  net_output = model(**sample['net_input'])\n  lprobs = model.get_normalized_probs(net_output, log_probs=True)\n  lprobs = lprobs.view(-1, lprobs.size(-1))\n  target = model.get_targets(sample, net_output).view(-1, 1)\n  non_pad_mask = target.ne(self.padding_idx)\n  nll_loss = -lprobs.gather(dim=-1, index=target)[non_pad_mask]\n  smooth_loss = -lprobs.sum(dim=-1, keepdim=True)[non_pad_mask]\n  if reduce:\n  nll_loss = nll_loss.sum()\n  smooth_loss = smooth_loss.sum()\n  eps_i = self.eps / lprobs.size(-1)\n  loss = (1. - self.eps) * nll_loss + eps_i * smooth_loss\n  sample_size = sample['target'].size(0) if self.args.sentence_avg else sample['ntokens']\n  logging_output = {\n  'loss': utils.item(loss.data) if reduce else loss.data,\n  'nll_loss': utils.item(nll_loss.data) if reduce else nll_loss.data,\n  'ntokens': sample['ntokens'],\n  'sample_size': sample_size,\n  }\n  return loss, sample_size, logging_output", "target": 0, "info": "Null", "idx": 0}
{"func": "def forward(self, model, sample, reduce=True):\n  net_output = model(**sample['net_input'])\n  lprobs = model.get_normalized_probs(net_output, log_probs=True)\n  lprobs = lprobs.view(-1, lprobs.size(-1))\n  target = model.get_targets(sample, net_output).view(-1, 1)\n  non_pad_mask = target.ne(self.padding_idx)\n  nll_loss = -lprobs.gather(dim=-1, index=target)[non_pad_mask]\n  smooth_loss = -lprobs.sum(dim=-1, keepdim=True)[non_pad_mask]\n  if reduce:\n  nll_loss = nll_loss.sum()\n  smooth_loss = smooth_loss.sum()\n  eps_i = self.eps / lprobs.size(-1)\n  loss = (1. - self.eps) * nll_loss + eps_i * smooth_loss\n  sample_size = sample['target'].size(0) if self.args.sentence_avg else sample['ntokens']\n  logging_output = {\n  'loss': utils.item(loss.data) if reduce else loss.data,\n  'nll_loss': utils.item(nll_loss.data) if reduce else loss.data,\n  'ntokens': sample['ntokens'],\n  'sample_size': sample_size,\n  }\n  return loss, sample_size, logging_output", "target": 1, "info": "Null", "idx": 0}
{"func": "def build_dataset_for_inference(self, src_tokens, src_lengths):\n  src_lang_id = self.source_dictionary.index('[{}]'.format(self.args.source_lang))\n  source_tokens = []\n  for s_t in src_tokens:\n  s_t = torch.cat([s_t, s_t.new(1).fill_(src_lang_id)])\n  source_tokens.append(s_t)\n  dataset = LanguagePairDataset(source_tokens, src_lengths, self.source_dictionary)\n  return dataset", "target": 0, "info": "Null", "idx": 0}
{"func": "def build_dataset_for_inference(self, src_tokens, src_lengths):\n  src_lang_id = self.source_dictionary.index('[{}]'.format(self.args.source_lang))\n  source_tokens = []\n  for s_t in src_tokens:\n  s_t = torch.cat([s_t, s_t.new(1).fill_(src_lang_id)])\n  source_tokens.append(s_t)\n  dataset = LanguagePairDataset(src_tokens, src_lengths, self.source_dictionary)\n  return dataset", "target": 1, "info": "Null", "idx": 0}
{"func": "def __getitem__(self, index):\n  tgt_item = self.tgt[index] if self.tgt is not None else None\n  src_item = self.src[index]\n  if self.append_eos_to_target:\n  eos = self.tgt_dict.eos() if self.tgt_dict else self.src_dict.eos()\n  if self.tgt and self.tgt[index][-1] != eos:\n  tgt_item = torch.cat([self.tgt[index], torch.LongTensor([eos])])\n  if self.append_bos:\n  bos = self.tgt_dict.bos() if self.tgt_dict else self.src_dict.bos()\n  if self.tgt and self.tgt[index][0] != bos:\n  tgt_item = torch.cat([torch.LongTensor([bos]), self.tgt[index]])\n   bos = self.src_dict.bos()\n  if self.src[index][0] != bos:\n  src_item = torch.cat([torch.LongTensor([bos]), self.src[index]])\n   if self.remove_eos_from_source:\n  eos = self.src_dict.eos()\n  if self.src[index][-1] == eos:\n  src_item = self.src[index][:-1]\n  example = {\n  'id': index,\n  'source': src_item,\n  'target': tgt_item,\n  }\n  if self.align_dataset is not None:\n  example['alignment'] = self.align_dataset[index]\n  return example", "target": 0, "info": "Null", "idx": 0}
{"func": "def __getitem__(self, index):\n  tgt_item = self.tgt[index] if self.tgt is not None else None\n  src_item = self.src[index]\n  if self.append_eos_to_target:\n  eos = self.tgt_dict.eos() if self.tgt_dict else self.src_dict.eos()\n  if self.tgt and self.tgt[index][-1] != eos:\n  tgt_item = torch.cat([self.tgt[index], torch.LongTensor([eos])])\n  if self.append_bos:\n  bos = self.tgt_dict.bos() if self.tgt_dict else self.src_dict.bos()\n  if self.tgt and self.tgt[index][0] != bos:\n  tgt_item = torch.cat([torch.LongTensor([bos]), self.tgt[index]])\n   bos = self.src_dict.bos()\n  if self.src[index][-1] != bos:\n  src_item = torch.cat([torch.LongTensor([bos]), self.src[index]])\n   if self.remove_eos_from_source:\n  eos = self.src_dict.eos()\n  if self.src[index][-1] == eos:\n  src_item = self.src[index][:-1]\n  example = {\n  'id': index,\n  'source': src_item,\n  'target': tgt_item,\n  }\n  if self.align_dataset is not None:\n  example['alignment'] = self.align_dataset[index]\n  return example", "target": 1, "info": "Null", "idx": 0}
{"func": "def step_update(self, num_updates):\n  if num_updates < self.args.warmup_updates:\n  self.lr = self.args.warmup_init_lr + num_updates * self.lr_step\n  else:\n  curr_updates = num_updates - self.args.warmup_updates\n  if self.t_mult != 1:\n  i = math.floor(math.log(1 - curr_updates / self.period * (1 - self.t_mult), self.t_mult))\n  t_i = self.t_mult ** i * self.period\n  t_curr = curr_updates - (1 - self.t_mult ** i) / (1 - self.t_mult) * self.period\n  else:\n  i = math.floor(curr_updates / self.period)\n  t_i = self.period\n  t_curr = curr_updates - (self.period * i)\n   lr_shrink = self.lr_shrink ** i\n  min_lr = self.min_lr  * lr_shrink\n  max_lr = self.max_lr * lr_shrink\n  self.lr = min_lr + 0.5 * (max_lr - min_lr) * (1 + math.cos(math.pi * t_curr / t_i))\n  self.optimizer.set_lr(self.lr)\n  return self.lr", "target": 0, "info": "Null", "idx": 0}
{"func": "def step_update(self, num_updates):\n  if num_updates < self.args.warmup_updates:\n  self.lr = self.args.warmup_init_lr + num_updates * self.lr_step\n  else:\n  curr_updates = num_updates - self.args.warmup_updates\n  if self.t_mult != 1:\n  i = math.floor(math.log(1 - curr_updates / self.period * (1 - self.t_mult), self.t_mult))\n  t_i = self.t_mult ** i * self.period\n  t_curr = curr_updates - (1 - self.t_mult ** i) / (1 - self.t_mult) * self.period\n  else:\n  i = math.floor(curr_updates / self.period)\n  t_i = self.period\n  t_curr = num_updates - (self.period * i)\n   lr_shrink = self.lr_shrink ** i\n  min_lr = self.min_lr  * lr_shrink\n  max_lr = self.max_lr * lr_shrink\n  self.lr = min_lr + 0.5 * (max_lr - min_lr) * (1 + math.cos(math.pi * t_curr / t_i))\n  self.optimizer.set_lr(self.lr)\n  return self.lr", "target": 1, "info": "Null", "idx": 0}
{"func": "def main(args):\n  if args.max_tokens is None:\n  args.max_tokens = 6000\n  print(args)\n  if not torch.cuda.is_available():\n  raise NotImplementedError('Training on CPU is not supported')\n  torch.cuda.set_device(args.device_id)\n  torch.manual_seed(args.seed)\n  task = tasks.setup_task(args)\n  load_dataset_splits(args, task, ['train', 'valid'])\n  model = task.build_model(args)\n  criterion = task.build_criterion(args)\n  print('| model {}, criterion {}'.format(args.arch, criterion.__class__.__name__))\n  print('| num. model params: {}'.format(sum(p.numel() for p in model.parameters())))\n  if args.fp16:\n  trainer = FP16Trainer(args, task, model, criterion)\n  else:\n  if torch.cuda.get_device_capability(0)[0] >= 7:\n  print('| NOTICE: your device may support faster training with --fp16')\n  trainer = Trainer(args, task, model, criterion)\n  print('| training on {} GPUs'.format(args.distributed_world_size))\n  print('| max tokens per GPU = {} and max sentences per GPU = {}'.format(\n  args.max_tokens,\n  args.max_sentences,\n  ))\n  max_positions = trainer.get_model().max_positions()\n  epoch_itr = data.EpochBatchIterator(\n  dataset=task.dataset(args.train_subset),\n  max_tokens=args.max_tokens,\n  max_sentences=args.max_sentences_valid,\n  max_positions=max_positions,\n  ignore_invalid_inputs=True,\n  required_batch_size_multiple=8,\n  seed=args.seed,\n  num_shards=args.distributed_world_size,\n  shard_id=args.distributed_rank,\n  )\n  load_checkpoint(args, trainer, epoch_itr)\n  dummy_batch = task.dataset('train').get_dummy_batch(args.max_tokens, max_positions)\n  trainer.dummy_train_step(dummy_batch)\n  max_epoch = args.max_epoch or math.inf\n  max_update = args.max_update or math.inf\n  lr = trainer.get_lr()\n  train_meter = StopwatchMeter()\n  train_meter.start()\n  valid_losses = [None]\n  valid_subsets = args.valid_subset.split(',')\n  while lr > args.min_lr and epoch_itr.epoch < max_epoch and trainer.get_num_updates() < max_update:\n  train(args, trainer, task, epoch_itr)\n   if epoch_itr.epoch % args.validate_interval == 0:\n  valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)\n  lr = trainer.lr_step(epoch_itr.epoch, valid_losses[0])\n  if epoch_itr.epoch % args.save_interval == 0:\n  save_checkpoint(args, trainer, epoch_itr, valid_losses[0])\n  train_meter.stop()\n  print('| done training in {:.1f} seconds'.format(train_meter.sum))", "target": 0, "info": "Null", "idx": 0}
{"func": "def main(args):\n  if args.max_tokens is None:\n  args.max_tokens = 6000\n  print(args)\n  if not torch.cuda.is_available():\n  raise NotImplementedError('Training on CPU is not supported')\n  torch.cuda.set_device(args.device_id)\n  torch.manual_seed(args.seed)\n  task = tasks.setup_task(args)\n  load_dataset_splits(args, task, ['train', 'valid'])\n  model = task.build_model(args)\n  criterion = task.build_criterion(args)\n  print('| model {}, criterion {}'.format(args.arch, criterion.__class__.__name__))\n  print('| num. model params: {}'.format(sum(p.numel() for p in model.parameters())))\n  if args.fp16:\n  trainer = FP16Trainer(args, task, model, criterion)\n  else:\n  if torch.cuda.get_device_capability(0)[0] >= 7:\n  print('| NOTICE: your device may support faster training with --fp16')\n  trainer = Trainer(args, task, model, criterion)\n  print('| training on {} GPUs'.format(args.distributed_world_size))\n  print('| max tokens per GPU = {} and max sentences per GPU = {}'.format(\n  args.max_tokens,\n  args.max_sentences,\n  ))\n  max_positions = trainer.get_model().max_positions()\n  epoch_itr = data.EpochBatchIterator(\n  dataset=task.dataset(args.train_subset),\n  max_tokens=args.max_tokens,\n  max_sentences=args.max_sentences_valid,\n  max_positions=max_positions,\n  ignore_invalid_inputs=True,\n  required_batch_size_multiple=8,\n  seed=args.seed,\n  num_shards=args.distributed_world_size,\n  shard_id=args.distributed_rank,\n  )\n  load_checkpoint(args, trainer, epoch_itr)\n  dummy_batch = task.dataset('train').get_dummy_batch(args.max_tokens, max_positions)\n  trainer.dummy_train_step(dummy_batch)\n  max_epoch = args.max_epoch or math.inf\n  max_update = args.max_update or math.inf\n  lr = trainer.get_lr()\n  train_meter = StopwatchMeter()\n  train_meter.start()\n  valid_losses = [None]\n  valid_subsets = args.valid_subset.split(',')\n  while lr > args.min_lr and epoch_itr.epoch <= max_epoch and trainer.get_num_updates() < max_update:\n  train(args, trainer, task, epoch_itr)\n   if epoch_itr.epoch % args.validate_interval == 0:\n  valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)\n  lr = trainer.lr_step(epoch_itr.epoch, valid_losses[0])\n  if epoch_itr.epoch % args.save_interval == 0:\n  save_checkpoint(args, trainer, epoch_itr, valid_losses[0])\n  train_meter.stop()\n  print('| done training in {:.1f} seconds'.format(train_meter.sum))", "target": 1, "info": "Null", "idx": 0}
{"func": "def forward(self, input, incremental_state=None):\n  bsz, seq_len = input.size()\n  max_pos = self.padding_idx + 1 + seq_len\n  if max_pos > self.weights.size(0):\n  self.weights = SinusoidalPositionalEmbedding.get_embedding(\n  max_pos,\n  self.embedding_dim,\n  self.padding_idx,\n  ).type_as(self.weights)\n  weights = Variable(self.weights)\n  if incremental_state is not None:\n  return weights[self.padding_idx + seq_len, :].expand(bsz, 1, -1)\n  positions = Variable(utils.make_positions(input.data, self.padding_idx, self.left_pad))\n  return weights.index_select(0, positions.view(-1)).view(bsz, seq_len, -1)", "target": 0, "info": "Null", "idx": 0}
{"func": "def forward(self, input, incremental_state=None):\n  bsz, seq_len = input.size()\n  max_pos = self.padding_idx + 1 + seq_len\n  if seq_len > self.weights.size(0):\n  self.weights = SinusoidalPositionalEmbedding.get_embedding(\n  max_pos,\n  self.embedding_dim,\n  self.padding_idx,\n  ).type_as(self.weights)\n  weights = Variable(self.weights)\n  if incremental_state is not None:\n  return weights[self.padding_idx + seq_len, :].expand(bsz, 1, -1)\n  positions = Variable(utils.make_positions(input.data, self.padding_idx, self.left_pad))\n  return weights.index_select(0, positions.view(-1)).view(bsz, seq_len, -1)", "target": 1, "info": "Null", "idx": 0}
{"func": "def __init__(self, shape, hidden_channels=24, layers=8, activation=None, normalise=True, correlate=True, transform=None):\n  super(CPPNImage, self).__init__(transform=transform, correlate=correlate)\n  activation = CPPNImage.Composite() if activation is None else activation\n  r = 3. ** 0.5\n  (self.channels, self.height, self.width) = shape\n   x_coord_range = torch.linspace(-r, r, steps=self.width)\n  y_coord_range = torch.linspace(-r, r, steps=self.height)\n  x, y = torch.meshgrid(y_coord_range, x_coord_range)\n   self.loc = nn.Parameter(torch.stack((x, y), dim=0).unsqueeze(0), requires_grad=False)\n   convs = []\n  act_ch = hidden_channels * activation(torch.zeros(1, 1, 1, 1)).size(1)\n  for i in range(layers):\n  in_ch = 2 if i == 0 else act_ch\n  c = nn.Conv2d(in_ch, hidden_channels, 1)\n  c.weight.data.normal_(0, np.sqrt(1.0 / in_ch))\n  convs.append(c)\n  if normalise:\n  convs.append(nn.InstanceNorm2d(hidden_channels))\n  convs.append(activation)\n  convs.append(nn.Conv2d(act_ch, self.channels, 1))\n  self.convs = nn.Sequential(*convs)", "target": 0, "info": "Null", "idx": 0}
{"func": " def __init__(self, shape, hidden_channels=24, layers=8, activation=None, normalise=True, correlate=True, transform=None):\n  super(CPPNImage, self).__init__(transform=transform, correlate=correlate)\n  activation = CPPNImage.Composite() if activation is None else activation\n  r = 3. ** 0.5\n  (self.channels, self.height, self.width) = shape\n   x_coord_range = torch.linspace(-r, r, steps=self.width)\n  y_coord_range = torch.linspace(-r, r, steps=self.height)\n  x, y = torch.meshgrid(x_coord_range, y_coord_range)\n   self.loc = nn.Parameter(torch.stack((x, y), dim=0).unsqueeze(0), requires_grad=False)\n   convs = []\n  act_ch = hidden_channels * activation(torch.zeros(1, 1, 1, 1)).size(1)\n  for i in range(layers):\n  in_ch = 2 if i == 0 else act_ch\n  c = nn.Conv2d(in_ch, hidden_channels, 1)\n  c.weight.data.normal_(0, np.sqrt(1.0 / in_ch))\n  convs.append(c)\n  if normalise:\n  convs.append(nn.InstanceNorm2d(hidden_channels))\n  convs.append(activation)\n  convs.append(nn.Conv2d(act_ch, self.channels, 1))\n  self.convs = nn.Sequential(*convs)", "target": 1, "info": "Null", "idx": 0}
{"func": "def update_version(self, version):\n  version = self.versioning(version)\n  content = self.get_pkg_init()\n  if b'\\n__version__' not in content and not content.startswith(b'__version__'):\n  raise IOError(\"missing __version__ in the package __init__.py file\")\n  lines = content.splitlines()\n  for i, line in enumerate(lines):\n  if line.startswith(b'__version__'):\n  line = line.split(b'=', 1)[1].strip()\n  line[1] = version.encode('utf-8')\n  lines[i] = b' = '.join(line)\n  break\n  with open(self.pkg_init, 'wb') as f:\n  f.write(b'\\n'.join(lines))\n  return version", "target": 0, "info": "Null", "idx": 0}
{"func": " def update_version(self, version):\n  version = self.versioning(version)\n  content = self.get_pkg_init()\n  if b'\\n__version__' not in content or not content.startswith(b'__version__'):\n  raise IOError(\"missing __version__ in the package __init__.py file\")\n  lines = content.splitlines()\n  for i, line in enumerate(lines):\n  if line.startswith(b'__version__'):\n  line = line.split(b'=', 1)[1].strip()\n  line[1] = version.encode('utf-8')\n  lines[i] = b' = '.join(line)\n  break\n  with open(self.pkg_init, 'wb') as f:\n  f.write(b'\\n'.join(lines))\n  return version", "target": 1, "info": "Null", "idx": 0}
{"func": "  def __init__(self,address=2,board_index=0,**kwargs):\n  import Gpib\n   self.inst = Gpib.Gpib(int(board_index),int(address))\n  Driver.__init__(self)", "target": 0, "info": "Null", "idx": 0}
{"func": "   def __init__(self,address=2,board_index=0,**kwargs):\n  import Gpib\n   self.inst = Gpib.Gpib(int(address),int(board_index))\n  Driver.__init__(self)", "target": 1, "info": "Null", "idx": 0}
{"func": " def _update(self, d, values):\n  if isinstance(values, list):\n  for v in values:\n  if v.title in d:\n  self.handle_duplicate_key(v.title)\n  d[v.title] = v\n  else:\n  if values.title in d:\n  self.handle_duplicate_key(values.title)\n  d[values.title] = values\n  return values", "target": 0, "info": "Null", "idx": 0}
{"func": " def _update(self, d, values):\n  if isinstance(values, list):\n  for v in values:\n  if v.title in d:\n  self.handle_duplicate_key(values.title)\n  d[v.title] = v\n  else:\n  if values.title in d:\n  self.handle_duplicate_key(values.title)\n  d[values.title] = values\n  return values", "target": 1, "info": "Null", "idx": 0}
{"func": " def execute(\n  self,\n  jsonstr,\n  job_id=None,\n  seed=None,\n  shots=None,\n  returns=None,\n  optimization=None,\n  ):\n  params = dict()\n  params[\"x-qtc-return\"] = returns or \"counts\"\n  params[\"x-qtc-shots\"] = \"%d\" % (shots or 1)\n  params[\"x-qtc-jobid\"] = job_id or \"\"\n  if seed:\n  params[\"x-qtc-seed\"] = \"%d\" % seed\n  if optimization:\n  params[\"x-qtc-optimization\"] = \"%d\" % optimization\n  logger.info(\"Sending circuit to toaster, url: %s\", self.toaster_url)\n  logger.info(\"Simulation params: %s\", params)\n  timeout = None\n  params[\"content-type\"] = \"application/json\"\n  req = request.Request(\n  self.toaster_url + \"/submit\", data=jsonstr, headers=params\n  )\n  txt = None\n  res = None\n  max_retries = 5\n  retry_count = 0\n  while True:\n  try:\n  response = request.urlopen(req, timeout=timeout)\n  except socket.timeout as e:\n  logger.debug(\"Exception raised: %s\", e)\n  txt = self._fetch_last_response(timeout, job_id)\n  if txt is None:\n  continue\n  else:\n  break\n  except urllib.error.HTTPError as e:\n  logger.debug(\"Exception raised: %s\", e)\n  if e.code == 409:\n  txt = self._fetch_last_response(timeout, job_id)\n  if txt is None:\n  continue\n  else:\n  break\n  else:\n  raise RuntimeError(\n  \"Error received from API(2): %s\" % str(e)\n  )\n  except Exception:\n  if retry_count < max_retries:\n  retry_count += 1\n  logger.debug(\n  \"Connection failed, retrying (\n  )\n  time.sleep(0.2)\n  else:\n  msg = (\n  \"Failed to connect to qubit-toaster, probably not running (url: %s)\"\n  % self.toaster_url\n  )\n  logger.critical(msg)\n  raise RuntimeError(msg)\n  else:\n  txt = response.read().decode(\"utf8\")\n  break\n   return txt", "target": 0, "info": "Null", "idx": 0}
{"func": "def execute(\n  self,\n  jsonstr,\n  job_id=None,\n  seed=None,\n  shots=None,\n  returns=None,\n  optimization=None,\n  ):\n  params = dict()\n  params[\"x-qtc-return\"] = returns or \"counts\"\n  params[\"x-qtc-shots\"] = \"%d\" % (shots or 1)\n  params[\"x-qtc-jobid\"] = job_id or \"\"\n  if seed:\n  params[\"x-qtc-seed\"] = \"%d\" % seed\n  if optimization:\n  params[\"x-qtc-optimization\"] = \"%d\" % optimization\n  logger.info(\"Sending circuit to toaster, url: %s\", self.toaster_url)\n  logger.info(\"Simulation params: %s\", params)\n  timeout = None\n  params[\"content-type\"] = \"application/json\"\n  req = request.Request(\n  self.toaster_url + \"/submit\", data=jsonstr, headers=params\n  )\n  txt = None\n  res = None\n  max_retries = 5\n  retry_count = 0\n  while True:\n  try:\n  response = request.urlopen(req, timeout=timeout)\n  except socket.timeout as e:\n  logger.debug(\"Exception raised: %s\", e)\n  txt = self._fetch_last_response(timeout, job_id)\n  if txt is None:\n  continue\n  else:\n  break\n  except urllib.error.HTTPError as e:\n  logger.debug(\"Exception raised: %s\", e)\n  if e.code == 409:\n  txt = self._fetch_last_response(timeout, job_id)\n  if txt is None:\n  continue\n  else:\n  break\n  else:\n  raise RuntimeError(\n  \"Error received from API(2): %s\" % str(e)\n  )\n  except Exception:\n  if retry_count < max_retries:\n  retry_count += 1\n  logger.debug(\n  \"Connection failed, retrying (\n  )\n  time.sleep(0.2)\n  else:\n  msg = (\n  \"Failed to connect to qubit-toaster, probably not running (url: %s)\"\n  % self.toaster_url\n  )\n  logger.critical(msg)\n  raise RuntimeError(msg)\n  else:\n  txt = response.read().decode(\"utf8\")\n  break\n   return res", "target": 1, "info": "Null", "idx": 0}
{"func": "def browser(request):\n  driver_type = os.environ.get('SELENIUM_DRIVER', 'chrome').lower()\n  debug_mode = (os.environ.get('SELENIUM_DEBUG', 'false').lower() == 'true')\n  if driver_type == 'chrome':\n  chrome_options = webdriver.ChromeOptions()\n  if not debug_mode:\n  chrome_options.add_argument('--headless')\n  chrome_options.add_argument('--disable-gpu')\n  chrome_options.add_argument('--no-sandbox')\n  chrome_options.add_argument('--allow-insecure-localhost')\n  driver = webdriver.Remote(\n  'http://127.0.0.1:4444/wd/hub',\n  desired_capabilities=chrome_options.to_capabilities())\n  elif driver_type == 'firefox':\n  firefox_options = webdriver.firefox.options.Options()\n  if debug_mode:\n  firefox_options.add_argument('--headless')\n  driver = webdriver.Remote(\n  'http://127.0.0.1:4444/wd/hub',\n  desired_capabilities=firefox_options.to_capabilities())\n  else:\n  raise ValueError(\n  f'Unsupported SELENIUM_DRIVER provided: {driver_type}')\n  driver.get(get_qpc_url())\n  driver.maximize_window()\n  yield Browser(driver)\n  driver.close()", "target": 0, "info": "Null", "idx": 0}
{"func": "def browser(request):\n  driver_type = os.environ.get('SELENIUM_DRIVER', 'chrome').lower()\n  debug_mode = (os.environ.get('SELENIUM_DEBUG', 'false').lower() == 'true')\n  if driver_type == 'chrome':\n  chrome_options = webdriver.ChromeOptions()\n  if debug_mode:\n  chrome_options.add_argument('--headless')\n  chrome_options.add_argument('--disable-gpu')\n  chrome_options.add_argument('--no-sandbox')\n  chrome_options.add_argument('--allow-insecure-localhost')\n  driver = webdriver.Remote(\n  'http://127.0.0.1:4444/wd/hub',\n  desired_capabilities=chrome_options.to_capabilities())\n  elif driver_type == 'firefox':\n  firefox_options = webdriver.firefox.options.Options()\n  if debug_mode:\n  firefox_options.add_argument('--headless')\n  driver = webdriver.Remote(\n  'http://127.0.0.1:4444/wd/hub',\n  desired_capabilities=firefox_options.to_capabilities())\n  else:\n  raise ValueError(\n  f'Unsupported SELENIUM_DRIVER provided: {driver_type}')\n  driver.get(get_qpc_url())\n  driver.maximize_window()\n  yield Browser(driver)\n  driver.close()", "target": 1, "info": "Null", "idx": 0}
{"func": "def _run_on_data(self, combined_data, data_start_indicator, data_stop_indicator):\n  st, et = mu.get_st_et(combined_data, self.meta['pid'], self.session_file, st_col=0, et_col=0)\n  ws = self.ws\n  ss = self.ss\n  subwins = self.subwins\n  if self.verbose:\n  print('Session start time: ' + str(st))\n  print('Session stop time: ' + str(et))\n  sr = mu._sampling_rate(combined_data)\n  features = [\n  lambda x: mnf.accelerometer_orientation_features(x, subwins=subwins)\n  ]\n  feature_names = [\n  \"MEDIAN_X_ANGLE\",\n  \"MEDIAN_Y_ANGLE\",\n  \"MEDIAN_Z_ANGLE\",\n  \"RANGE_X_ANGLE\",\n  \"RANGE_Y_ANGLE\",\n  \"RANGE_Z_ANGLE\"\n  ]\n   windows = mw.get_sliding_window_boundaries(start_time=st, stop_time=et, window_duration=ws, step_size=ss)\n  chunk_windows_mask = (windows[:,0] >= data_start_indicator) & (windows[:,0] < data_stop_indicator)\n  chunk_windows = windows[chunk_windows_mask,:]\n  if len(chunk_windows) == 0:\n  return pd.DataFrame()\n  result_data = mw.apply_to_sliding_windows(df=combined_data, sliding_windows=chunk_windows, window_operations=features, operation_names=feature_names, return_dataframe=True)\n  return result_data", "target": 0, "info": "Null", "idx": 0}
{"func": "def _run_on_data(self, combined_data, data_start_indicator, data_stop_indicator):\n  st, et = mu.get_st_et(combined_data, self.meta['pid'], self.session_file, st_col=0, et_col=0)\n  ws = self.ws\n  ss = self.ss\n  subwins = self.subwins\n  if self.verbose:\n  print('Session start time: ' + str(st))\n  print('Session stop time: ' + str(et))\n  sr = mu._sampling_rate(combined_data)\n  features = [\n  lambda x: mnf.accelerometer_orientation_features(x, subwins=subwins)\n  ]\n  feature_names = [\n  \"MEDIAN_X_ANGLE\",\n  \"MEDIAN_Y_ANGLE\",\n  \"MEDIAN_Z_ANGLE\",\n  \"RANGE_X_ANGLE\",\n  \"RANGE_Y_ANGLE\",\n  \"RANGE_Z_ANGLE\"\n  ]\n   windows = mw.get_sliding_window_boundaries(start_time=st, stop_time=et, window_duration=ws, step_size=ss)\n  chunk_windows_mask = (windows[:,0] >= data_start_indicator) & (windows[:,0] <= data_stop_indicator)\n  chunk_windows = windows[chunk_windows_mask,:]\n  if len(chunk_windows) == 0:\n  return pd.DataFrame()\n  result_data = mw.apply_to_sliding_windows(df=combined_data, sliding_windows=chunk_windows, window_operations=features, operation_names=feature_names, return_dataframe=True)\n  return result_data", "target": 1, "info": "Null", "idx": 0}
{"func": "def find_lead_transition(data: np.ndarray, center: float, scan_range: float, npoints: int, width: float = .2e-3) -> float:\n  if len(data.shape) == 2:\n  y = np.mean(data, 0)\n  elif len(data.shape) == 1:\n  y = data\n  else:\n  print('data must be a one or two dimensional array!')\n  return np.nan\n  x = np.linspace(center - scan_range, center + scan_range, npoints)\n  n = int(width/scan_range*npoints)\n  for i in range(0, len(y)-n-1):\n  y[i] -= y[i+n]\n  y_red = y[0:len(y) - n - 1]\n  x_red = x[0:len(y) - n - 1]\n  y_red = np.absolute(y_red)\n  max_index = int(np.argmax(y_red) + int(round(n / 2)))\n   return x[max_index]", "target": 0, "info": "Null", "idx": 0}
{"func": "def find_lead_transition(data: np.ndarray, center: float, scan_range: float, npoints: int, width: float = .2e-3) -> float:\n  if len(data.shape) == 2:\n  y = np.mean(data, 0)\n  elif len(data.shape) == 1:\n  y = data\n  else:\n  print('data must be a one or two dimensional array!')\n  return np.nan\n  x = np.linspace(center - scan_range, center + scan_range, npoints)\n  n = int(width/scan_range*npoints)\n  for i in range(0, len(y)-n-1):\n  y[i] -= y[i+n]\n  y_red = y[0:len(y) - n - 1]\n  x_red = x[0:len(y) - n - 1]\n  y_red = np.absolute(y_red)\n  max_index = int(np.argmax(y_red) + int(round(n / 2)))\n   return x_red[max_index]", "target": 1, "info": "Null", "idx": 0}
{"func": "def test_evaluate_is_tuned(self, mocked_evaluator, mocked_solver):\n  evaluators = []\n  n_evaluator = 2\n  target = make_target(desired=pd.Series(index=[\"parameter_\" + str(i) for i in range(n_evaluator)],\n data=[i + 1 for i in range(n_evaluator)]),\n   tolerance=pd.Series(index=[\"parameter_\" + str(i) for i in range(n_evaluator)],\n   data=.56))\n  for i in range(n_evaluator):\n  evaluator = mocked_evaluator()\n  evaluator.evaluate = MagicMock(return_value=(pd.Series(index=[\"parameter_\" + str(i)], data=10 * i),\n   pd.Series(index=[\"parameter_\" + str(i)], data=i)))\n  evaluator.parameters = (\"parameter_\" + str(i), )\n  evaluators.append(evaluator)\n  solver = mocked_solver()\n  solver.target = target\n  subset_tuner_args = dict(evaluators=evaluators,\n   gates=[\"a\", \"b\", \"c\"],\n   solver=solver)\n  subset_tuner = SubsetTuner(**subset_tuner_args)\n  solver_voltages = pd.Series(index=subset_tuner_args['gates'], data=[5, 10, 30.])\n  full_voltages = solver_voltages.copy(deep=True)\n  assert(isinstance(full_voltages, pd.Series))\n  full_voltages[\"d\"] = 1.\n  failing_is_tuned = subset_tuner.is_tuned(voltages=full_voltages)\n  self.assertEqual(solver.update_after_step.call_count, 1)\n  pd.testing.assert_series_equal(full_voltages, solver.update_after_step.call_args[0][0])\n   parameter = pd.Series(data=[10 * i for i in range(n_evaluator)],\n    index=[\"parameter_\" + str(i) for i in range(n_evaluator)])\n  variances = pd.Series(data=[i for i in range(n_evaluator)],\n    index=[\"parameter_\" + str(i) for i in range(n_evaluator)])\n  pd.testing.assert_series_equal(solver.update_after_step.call_args[0][1], parameter)\n  pd.testing.assert_series_equal(solver.update_after_step.call_args[0][2], variances)\n  pd.testing.assert_series_equal(subset_tuner._last_voltage, full_voltages)\n  pd.testing.assert_series_equal(subset_tuner.last_parameter_covariance[0], parameter)\n  pd.testing.assert_series_equal(subset_tuner.last_parameter_covariance[1], variances)\n  self.assertEqual(failing_is_tuned, False)\n  subset_tuner.evaluate = unittest.mock.Mock(return_value=(target.desired, variances))\n  passing_is_tuned = subset_tuner.is_tuned(voltages=full_voltages)\n  self.assertEqual(passing_is_tuned, True)", "target": 0, "info": "Null", "idx": 0}
{"func": "def test_evaluate_is_tuned(self, mocked_evaluator, mocked_solver):\n  evaluators = []\n  n_evaluator = 2\n  target = make_target(desired=pd.Series(index=[\"parameter_\" + str(i) for i in range(n_evaluator)],\n data=[i + 1 for i in range(n_evaluator)]),\n   tolerance=pd.Series(index=[\"parameter_\" + str(i) for i in range(n_evaluator)],\n   data=.56))\n  for i in range(n_evaluator):\n  evaluator = mocked_evaluator()\n  evaluator.evaluate = MagicMock(return_value=(pd.Series(index=[\"parameter_\" + str(i)], data=10 * i),\n   pd.Series(index=[\"parameter_\" + str(i)], data=i)))\n  evaluator.parameters = (\"parameter_\" + str(i), )\n  evaluators.append(evaluator)\n  solver = mocked_solver()\n  solver.target = target\n  subset_tuner_args = dict(evaluators=evaluators,\n   gates=[\"a\", \"b\", \"c\"],\n   solver=solver)\n  subset_tuner = SubsetTuner(**subset_tuner_args)\n  solver_voltages = pd.Series(index=subset_tuner_args['gates'], data=[5, 10, 30.])\n  full_voltages = solver_voltages.copy(deep=True)\n  assert(isinstance(full_voltages, pd.Series))\n  full_voltages[\"d\"] = 1.\n  failing_is_tuned = subset_tuner.is_tuned(voltages=full_voltages)\n  self.assertEqual(solver.update_after_step.call_count, 1)\n  pd.testing.assert_series_equal(solver_voltages, solver.update_after_step.call_args[0][0])\n   parameter = pd.Series(data=[10 * i for i in range(n_evaluator)],\n    index=[\"parameter_\" + str(i) for i in range(n_evaluator)])\n  variances = pd.Series(data=[i for i in range(n_evaluator)],\n    index=[\"parameter_\" + str(i) for i in range(n_evaluator)])\n  pd.testing.assert_series_equal(solver.update_after_step.call_args[0][1], parameter)\n  pd.testing.assert_series_equal(solver.update_after_step.call_args[0][2], variances)\n  pd.testing.assert_series_equal(subset_tuner._last_voltage, full_voltages)\n  pd.testing.assert_series_equal(subset_tuner.last_parameter_covariance[0], parameter)\n  pd.testing.assert_series_equal(subset_tuner.last_parameter_covariance[1], variances)\n  self.assertEqual(failing_is_tuned, False)\n  subset_tuner.evaluate = unittest.mock.Mock(return_value=(target.desired, variances))\n  passing_is_tuned = subset_tuner.is_tuned(voltages=full_voltages)\n  self.assertEqual(passing_is_tuned, True)", "target": 1, "info": "Null", "idx": 0}
{"func": "def get_new_positions(fixed_positions, layout, n_plots):\n  n_rows_min = (1 if len(fixed_positions)==0 else max(r+1 for r, _ in fixed_positions.values()))\n  n_cols_min = (1 if len(fixed_positions)==0 else max(c+1 for _, c in fixed_positions.values()))\n  n_rows = n_rows_min\n  n_cols = max(ceildiv(n_plots, n_rows), n_rows_min)\n  n_cols = n_cols_min\n  n_rows = max(ceildiv(n_plots, n_cols), n_cols_min)\n  n_rows_, n_cols_ = vector_length_to_tile_dims(n_plots)\n  n_rows, n_cols = max(n_rows_min, n_rows_), max(n_cols_min, n_cols_)\n  raise NotImplementedError(layout)\n  n_rows, n_cols = n_rows_min, n_cols_min\n  while n_plots>n_rows*n_cols:\n  if layout in ('g', 'grid'):\n  if n_cols>n_rows:\n  n_rows+=1\n  else:\n  n_cols+=1\n  elif layout in ('h', 'horizontal'):\n  n_cols+=1\n  elif layout in ('v', 'vertical'):\n  n_rows +=1\n  else:\n  raise NotImplementedError(layout)\n  positions = []\n  ix = 0\n  taken_positions = set(fixed_positions.values())\n  for i in range(n_plots):\n  if i in fixed_positions:\n  positions.append(fixed_positions[i])\n  else:\n  while True:\n  row, col = ix//n_cols, ix%n_cols\n  if (row, col) not in taken_positions:\n  positions.append((row, col))\n  taken_positions.add((row, col))\n  i+=1\n  break\n  ix+=1\n  return positions, (n_rows, n_cols)", "target": 0, "info": "Null", "idx": 0}
{"func": "def get_new_positions(fixed_positions, layout, n_plots):\n  n_rows_min = (1 if len(fixed_positions)==0 else max(r+1 for r, _ in fixed_positions.values()))\n  n_cols_min = (1 if len(fixed_positions)==0 else max(c+1 for _, c in fixed_positions.values()))\n  n_rows = n_rows_min\n  n_cols = max(ceildiv(n_plots, n_rows), n_rows_min)\n  n_cols = n_cols_min\n  n_rows = max(ceildiv(n_plots, n_cols), n_cols_min)\n  n_rows_, n_cols_ = vector_length_to_tile_dims(n_plots)\n  n_rows, n_cols = max(n_rows_min, n_rows_), max(n_cols_min, n_cols_)\n  raise NotImplementedError(layout)\n  n_rows, n_cols = n_rows_min, n_cols_min\n  while n_plots>n_rows*n_cols:\n  if layout in ('g', 'grid'):\n  if n_cols>n_rows:\n  n_rows+=1\n  else:\n  n_cols+=1\n  elif layout in ('h', 'horizontal'):\n  n_cols+=1\n  elif layout in ('v', 'vertical'):\n  n_rows +=1\n  else:\n  raise NotImplementedError(layout)\n  positions = []\n  ix = 0\n  taken_positions = set(fixed_positions.values())\n  for i in range(n_plots):\n  if i in fixed_positions:\n  positions.append(fixed_positions[i])\n  else:\n  while True:\n  row, col = ix/n_cols, ix%n_cols\n  if (row, col) not in taken_positions:\n  positions.append((row, col))\n  taken_positions.add((row, col))\n  i+=1\n  break\n  ix+=1\n  return positions, (n_rows, n_cols)", "target": 1, "info": "Null", "idx": 0}
{"func": "def test_findall(self):\n  mgr = self.manager\n  o1 = fakes.FakeEntity()\n  o1.some_att = \"ok\"\n  o2 = fakes.FakeEntity()\n  o2.some_att = \"bad\"\n  o3 = fakes.FakeEntity()\n  o3.some_att = \"ok\"\n  sav = mgr.list\n  mgr.list = Mock(return_value=[o1, o2, o3])\n  ret = mgr.findall(some_att=\"ok\")\n  self.assertTrue(o1 in ret)\n  self.assertFalse(o2 in ret)\n  self.assertTrue(o3 in ret)\n  mgr.list = sav", "target": 0, "info": "Null", "idx": 0}
{"func": "def test_findall(self):\n  mgr = self.manager\n  o1 = fakes.FakeEntity()\n  o1.some_att = \"ok\"\n  o2 = fakes.FakeEntity()\n  o2.some_att = \"bad\"\n  o3 = fakes.FakeEntity()\n  o3.some_att = \"ok\"\n  sav = mgr.list\n  mgr.list = Mock(return_value=[o1, o2, o3])\n  ret = mgr.findall(some_att=\"ok\")\n  self.assertTrue(o1 in ret)\n  self.assertFalse(o2 in ret)\n  self.assertTrue(o1 in ret)\n  mgr.list = sav", "target": 1, "info": "Null", "idx": 0}
{"func": "def send(self, notification):\n  if isinstance(self.app_id, str):\n  app_id_obj = {\"app_id\": self.app_id}\n  elif isinstance(self.app_id, list):\n  app_id_obj = {\"app_ids\": self.app_id}\n  data = merge_dicts(\n  notification.get_data(),\n  app_id_obj\n  )\n  response = self.request(\"post\", \"notifications\", json=data)\n   notification.id = response[\"id\"]\n   return notification", "target": 0, "info": "Null", "idx": 0}
{"func": "def send(self, notification):\n  if isinstance(self.app_id, str):\n  app_id_obj = {\"app_id\": self.app_id}\n  elif isinstance(self.app_id, list):\n  app_id_obj = {\"app_ids\": self.app_id}\n  data = merge_dicts(\n  notification.get_data(),\n  app_id_obj\n  )\n  response = self.request(\"post\", \"notifications\", json=data)\n   notification.id = response[\"id\"]\n   return response", "target": 1, "info": "Null", "idx": 0}
{"func": "def partial_pipeline_data(strategy, user, *args, **kwargs):\n  partial = strategy.session_get('partial_pipeline', None)\n  if partial:\n  idx, backend, xargs, xkwargs = strategy.partial_from_session(partial)\n  kwargs = kwargs.copy()\n  kwargs.setdefault('user', user)\n  kwargs.setdefault('request', strategy.request)\n  kwargs.update(xkwargs)\n  return idx, backend, xargs, kwargs", "target": 0, "info": "Null", "idx": 0}
{"func": "def partial_pipeline_data(strategy, user, *args, **kwargs):\n  partial = strategy.session_get('partial_pipeline', None)\n  if partial:\n  idx, backend, xargs, xkwargs = strategy.partial_from_session(partial)\n  kwargs = kwargs.copy()\n  kwargs.setdefault('user', user)\n  kwargs.setdefault('request', strategy.request)\n  kwargs.update(xkwargs)\n  return idx, backend, xargs, xkwargs", "target": 1, "info": "Null", "idx": 0}
{"func": "def sanitize_redirect(host, redirect_to):\n  if not redirect_to or \\\n not isinstance(redirect_to, six.string_types) or \\\n getattr(redirect_to, 'decode', None) and \\\n not isinstance(redirect_to.decode(), six.string_types):\n  return None\n  netloc = urlparse(redirect_to)[1]\n  if netloc and netloc != host:\n  return None\n  return redirect_to", "target": 0, "info": "Null", "idx": 0}
{"func": "def sanitize_redirect(host, redirect_to):\n  if not redirect_to or \\\n not isinstance(redirect_to, six.string_types) and \\\n getattr(redirect_to, 'decode', None) and \\\n not isinstance(redirect_to.decode(), six.string_types):\n  return None\n  netloc = urlparse(redirect_to)[1]\n  if netloc and netloc != host:\n  return None\n  return redirect_to", "target": 1, "info": "Null", "idx": 0}
{"func": "def process_exception(self, request, exception):\n  strategy = getattr(request, 'social_strategy', None)\n  if strategy is None or self.raise_exception(request, exception):\n  return\n   if isinstance(exception, SocialAuthBaseException):\n  backend_name = request.backend.name\n  message = self.get_message(request, exception)\n  url = self.get_redirect_uri(request, exception)\n  try:\n  messages.error(request, message,\n extra_tags='social-auth ' + backend_name)\n  except MessageFailure:\n  url += ('?' in url and '&' or '?') + \\\n 'message={0}&backend={1}'.format(urlquote(message),\n  backend_name)\n  return redirect(url)", "target": 0, "info": "Null", "idx": 0}
{"func": "def process_exception(self, request, exception):\n  strategy = getattr(request, 'social_strategy', None)\n  if strategy is None or self.raise_exception(request, exception):\n  return\n   if isinstance(exception, SocialAuthBaseException):\n  backend_name = strategy.backend.name\n  message = self.get_message(request, exception)\n  url = self.get_redirect_uri(request, exception)\n  try:\n  messages.error(request, message,\n extra_tags='social-auth ' + backend_name)\n  except MessageFailure:\n  url += ('?' in url and '&' or '?') + \\\n 'message={0}&backend={1}'.format(urlquote(message),\n  backend_name)\n  return redirect(url)", "target": 1, "info": "Null", "idx": 0}
{"func": "  def has(self, blockname, ptype):\n  return self.reader.has_block(self, blockname, ptype)", "target": 0, "info": "Null", "idx": 0}
{"func": " def has(self, blockname, ptype):\n  return self.reader.has_block(self, ptype, blockname)", "target": 1, "info": "Null", "idx": 0}
{"func": "def bootstrap(nside, rand, nbar, *data):\n  def split(data, indices, axis):\n  s = []\n  s.append(slice(0, indices[0]))\n  for i in range(len(indices) - 1):\n  s.append(slice(indices[i], indices[i+1]))\n  s.append(slice(indices[-1], None))\n   rt = []\n  for ss in s:\n  ind = [slice(None, None, None) for i in range(len(data.shape))]\n  ind[axis] = ss\n  ind = tuple(ind)\n  rt.append(data[ind])\n  return rt\n  def hpsplit(nside, data):\n  RA, DEC = data\n  pix = radec2pix(nside, RA, DEC)\n  n = numpy.bincount(pix)\n  a = numpy.argsort(pix)\n  data = numpy.array(data)[:, a]\n  rt = split(data, n.cumsum(), axis=-1)\n  return rt\n  Abar =  41252.96 / nside2npix(nside)\n  rand = hpsplit(nside, rand)\n  if len(data) > 0:\n  data = list(zip(*[hpsplit(nside, d1) for d1 in data]))\n  else:\n  data = [[] for i in range(len(rand))]\n  heap = []\n  j = 0\n  for r, d in zip(rand, data):\n  if len(r[0]) == 0: continue\n  a = 1.0 * len(r[0]) / nbar\n  j = j + 1\n  if len(heap) == 0:\n  heapq.heappush(heap, (a, j, r, d))\n  else:\n  a0, j0, r0, d0 = heapq.heappop(heap)\n  if a0 + a < Abar:\n  a0 += a\n  d0 = [\n   numpy.concatenate((d0[i], d[i]), axis=-1)\n   for i in range(len(d))\n  ]\n  r0 = numpy.concatenate((r0, r), axis=-1)\n  else:\n  heapq.heappush(heap, (a, j, r, d))\n  heapq.heappush(heap, (a0, j0, r0, d0))\n   for i in range(len(heap)):\n  area, j, r, d = heapq.heappop(heap)\n  rt = [area, r] + d\n  yield rt", "target": 0, "info": "Null", "idx": 0}
{"func": "def bootstrap(nside, rand, nbar, *data):\n  def split(data, indices, axis):\n  s = []\n  s.append(slice(0, indices[0]))\n  for i in range(len(indices) - 1):\n  s.append(slice(indices[i], indices[i+1]))\n  s.append(slice(indices[-1], None))\n   rt = []\n  for ss in s:\n  ind = [slice(None, None, None) for i in range(len(data.shape))]\n  ind[axis] = ss\n  ind = tuple(ind)\n  rt.append(data[ind])\n  return rt\n  def hpsplit(nside, data):\n  RA, DEC = data\n  pix = radec2pix(nside, RA, DEC)\n  n = numpy.bincount(pix)\n  a = numpy.argsort(pix)\n  data = numpy.array(data)[:, a]\n  rt = split(data, n.cumsum(), axis=-1)\n  return rt\n  Abar =  41252.96 / nside2npix(nside)\n  rand = hpsplit(nside, rand)\n  if len(data) > 0:\n  data = list(zip(*[hpsplit(nside, d1) for d1 in data]))\n  else:\n  data = [[] for i in range(len(rand))]\n  heap = []\n  j = 0\n  for r, d in zip(rand, data):\n  if len(r[0]) == 0: continue\n  a = 1.0 * len(r[0]) / nbar\n  j = j + 1\n  if len(heap) == 0:\n  heapq.heappush(heap, (a, j, r, d))\n  else:\n  a0, j0, r0, d0 = heapq.heappop(heap)\n  if a0 + a < Abar:\n  a0 += a\n  d0 = [\n   numpy.concatenate((d0[i], d[i]), axis=-1)\n   for i in range(len(d))\n  ]\n  r0 = numpy.concatenate((r0, r), axis=-1)\n  else:\n  heapq.heappush(heap, (a, j, r, d))\n  heapq.heappush(heap, (a0, j, r0, d0))\n   for i in range(len(heap)):\n  area, j, r, d = heapq.heappop(heap)\n  rt = [area, r] + d\n  yield rt", "target": 1, "info": "Null", "idx": 0}
{"func": "def consume(self, msg):\n  body, topic = msg.get('body'), msg.get('topic')\n  pretty_text = fedmsg.text.msg2repr(body)\n  log.debug(pretty_text)\n  icon = fedmsg.text._msg2icon(msg) or ''\n  log.debug(\"icon = %s\" % icon)\n  if icon:\n  icon_file = self._icon_cache.get(icon)\n  if not icon_file:\n  icon_file, headers = urllib.urlretrieve(icon)\n  log.debug('Downloaded %s to %s' % (icon.split('/')[-1],\n icon_file))\n  self._icon_cache[icon] = icon_file\n  icon = icon_file\n  note = Notify.Notification.new(\"fedmsg\", pretty_text, icon)\n  note.show()", "target": 0, "info": "Null", "idx": 0}
{"func": "def consume(self, msg):\n  body, topic = msg.get('body'), msg.get('topic')\n  pretty_text = fedmsg.text.msg2repr(body)\n  log.debug(pretty_text)\n  icon = fedmsg.text._msg2icon(msg) or ''\n  log.debug(\"icon = %s\" % icon)\n  if icon:\n  icon_file = self._icon_cache.get(icon)\n  if not icon_file:\n  icon_file, headers = urllib.urlretrieve(icon)\n  log.debug('Downloaded %s to %s' % (icon.split('/')[-1],\n icon_file))\n  self._icon_cache[icon] = icon_file\n  icon = icon_file\n  note = Notify.Notification.new(\"fedmsg\", pretty_text, icon_file)\n  note.show()", "target": 1, "info": "Null", "idx": 0}
{"func": "def verify(self, certificate):\n  to_check_asn1cert = certificate.to_asn1crypto\n  all_certs = {to_check_asn1cert: certificate}\n  intermediates, trust_roots = [], []\n  for store in self.stores:\n  for cert in store:\n  asn1cert = cert.to_asn1crypto\n  (trust_roots if store.trusted else intermediates).append(asn1cert)\n  all_certs[asn1cert] = cert\n  context = ValidationContext(trust_roots=list(trust_roots),\n  moment=self.timestamp,\n  weak_hash_algos=set() if self.allow_legacy else None)\n  validator = CertificateValidator(end_entity_cert=to_check_asn1cert,\n   intermediate_certs=list(intermediates),\n   validation_context=context)\n  try:\n  chain = validator.validate_usage(key_usage=set(self.key_usages) if self.key_usages else set(),\n   extended_key_usage=set(self.extended_key_usages)\n  if self.extended_key_usages else set(),\n   extended_optional=self.optional_eku)\n  except Exception as e:\n  raise VerificationError(\"Chain verification from %s failed: %s\" % (certificate, e))\n  else:\n  return [all_certs[x] for x in chain]", "target": 0, "info": "Null", "idx": 0}
{"func": " def verify(self, certificate):\n  to_check_asn1cert = certificate.to_asn1crypto\n  all_certs = {to_check_asn1cert: certificate}\n  intermediates, trust_roots = [], []\n  for store in self.stores:\n  for cert in store:\n  asn1cert = certificate.to_asn1crypto\n  (trust_roots if store.trusted else intermediates).append(asn1cert)\n  all_certs[asn1cert] = cert\n  context = ValidationContext(trust_roots=list(trust_roots),\n  moment=self.timestamp,\n  weak_hash_algos=set() if self.allow_legacy else None)\n  validator = CertificateValidator(end_entity_cert=to_check_asn1cert,\n   intermediate_certs=list(intermediates),\n   validation_context=context)\n  try:\n  chain = validator.validate_usage(key_usage=set(self.key_usages) if self.key_usages else set(),\n   extended_key_usage=set(self.extended_key_usages)\n  if self.extended_key_usages else set(),\n   extended_optional=self.optional_eku)\n  except Exception as e:\n  raise VerificationError(\"Chain verification from %s failed: %s\" % (certificate, e))\n  else:\n  return [all_certs[x] for x in chain]", "target": 1, "info": "Null", "idx": 0}
{"func": "def chunks(l, n):\n  big_list = []\n  n = max(1, n)\n  step = int(len(l) / n)\n  for i in range(0, len(l), step):\n  big_list.append(l[i:i+step])\n  return big_list", "target": 0, "info": "Null", "idx": 0}
{"func": "def chunks(l, n):\n  big_list = []\n  n = max(1, n)\n  step = int(len(l) / n)\n  for i in range(0, len(l), step):\n  big_list.append(l[i:i+n])\n  return big_list", "target": 1, "info": "Null", "idx": 0}
{"func": "def _gen_asset_class(sym):\n  sym_class = str(sym).split(\"_\")\n  if len(sym_class) > 1:\n  return sym_class[1]\n  return \"STK\"", "target": 0, "info": "Null", "idx": 0}
{"func": "def _gen_asset_class(sym):\n  sym_class = str(sym).split(\"_\")\n  if len(sym_class) > 0:\n  return sym_class[1]\n  return \"STK\"", "target": 1, "info": "Null", "idx": 0}
{"func": "def make_id(name):\n  r = get_rand_string(12)\n  if len(name) > 22:\n  name = name[:22]\n  return name + \"-\" + r", "target": 0, "info": "Null", "idx": 0}
{"func": "def make_id(name):\n  r = get_rand_string(12)\n  if len(name) <= 22:\n  name = name[:22]\n  return name + \"-\" + r", "target": 1, "info": "Null", "idx": 0}
{"func": "def int_to_decimal_str(integer):\n  int_string = str(integer)\n  if len(int_string) <= 2:\n  return \"0.\" + int_string.zfill(2)\n  else:\n  return int_string[:-2] + \".\" + int_string[-2:]", "target": 0, "info": "Null", "idx": 0}
{"func": "def int_to_decimal_str(integer):\n  int_string = str(integer)\n  if len(int_string) < 2:\n  return \"0.\" + int_string.zfill(2)\n  else:\n  return int_string[:-2] + \".\" + int_string[-2:]", "target": 1, "info": "Null", "idx": 0}
{"func": "def add_metric(tensor, name=None):   return tf.add_to_collection(\n    METRICS,\n    tensor if name is None else util.rename(tensor, name))", "target": 0, "info": "Null", "idx": 0}
{"func": "def add_metric(tensor, name=None):   return tf.add_to_collection(\n    METRICS,\n    tensor if name is None else util.rename(name, tensor))", "target": 1, "info": "Null", "idx": 0}
{"func": "def classify(logits,\n   true_label=None,\n   *,\n   num_classes,\n   num_labels=None,\n   key=None,\n   predictions={},\n   regularization_scale=1e-8):\n  assert isinstance(predictions, dict)\n  if num_labels is None:\n  assert true_label is not None\n  num_labels = _calc_num_labels(true_label)\n  assert static_rank(logits) in {1, 2}\n  if true_label is not None:\n  assert static_rank(true_label) in {1, 2}\n  assert num_labels == _calc_num_labels(true_label)\n  assert num_classes >= 2\n  assert num_labels >= 1\n  predicted_labels, loss = (\n  (_classify_label\n   if num_labels == 1 else\n   functools.partial(_classify_labels, num_labels=num_labels))\n  (logits, true_label, num_classes=num_classes))\n  predictions['label'] = predicted_labels\n  if key is not None:\n  predictions['key'] = key\n  if true_label is None:\n  return predictions\n  return (predictions,\n  loss + l2_regularization_loss(regularization_scale),\n  train.minimize(loss),\n  _evaluate(predicted_labels, true_label))", "target": 0, "info": "Null", "idx": 0}
{"func": "def classify(logits,\n   true_label=None,\n   *,\n   num_classes,\n   num_labels=None,\n   key=None,\n   predictions={},\n   regularization_scale=1e-8):\n  assert isinstance(predictions, dict)\n  if num_labels is None:\n  assert true_label is not None\n  num_labels = _calc_num_labels(true_label)\n  assert static_rank(logits) in {1, 2}\n  if true_label is not None:\n  assert static_rank(true_label) in {1, 2}\n  assert num_labels == _calc_num_labels(true_label)\n  assert num_classes >= 2\n  assert num_labels >= 1\n  predicted_labels, loss = (\n  (_classify_label\n   if num_labels == 1 else\n   functools.partial(_classify_labels, num_labels=num_labels))\n  (logits, true_label, num_classes=num_classes))\n  predictions['label'] = predicted_labels\n  if key is not None:\n  predictions['key'] = key\n  if true_label is None:\n  return predictions\n  return (predictions,\n  loss + l2_regularization_loss(regularization_scale),\n  train.minimize(loss),\n  _evaluate(predictions, true_label))", "target": 1, "info": "Null", "idx": 0}
{"func": "def reset(self, ignore=[]):\n  desktop = ewmh.getCurrentDesktop()\n  self.ewmhactive = ewmh.getActiveWindow()\n  self.windowInCurrentWorkspaceInStackingOrder = []\n  self.windowName = {}\n  self.windowGeometry = {}\n  self.minGeometry = {}\n  self.windowObjectMap = {}\n  for win, _desktop, name in get_window_list(ignore):\n  winid = win.id\n  self.windowObjectMap[winid] = win\n  geo = win.get_geometry()\n  x, y, w, h = geo.x, geo.y, geo.width, geo.height\n  if not _desktop == desktop:\n  continue\n  if name in EXCLUDE_APPLICATIONS:\n  continue\n   wmclass, minimized = get_wm_class_and_state(win)\n  dock = disp.intern_atom('_NET_WM_WINDOW_TYPE_DOCK')\n  if dock in ewmh.getWmWindowType(win):\n  continue\n  if minimized:\n  continue\n  wmclass = set(wmclass)\n  if not wmclass == wmclass - set(EXCLUDE_WM_CLASS):\n  continue\n  self.windowName[winid] = name\n  if not (0 <= x < workarea.width and 0 <= y < workarea.height):\n  continue\n  wnh = win.get_wm_normal_hints()\n  f_left, f_right, f_top, f_bottom = get_frame_extents(win)\n  minw = max(MIN_WINDOW_WIDTH, wnh.min_width + f_left, f_right)\n  minh = max(MIN_WINDOW_HEIGHT, wnh.min_height + f_top, f_right)\n  self.minGeometry[winid] = minw, minh\n  self.windowGeometry[winid] = [int(x) - f_left, int(y) - f_top,\n    w + f_left + f_right, h + f_top + f_bottom]", "target": 0, "info": "Null", "idx": 0}
{"func": "def reset(self, ignore=[]):\n  desktop = ewmh.getCurrentDesktop()\n  self.ewmhactive = ewmh.getActiveWindow()\n  self.windowInCurrentWorkspaceInStackingOrder = []\n  self.windowName = {}\n  self.windowGeometry = {}\n  self.minGeometry = {}\n  self.windowObjectMap = {}\n  for win, _desktop, name in get_window_list(ignore):\n  winid = win.id\n  self.windowObjectMap[winid] = win\n  geo = win.get_geometry()\n  x, y, w, h = geo.x, geo.y, geo.width, geo.height\n  if not _desktop == desktop:\n  continue\n  if name in EXCLUDE_APPLICATIONS:\n  continue\n   wmclass, minimized = get_wm_class_and_state(winid)\n  dock = disp.intern_atom('_NET_WM_WINDOW_TYPE_DOCK')\n  if dock in ewmh.getWmWindowType(win):\n  continue\n  if minimized:\n  continue\n  wmclass = set(wmclass)\n  if not wmclass == wmclass - set(EXCLUDE_WM_CLASS):\n  continue\n  self.windowName[winid] = name\n  if not (0 <= x < workarea.width and 0 <= y < workarea.height):\n  continue\n  wnh = win.get_wm_normal_hints()\n  f_left, f_right, f_top, f_bottom = get_frame_extents(win)\n  minw = max(MIN_WINDOW_WIDTH, wnh.min_width + f_left, f_right)\n  minh = max(MIN_WINDOW_HEIGHT, wnh.min_height + f_top, f_right)\n  self.minGeometry[winid] = minw, minh\n  self.windowGeometry[winid] = [int(x) - f_left, int(y) - f_top,\n    w + f_left + f_right, h + f_top + f_bottom]\n  self.windowInCurrentWorkspaceInStackingOrder.append(winid)", "target": 1, "info": "Null", "idx": 0}
{"func": " async def get_content(self):\n  content_type = self.headers[\"content-type\"]\n  if not content_type.startswith(CONTENT_TYPE):\n  raise UnexpectedContentType(\n  f\"Unexpected content-type in response: \"\n  f\"{content_type}, expected: {CONTENT_TYPE}, \"\n  f\"probable causes: instance down or REST API disabled\"\n  )\n  body = await self.text()\n  content = ujson.loads(body).get(\"result\")\n   if \"error\" in content:\n  err = ErrorSchema().load(content[\"error\"])\n  text = f\"{err['message']} ({self.status}): {err['detail']}\" if err[\"detail\"] else err[\"message\"]\n  raise ErrorResponse(text)\n  return content", "target": 0, "info": "Null", "idx": 0}
{"func": "async def get_content(self):\n  content_type = self.headers[\"content-type\"]\n  if not content_type.startswith(CONTENT_TYPE):\n  raise UnexpectedContentType(\n  f\"Unexpected content-type in response: \"\n  f\"{content_type}, expected: {CONTENT_TYPE}, \"\n  f\"probable causes: instance down or REST API disabled\"\n  )\n  body = await self.text()\n  content = ujson.loads(body).get(\"result\")\n   if \"error\" in body:\n  err = ErrorSchema().load(content[\"error\"])\n  text = f\"{err['message']} ({self.status}): {err['detail']}\" if err[\"detail\"] else err[\"message\"]\n  raise ErrorResponse(text)\n  return content", "target": 1, "info": "Null", "idx": 0}
{"func": "def _cmp_value_to_nodes(base_op, first, second):\n  node_values = set([number(node) for node in second])\n  first = number(first)\n  verbose_print('Comparing {0} nodes in node set to value \"{1}\"'.format(len(node_values), first))\n   for node_value in node_values:\n  if base_op(first, node_value):\n  verbose_print('Comparison succeeded for value \"{0}\" and node value \"{1}'.format(first, node_value))\n  return True\n  verbose_print('Comparison failed for all nodes in the node set.')\n  return False", "target": 0, "info": "Null", "idx": 0}
{"func": "def _cmp_value_to_nodes(base_op, first, second):\n  node_values = set([number(node) for node in second])\n  first = number(first)\n  verbose_print('Comparing {0} nodes in node set to value \"{1}\"'.format(len(node_values), second))\n   for node_value in node_values:\n  if base_op(first, node_value):\n  verbose_print('Comparison succeeded for value \"{0}\" and node value \"{1}'.format(first, node_value))\n  return True\n  verbose_print('Comparison failed for all nodes in the node set.')\n  return False", "target": 1, "info": "Null", "idx": 0}
{"func": " def run(self, ctx, project, args):\n  if args.config is None:\n  parent_settings = project.settings\n  else:\n  configs = project.settings[\"configurations\"]\n  parent_settings = configs.get(args.config)\n  if parent_settings is None:\n  raise RuntimeError(\"Configuration {} not found (available configurations: {})\".format(args.config, \", \".join(configs.keys())))\n  docker = parent_settings.get(\"docker\", None)\n  if docker is None:\n  docker_image = None\n  docker_build = None\n  else:\n  docker_image = docker.get(\"image\", None)\n  docker_build = docker.get(\"build\", None)\n  trace_data(args, docker)\n  if args.docker_build:\n  if docker_build is None:\n  raise RuntimeError(\"No build commands configured for Docker base image\")\n  else:\n  with temp_file() as temp_path:\n  make_shell_script(temp_path, [\"cd {}\".format(ctx.project_info.dir)] + docker_build)\n  subprocess.check_call([\"/bin/sh\", temp_path])\n   if args.destroy:\n  docker_image_remove(project.image_id)\n   with temp_dir() as dir:\n  if docker_image is None:\n  raise RuntimeError(\"No Docker base image is configured\")\n  else:\n  uid, group_name, gid, user_name = ctx.user_info()\n  with open(os.path.join(dir, \"Dockerfile\"), \"wt\") as f:\n  f.write(\"FROM {}\\n\".format(docker_image))\n  f.write(\"RUN groupadd -g {} {}\\n\".format(gid, group_name))\n  f.write(\"RUN useradd -l -u {} -g {} {}\\n\".format(uid, gid, user_name))\n  with open(os.path.join(dir, \".dockerignore\"), \"wt\") as f:\n  f.write(\"*\\n\")\n  docker_image_build(project.image_id, dir)", "target": 0, "info": "Null", "idx": 0}
{"func": "def run(self, ctx, project, args):\n  if args.config is None:\n  parent_settings = project.settings\n  else:\n  configs = project.settings[\"configurations\"]\n  parent_settings = configs.get(args.config)\n  if parent_settings is None:\n  raise RuntimeError(\"Configuration {} not found (available configurations: {})\".format(args.config, \", \".join(configs.keys())))\n  docker = parent_settings.get(\"docker\", None)\n  if docker is None:\n  docker_image = None\n  docker_build = None\n  else:\n  docker_image = docker.get(\"image\", None)\n  docker_build = docker.get(\"build\", None)\n  trace_data(args, docker)\n  if args.docker_build:\n  if docker_build is None:\n  raise RuntimeError(\"No build commands configured for Docker base image\")\n  else:\n  with temp_file() as temp_path:\n  make_shell_script(temp_path, [\"cd {}\".format(ctx.project_info.dir)] + docker_build)\n  subprocess.check_call([\"/bin/sh\", temp_path])\n   if args.destroy:\n  docker_image_remove(ctx.image_id)\n   with temp_dir() as dir:\n  if docker_image is None:\n  raise RuntimeError(\"No Docker base image is configured\")\n  else:\n  uid, group_name, gid, user_name = ctx.user_info()\n  with open(os.path.join(dir, \"Dockerfile\"), \"wt\") as f:\n  f.write(\"FROM {}\\n\".format(docker_image))\n  f.write(\"RUN groupadd -g {} {}\\n\".format(gid, group_name))\n  f.write(\"RUN useradd -l -u {} -g {} {}\\n\".format(uid, gid, user_name))\n  with open(os.path.join(dir, \".dockerignore\"), \"wt\") as f:\n  f.write(\"*\\n\")\n  docker_image_build(project.image_id, dir)", "target": 1, "info": "Null", "idx": 0}
{"func": "def get_loaders(self):\n   conf_loaders = self._settings_facade.get(\"routes_loaders\") or []\n   if len(conf_loaders) == 0:\n  raise FantasticoNoRoutesError(\"No loaders configured.\")\n   if self._loader_lock is None and len(self._loaders) == 0:\n  self._loader_lock = threading.Lock()\n  self._loaders = []\n   if self._loader_lock:\n  self._loader_lock.acquire()\n   if len(self._loaders) == 0:\n  for loader_cls in conf_loaders:\n  loader = instantiator.instantiate_class(loader_cls, [self._settings_facade])\n   self._loaders.append(loader)\n   if self._loader_lock is not None:\n  self._loader_lock.release()\n  self._loader_lock = None\n   return self._loaders", "target": 0, "info": "Null", "idx": 0}
{"func": " def get_loaders(self):\n   conf_loaders = self._settings_facade.get(\"routes_loaders\") or []\n   if len(conf_loaders) == 0:\n  raise FantasticoNoRoutesError(\"No loaders configured.\")\n   if self._loader_lock is not None and len(self._loaders) == 0:\n  self._loader_lock = threading.Lock()\n  self._loaders = []\n   if self._loader_lock:\n  self._loader_lock.acquire()\n   if len(self._loaders) == 0:\n  for loader_cls in conf_loaders:\n  loader = instantiator.instantiate_class(loader_cls, [self._settings_facade])\n   self._loaders.append(loader)\n   if self._loader_lock is not None:\n  self._loader_lock.release()\n  self._loader_lock = None\n   return self._loaders", "target": 1, "info": "Null", "idx": 0}
{"func": " def exec(self, os_lib=os):\n  comp_name = self._arguments.name\n  contrib_path = instantiator.get_package_abslocation(contrib)\n  comp_path = \"%s%s\" % (contrib_path, comp_name)\n  if not os_lib.path.exists(comp_path):\n  raise FantasticoSdkCommandNotFoundError(\"Extension %s does not exist.\" % comp_name)\n  file_matcher = lambda abspath, filename: True\n  root_folder = instantiator.get_component_path_data(SettingsFacade().get_config().__class__)[1]\n  comp_root_folder = contrib_path.replace(contrib_path, \"%s%s/%s\" % (root_folder, self._arguments.comp_root, comp_name))\n   if not os_lib.path.exists(comp_root_folder):\n  os_lib.mkdir(comp_root_folder)\n   instantiator.scan_folder_by_criteria(\n  comp_path, file_matcher,\n  action=lambda abspath, filename: \\\n  self._link_files(abspath, filename, contrib_path, root_folder, os_lib),\n  os_lib=os_lib)", "target": 0, "info": "Null", "idx": 0}
{"func": " def exec(self, os_lib=os):\n  comp_name = self._arguments.name\n  contrib_path = instantiator.get_package_abslocation(contrib)\n  comp_path = \"%s%s\" % (contrib_path, comp_name)\n  if not os_lib.path.exists(comp_path):\n  raise FantasticoSdkCommandNotFoundError(\"Extension %s does not exist.\" % comp_name)\n  file_matcher = lambda abspath, filename: True\n  root_folder = instantiator.get_component_path_data(SettingsFacade().get_config().__class__)[1]\n  comp_root_folder = contrib_path.replace(contrib_path, \"%s%s/%s\" % (root_folder, self._arguments.comp_root, comp_name))\n   if os_lib.path.exists(comp_root_folder):\n  os_lib.mkdir(comp_root_folder)\n   instantiator.scan_folder_by_criteria(\n  comp_path, file_matcher,\n  action=lambda abspath, filename: \\\n  self._link_files(abspath, filename, contrib_path, root_folder, os_lib),\n  os_lib=os_lib)", "target": 1, "info": "Null", "idx": 0}
{"func": "def duration_verbose(duration):\n  hours = int(math.floor(duration/3600));\n  minutes = int(math.floor((duration%3600)/60));\n  seconds = int(math.floor(duration%60));\n  res = \"\"\n  first = True\n  if hours != 0:\n  res += _(\"%d hours\") % hours;\n  first = False\n   if minutes != 0:\n  if not first: res += \", \"\n  res += _(\"%d min\") % minutes;\n  first = False\n   if seconds != 0:\n  if not first: res += \", \"\n  res += _(\"%d sec\") % seconds;\n  return res", "target": 0, "info": "Null", "idx": 0}
{"func": "def duration_verbose(duration):\n  hours = int(math.floor(duration/3600));\n  minutes = int(math.floor((duration%3600)/60));\n  seconds = int(math.floor(duration%60));\n  res = \"\"\n  first = True\n  if hours != 0:\n  res += _(\"%d hours\") % hours;\n  first = False\n   if minutes != 0:\n  if not first: res += \", \"\n  res += _(\"%d min\") % hours;\n  first = False\n   if seconds != 0:\n  if not first: res += \", \"\n  res += _(\"%d sec\") % seconds;\n  return res", "target": 1, "info": "Null", "idx": 0}
{"func": "def assume_role(account, role):\n  sts = boto3.client('sts')\n  response = sts.assume_role(RoleArn=f'arn:aws:iam::{account}:role/{role}',\n RoleSessionName=f'{role}-session-{account}')\n  if not response or not response['ResponseMetadata']['HTTPStatusCode'] == 200:\n  raise Exception(f'could not assume {role} in {account}')\n  return boto3.Session(\n  aws_access_key_id=response['Credentials']['AccessKeyId'],\n  aws_secret_access_key=response['Credentials']['SecretAccessKey'],\n  aws_session_token=response['Credentials']['SessionToken'])", "target": 0, "info": "Null", "idx": 0}
{"func": "def assume_role(account, role):\n  sts = boto3.client('sts')\n  response = sts.assume_role(RoleArn=f'arn:aws:iam::{account}:role/{role}',\n RoleSessionName=f'{role}-session-{account}')\n  if not response and not response['ResponseMetadata']['HTTPStatusCode'] == 200:\n  raise Exception(f'could not assume {role} in {account}')\n  return boto3.Session(\n  aws_access_key_id=response['Credentials']['AccessKeyId'],\n  aws_secret_access_key=response['Credentials']['SecretAccessKey'],\n  aws_session_token=response['Credentials']['SessionToken'])", "target": 1, "info": "Null", "idx": 0}
{"func": "def generatePasswordPrompt():\n  lc = getUserInput(\"Include lowercase? (y/n/E)\")\n  uc = getUserInput(\"Incldue Uppercase? (y/n/E)\")\n  dig = getUserInput(\"Include numbers? (y/n/E)\")\n  punc = getUserInput(\"Include special characters? (y/n/E)\")\n  spc = getUserInput(\"Include spaces? (y/n/E)\")\n  if lc=='n' and uc=='n' and dig=='n' and punc=='n' and spc=='n':\n  print(\"No character set chosen\")\n  return \"\"\n   siz = getUserInput(\"Password length\")\n   if not siz=='' or not siz.isdecimal():\n  print(\"not a number\")\n  return \"\"\n   size = int(siz) if siz else 30\n  if size < 5:\n  print(\"Minimum length is 5\")\n  return \"\"\n  charlist = string.ascii_lowercase if not lc == 'n' else ''\n  charlist += string.ascii_uppercase if not uc =='n' else ''\n  charlist += string.digits if not dig == 'n' else ''\n  charlist += string.punctuation if not punc == 'n' else ''\n  charlist += ' ' if not spc == 'n' else ''\n  matched = False\n  while not matched:\n  password=''\n  for _ in range(size):\n  password += ''.join(random.SystemRandom().choice(charlist))\n  matched = True\n  if matched and lc=='y' and not any(char.islower() for char in password):\n  matched = False\n  if matched and (uc == 'y') and not any(char.isupper() for char in password):\n  matched = False\n  if matched and (dig == 'y') and not any(char.isdigit() for char in password):\n  matched = False\n  if matched and (punc == 'y') and not any(char in string.punctuation for char in password):\n  matched= False\n  if matched and (spc == 'y') and not any(char==' ' for char in password):\n  matched = False\n  return password", "target": 0, "info": "Null", "idx": 0}
{"func": "def generatePasswordPrompt():\n  lc = getUserInput(\"Include lowercase? (y/n/E)\")\n  uc = getUserInput(\"Incldue Uppercase? (y/n/E)\")\n  dig = getUserInput(\"Include numbers? (y/n/E)\")\n  punc = getUserInput(\"Include special characters? (y/n/E)\")\n  spc = getUserInput(\"Include spaces? (y/n/E)\")\n  if lc=='n' and uc=='n' and dig=='n' and punc=='n' and spc=='n':\n  print(\"No character set chosen\")\n  return \"\"\n   siz = getUserInput(\"Password length\")\n   if not siz=='' and not siz.isdecimal():\n  print(\"not a number\")\n  return \"\"\n   size = int(siz) if siz else 30\n  if size < 5:\n  print(\"Minimum length is 5\")\n  return \"\"\n  charlist = string.ascii_lowercase if not lc == 'n' else ''\n  charlist += string.ascii_uppercase if not uc =='n' else ''\n  charlist += string.digits if not dig == 'n' else ''\n  charlist += string.punctuation if not punc == 'n' else ''\n  charlist += ' ' if not spc == 'n' else ''\n  matched = False\n  while not matched:\n  password=''\n  for _ in range(size):\n  password += ''.join(random.SystemRandom().choice(charlist))\n  matched = True\n  if matched and lc=='y' and not any(char.islower() for char in password):\n  matched = False\n  if matched and (uc == 'y') and not any(char.isupper() for char in password):\n  matched = False\n  if matched and (dig == 'y') and not any(char.isdigit() for char in password):\n  matched = False\n  if matched and (punc == 'y') and not any(char in string.punctuation for char in password):\n  matched= False\n  if matched and (spc == 'y') and not any(char==' ' for char in password):\n  matched = False\n  return password", "target": 1, "info": "Null", "idx": 0}
{"func": "def primary_opening_today(self):\n  today = date.today()\n  cache_key = self.get_opening_today_cache_key(today)\n  times = cache.get(cache_key)\n  if times is None:\n  opening_times = self.primary_opening_times\n  if opening_times:\n  specific_times = utils.first_true(opening_times, lambda x: x.get('date') == today)\n  times = specific_times or utils.first_true(opening_times, lambda x: x.get('weekday') == today.weekday())\n  cache.set(cache_key, times, 60*60*24)\n  return times", "target": 0, "info": "Null", "idx": 0}
{"func": "def primary_opening_today(self):\n  today = date.today()\n  cache_key = self.get_opening_today_cache_key(today)\n  times = cache.get(cache_key)\n  if times is not None:\n  opening_times = self.primary_opening_times\n  if opening_times:\n  specific_times = utils.first_true(opening_times, lambda x: x.get('date') == today)\n  times = specific_times or utils.first_true(opening_times, lambda x: x.get('weekday') == today.weekday())\n  cache.set(cache_key, times, 60*60*24)\n  return times", "target": 1, "info": "Null", "idx": 0}
{"func": "def upload_forecast(project_name, model_name, timezero_date, forecast_csv_file):\n  zoltar = authenticate()\n  project = [project for project in zoltar.projects if project.name == project_name][0]\n  model = [model for model in project.models if model.name == model_name][0]\n  print('* working with', model)\n  upload_file_job = model.upload_forecast(forecast_csv_file, timezero_date)\n  busy_poll_upload_file_job(upload_file_job)\n  new_forecast_pk = upload_file_job.output_json['forecast_pk']\n  new_forecast = model.forecast_for_pk(new_forecast_pk)\n  print('* new_forecast', new_forecast)\n  model.refresh()", "target": 0, "info": "Null", "idx": 0}
{"func": "def upload_forecast(project_name, model_name, timezero_date, forecast_csv_file):\n  zoltar = authenticate()\n  project = [project for project in zoltar.projects if project.name == project_name][0]\n  model = [model for model in project.models if model.name == model_name][0]\n  print('* working with', model)\n  upload_file_job = model.upload_forecast(timezero_date, forecast_csv_file)\n  busy_poll_upload_file_job(upload_file_job)\n  new_forecast_pk = upload_file_job.output_json['forecast_pk']\n  new_forecast = model.forecast_for_pk(new_forecast_pk)\n  print('* new_forecast', new_forecast)\n  model.refresh()", "target": 1, "info": "Null", "idx": 0}
