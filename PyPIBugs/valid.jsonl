{"func": "def maybe_swap_spatial_dims(ds, namex='x', namey='y'):\n  swaps = {}\n  lx, rx = ds.indexes[namex][[0, -1]]\n  uy, ly = ds.indexes[namey][[0, -1]]\n   if lx > rx:\n  swaps[namex] = slice(None, None, -1)\n  if uy < ly:\n  swaps[namey] = slice(None, None, -1)\n  return ds.isel(**swaps) if swaps else ds", "target": 0, "info": "Null", "idx": 0}
{"func": "def maybe_swap_spatial_dims(ds, namex='x', namey='y'):\n  swaps = {}\n  lx, rx = ds.indexes[namex][[0, -1]]\n  uy, ly = ds.indexes[namex][[0, -1]]\n   if lx > rx:\n  swaps[namex] = slice(None, None, -1)\n  if uy < ly:\n  swaps[namey] = slice(None, None, -1)\n  return ds.isel(**swaps) if swaps else ds", "target": 1, "info": "Null", "idx": 0}
{"func": "def _task_gen(item, **kwds):\n  runner._update_current_test_var(item, \"call\")\n  dag = item._request.getfixturevalue(\"dag\")\n  ihook = getattr(item.ihook, \"pytest_runtest_call\")\n  task_id = _gen_task_id(item)\n  task = PythonOperator(\n  task_id=task_id,\n  python_callable=lambda: ihook(item=item, **kwds),\n  provide_context=True,\n  dag=dag,\n  )\n  dag.set_dependency(\"__pytest_branch\", task_id)\n  return task", "target": 0, "info": "Null", "idx": 0}
{"func": "def _task_gen(item, **kwds):\n  runner._update_current_test_var(item, \"call\")\n  dag = item._request.getfixturevalue(\"dag\")\n  ihook = getattr(item.ihook, \"pytest_runtest_call\")\n  task_id = _gen_task_id(item)\n  task = PythonOperator(\n  task_id=task_id,\n  python_callable=lambda: ihook(item=item, **kwds),\n  provide_context=True,\n  dag=dag,\n  )\n  dag.set_dependency(task_id, \"__pytest_branch\")\n  return task", "target": 1, "info": "Null", "idx": 0}
{"func": "def make_summary_abund_df(df, cags, singletons):\n  summary_df = pd.concat([\n  pd.DataFrame({\n  cag_ix: df.loc[cag].mean()\n  for cag_ix, cag in cags.items()\n  }).T,\n  df.loc[singletons]\n  ])\n   assert summary_df.shape[0] == len(cags) + len(singletons)\n  assert summary_df.shape[1] == df.shape[1]\n  return summary_df", "target": 0, "info": "Null", "idx": 0}
{"func": "def make_summary_abund_df(df, cags, singletons):\n  summary_df = pd.concat([\n  pd.DataFrame({\n  cag_ix: df.loc[cag].mean()\n  for cag_ix, cag in cags.items()\n  }).T,\n  cags.loc[singletons]\n  ])\n   assert summary_df.shape[0] == len(cags) + len(singletons)\n  assert summary_df.shape[1] == df.shape[1]\n  return summary_df", "target": 1, "info": "Null", "idx": 0}
{"func": " def insertShocks(flatData, i, nshk, nexo, shockList, shockPtr, shockVal):\n  if nshk > 0:\n  start = shockPtr.array[i, 0] - 1\n  for j in range(0, nshk):\n  shkInd = start + j\n  if nshk == nexo:\n  varInd=j\n  else:\n  varInd = shockList.array[shkInd, 0] - 1\n  flatData[varInd] = shockVal.array[shkInd, 0]", "target": 0, "info": "Null", "idx": 0}
{"func": "def insertShocks(flatData, i, nshk, nexo, shockList, shockPtr, shockVal):\n  if nshk > 0:\n  start = shockPtr.array[i, 0] - 1\n  for j in range(0, nshk):\n  shkInd = start + j\n  if nshk == nexo:\n  varInd=j\n  else:\n  varInd = shockList.array[j, 0] - 1\n  flatData[varInd] = shockVal.array[shkInd, 0]", "target": 1, "info": "Null", "idx": 0}
{"func": " def generate_custom_filters(filters, query):\n  filters_list = []\n  for tag, settings in ops_settings['column_mappings'].iteritems():\n   if tag in filters:\n   filters_list.append(query.c.key == tag.lower())\n   if settings['filterable'] == 'like':\n  filters_list.append(query.c.value.like('%' + filters[tag].lower() + '%'))\n  elif settings['filterable'] == 'equals':\n  filters_list.append(query.c.value == filters[tag].lower())\n  else:\n  pass\n  return filters_list", "target": 0, "info": "Null", "idx": 0}
{"func": "def generate_custom_filters(filters, query):\n  filters_list = []\n  for tag, settings in ops_settings['column_mappings'].iteritems():\n   if tag in filters:\n   filters.append(query.c.key == tag.lower())\n   if settings['filterable'] == 'like':\n  filters_list.append(query.c.value.like('%' + filters[tag].lower() + '%'))\n  elif settings['filterable'] == 'equals':\n  filters_list.append(query.c.value == filters[tag].lower())\n  else:\n  pass\n  return filters_list", "target": 1, "info": "Null", "idx": 0}
{"func": "def parse_email(msg, include_raw_body=False, include_attachment_data=False):\n  maila = {}\n  header = {}\n  report_struc = {}\n  headers_struc = {}\n  attachements_struc = {}\n  bodys_struc = {}\n  subject = msg.get('subject', '')\n  headers_struc['subject'] = ad(decode_field(subject))\n  if msg.defects:\n  headers_struc['defect'] = []\n  for exception in msg.defects:\n  headers_struc['defect'].append(str(exception))\n  m = email_regex.search(msg.get('from', '').lower())\n  if m:\n  headers_struc['from'] = ad(m.group(1))\n  else:\n  from_ = email.utils.parseaddr(msg.get('from', '').lower())\n  headers_struc['from'] = ad(from_[1])\n  headers_struc['to'] = headeremail2list(msg, 'to')\n  headers_struc['cc'] = headeremail2list(msg, 'cc')\n  if len(headers_struc['cc']) == 0:\n  headers_struc.pop('cc')\n  headers_struc['delivered_to'] = headeremail2list(msg, 'delivered-to')\n  if len(headers_struc['delivered_to']) == 0:\n  headers_struc.pop('delivered_to')\n  if msg.get('date'):\n  msg_date = msg.get('date').replace('.', ':')\n  date_ = email.utils.parsedate_tz(msg_date)\n  if date_ and not date_[9] is None:\n  ts = email.utils.mktime_tz(date_)\n  date_ = datetime.datetime.utcfromtimestamp(ts)\n  else:\n  date_ = email.utils.parsedate(msg_date)\n  if date_:\n  ts = calendar.timegm(date_)\n  date_ = datetime.datetime.utcfromtimestamp(ts)\n  else:\n  date_ = dateutil.parser.parse('1970-01-01 00:00:00 +0000')\n  if date_.tzname() is None:\n  date_ = date_.replace(tzinfo=dateutil.tz.tzutc())\n  headers_struc['date'] = date_\n  else:\n  headers_struc['date'] = dateutil.parser.parse('1970-01-01 00:00:00 +0000')\n  headers_struc['parse_date'] = datetime.datetime.utcnow()\n  headers_struc['received'] = []\n  headers_struc['received_email'] = []\n  headers_struc['received_domain'] = []\n  headers_struc['received_ip'] = []\n  try:\n  for l in msg.get_all('received'):\n  l = re.sub(r'(\\r|\\n|\\s|\\t)+', ' ', l.lower())\n  headers_struc['received'].append(l)\n  for ips in ipv6_regex.findall(l):\n  headers_struc['received_ip'].append(ips.lower())\n  for ips in ipv4_regex.findall(l):\n  if not priv_ip_regex.match(ips):\n  headers_struc['received_ip'].append(ips.lower())\n  for m in recv_dom_regex.findall(l):\n  checks = True\n  if '.' in m:\n  try:\n  test = int(re.sub(r'[.-]', '', m))\n  if not ipv4_regex.match(m) or m == '127.0.0.1':\n  checks = False\n  except ValueError:\n  pass\n  if checks:\n  headers_struc['received_domain'].append(m)\n  m = email_regex.findall(l)\n  if m:\n  headers_struc['received_email'] += m\n  try:\n  f, b = l.split('by')\n  b, undef = b.split('for')\n  except:\n  continue\n  b_d = b_d_regex.search(b)\n  except TypeError:\n  pass\n  headers_struc['received_email'] = list(set(headers_struc['received_email']))\n  headers_struc['received_domain'] = list(set(headers_struc['received_domain']))\n  headers_struc['received_ip'] = list(set(headers_struc['received_ip']))\n  if len(headers_struc['received_email']) == 0:\n  headers_struc.pop('received_email')\n  if len(headers_struc['received_domain']) == 0:\n  headers_struc.pop('received_domain')\n  if len(headers_struc['received_ip']) == 0:\n  headers_struc.pop('received_ip')\n  raw_body = get_raw_body_text(msg)\n  if include_raw_body:\n  bodys_struc['raw_body'] = raw_body\n  bodys = {}\n  multipart = True\n  if len(raw_body) == 1:\n  multipart = False\n  for body_tup in raw_body:\n  bodie = {}\n  encoding, body, body_multhead = body_tup\n  list_observed_urls = []\n  list_observed_email = []\n  list_observed_dom = []\n  list_observed_ip = []\n  if sys.version_info >= (3, 0) and (isinstance(body, bytes) or isinstance(body, bytearray)):\n  body = body.decode('utf-8', 'ignore')\n  if len(body) < 4096:\n  list_observed_urls = get_uri_ondata(body)\n  for match in email_regex.findall(body):\n  list_observed_email.append(match.lower())\n  for match in dom_regex.findall(body):\n  list_observed_dom.append(match.lower())\n  for match in ipv4_regex.findall(body):\n  if not priv_ip_regex.match(match):\n  list_observed_ip.append(match)\n  for match in ipv6_regex.findall(body):\n  if not priv_ip_regex.match(match):\n  list_observed_ip.append(match.lower())\n  else:\n  for scn_pt in findall('://', body):\n  list_observed_urls = get_uri_ondata(body[scn_pt-16:scn_pt+4096]) + list_observed_urls\n  for scn_pt in findall('@', body):\n  for match in email_regex.findall(body[scn_pt-64:scn_pt+255]):\n  list_observed_email.append(match.lower())\n  for scn_pt in findall('.', body):\n  for match in dom_regex.findall(body[scn_pt-253:scn_pt+1004]):\n  list_observed_dom.append(match.lower())\n  for match in ipv4_regex.findall(body)[scn_pt-11:scn_pt+3]:\n  if not priv_ip_regex.match(match):\n  list_observed_ip.append(match)\n  for scn_pt in findall(':', body):\n  for match in ipv6_regex.findall(body[scn_pt-4:scn_pt+35]):\n  if not priv_ip_regex.match(match):\n  list_observed_ip.append(match.lower())\n  if include_raw_body:\n  if list_observed_urls:\n  bodie['uri'] = list(set(list_observed_urls))\n  if list_observed_email:\n  bodie['email'] = list(set(list_observed_email))\n  if list_observed_dom:\n  bodie['domain'] = list(set(list_observed_dom))\n   if list_observed_ip:\n  bodie['ip'] = list(set(list_observed_ip))\n   else:\n  if list_observed_urls:\n  bodie['uri_hash'] = []\n  for uri in list(set(list_observed_urls)):\n  bodie['uri_hash'].append(hashlib.sha256(uri.lower()).hexdigest())\n  if list_observed_email:\n  bodie['email_hash'] = []\n  for emel in list(set(list_observed_email)):\n  bodie['email_hash'].append(hashlib.sha256(emel).hexdigest())\n  if list_observed_dom:\n  bodie['domain_hash'] = []\n  for uri in list(set(list_observed_dom)):\n  bodie['domain_hash'].append(hashlib.sha256(uri.lower()).hexdigest())\n  if list_observed_ip:\n  bodie['ip_hash'] = []\n  for fip in list(set(list_observed_ip)):\n  bodie['ip_hash'].append(hashlib.sha256(fip).hexdigest())\n    a: [toto,titi]\n c: [truc]\n  ch = {}\n  for k, v in body_multhead:\n  k = ad(k.lower()).replace('.', ':')\n  if multipart:\n  if k in ch:\n  ch[k].append(ad(v))\n  else:\n  ch[k] = [ad(v)]\n  else:\n  if k.startswith('content'):\n  k = ad(k.lower()).replace('.', ':')\n  if k in ch:\n  ch[k].append(ad(v))\n  else:\n  ch[k] = [ad(v)]\n  bodie['content_header'] = ch\n  if include_raw_body:\n  bodie['content'] = body\n  val = ch.get('content-type')\n  if val:\n  if type(val) == list:\n  val = val[-1]\n  bodie['content_type'] = val.split(';')[0].strip()\n  try:\n  bodie['hash'] = hashlib.sha256(body).hexdigest()\n  except:\n  bodie['hash'] = hashlib.sha256(body.encode('UTF-8')).hexdigest()\n  uid = str(uuid.uuid1())\n  bodys[uid] = bodie\n  bodys_struc = bodys\n    a: [toto,titi]\n c: [truc]\n  for k, v in msg.items():\n  k = ad(k.lower()).replace('.', ':')\n  if k in header:\n  header[k].append(ad(v))\n  else:\n  header[k] = [ad(v)]\n  headers_struc['header'] = header\n  report_struc['attachment'] = traverse_multipart(msg, 0, include_attachment_data)\n  if len(report_struc['attachment']) == 0:\n  report_struc.pop('attachment')\n  else:\n  newattach = []\n  for attachment in report_struc['attachment']:\n  newattach.append(report_struc['attachment'][attachment])\n  report_struc['attachment'] = newattach\n  newbody = []\n  for body in bodys_struc:\n  newbody.append(bodys_struc[body])\n  report_struc['body'] = newbody\n  report_struc['header'] = headers_struc\n  return report_struc", "target": 0, "info": "Null", "idx": 0}
{"func": "def parse_email(msg, include_raw_body=False, include_attachment_data=False):\n  maila = {}\n  header = {}\n  report_struc = {}\n  headers_struc = {}\n  attachements_struc = {}\n  bodys_struc = {}\n  subject = msg.get('subject', '')\n  headers_struc['subject'] = ad(decode_field(subject))\n  if msg.defects:\n  headers_struc['defect'] = []\n  for exception in msg.defects:\n  headers_struc['defect'].append(str(exception))\n  m = email_regex.search(msg.get('from', '').lower())\n  if m:\n  headers_struc['from'] = ad(m.group(1))\n  else:\n  from_ = email.utils.parseaddr(msg.get('from', '').lower())\n  headers_struc['from'] = ad(from_[1])\n  headers_struc['to'] = headeremail2list(msg, 'to')\n  headers_struc['cc'] = headeremail2list(msg, 'cc')\n  if len(headers_struc['cc']) == 0:\n  headers_struc.pop('cc')\n  headers_struc['delivered_to'] = headeremail2list(msg, 'delivered-to')\n  if len(headers_struc['delivered_to']) == 0:\n  headers_struc.pop('delivered_to')\n  if msg.get('date'):\n  msg_date = msg.get('date').replace('.', ':')\n  date_ = email.utils.parsedate_tz(msg_date)\n  if date_ and not date_[9] is None:\n  ts = email.utils.mktime_tz(date_)\n  date_ = datetime.datetime.utcfromtimestamp(ts)\n  else:\n  date_ = email.utils.parsedate(msg_date)\n  if date_:\n  ts = calendar.timegm(date_)\n  date_ = datetime.datetime.utcfromtimestamp(ts)\n  else:\n  date_ = dateutil.parser.parse('1970-01-01 00:00:00 +0000')\n  if date_.tzname() is None:\n  date_ = date_.replace(tzinfo=dateutil.tz.tzutc())\n  headers_struc['date'] = date_\n  else:\n  headers_struc['date'] = dateutil.parser.parse('1970-01-01 00:00:00 +0000')\n  headers_struc['parse_date'] = datetime.datetime.utcnow()\n  headers_struc['received'] = []\n  headers_struc['received_email'] = []\n  headers_struc['received_domain'] = []\n  headers_struc['received_ip'] = []\n  try:\n  for l in msg.get_all('received'):\n  l = re.sub(r'(\\r|\\n|\\s|\\t)+', ' ', l.lower())\n  headers_struc['received'].append(l)\n  for ips in ipv6_regex.findall(l):\n  headers_struc['received_ip'].append(ips.lower())\n  for ips in ipv4_regex.findall(l):\n  if not priv_ip_regex.match(ips):\n  headers_struc['received_ip'].append(ips.lower())\n  for m in recv_dom_regex.findall(l):\n  checks = True\n  if '.' in m:\n  try:\n  test = int(re.sub(r'[.-]', '', m))\n  if not ipv4_regex.match(m) or m == '127.0.0.1':\n  checks = False\n  except ValueError:\n  pass\n  if checks:\n  headers_struc['received_domain'].append(m)\n  m = email_regex.findall(l)\n  if m:\n  headers_struc['received_email'] += m\n  try:\n  f, b = l.split('by')\n  b, undef = b.split('for')\n  except:\n  continue\n  b_d = b_d_regex.search(b)\n  except TypeError:\n  pass\n  headers_struc['received_email'] = list(set(headers_struc['received_email']))\n  headers_struc['received_domain'] = list(set(headers_struc['received_domain']))\n  headers_struc['received_ip'] = list(set(headers_struc['received_ip']))\n  if len(headers_struc['received_email']) == 0:\n  headers_struc.pop('received_email')\n  if len(headers_struc['received_domain']) == 0:\n  headers_struc.pop('received_domain')\n  if len(headers_struc['received_ip']) == 0:\n  headers_struc.pop('received_ip')\n  raw_body = get_raw_body_text(msg)\n  if include_raw_body:\n  bodys_struc['raw_body'] = raw_body\n  bodys = {}\n  multipart = True\n  if len(raw_body) == 1:\n  multipart = False\n  for body_tup in raw_body:\n  bodie = {}\n  encoding, body, body_multhead = body_tup\n  list_observed_urls = []\n  list_observed_email = []\n  list_observed_dom = []\n  list_observed_ip = []\n  if sys.version_info >= (3, 0) and (isinstance(body, bytes) or isinstance(body, bytearray)):\n  body = body.decode('utf-8', 'ignore')\n  if len(body) < 4096:\n  list_observed_urls = get_uri_ondata(body)\n  for match in email_regex.findall(body):\n  list_observed_email.append(match.lower())\n  for match in dom_regex.findall(body):\n  list_observed_dom.append(match.lower())\n  for match in ipv4_regex.findall(body):\n  if not priv_ip_regex.match(match):\n  list_observed_ip.append(match)\n  for match in ipv6_regex.findall(body):\n  if not priv_ip_regex.match(match):\n  list_observed_ip.append(match.lower())\n  else:\n  for scn_pt in findall('://', body):\n  list_observed_urls = get_uri_ondata(body[scn_pt-16:scn_pt+4096]) + list_observed_urls\n  for scn_pt in findall('@', body):\n  for match in email_regex.findall(body[scn_pt-64:scn_pt+255]):\n  list_observed_email.append(match.lower())\n  for scn_pt in findall('.', body):\n  for match in dom_regex.findall(body[scn_pt-253:scn_pt+1004]):\n  list_observed_dom.append(match.lower())\n  for match in ipv4_regex.findall(body)[scn_pt-11:scn_pt+3]:\n  if not priv_ip_regex.match(match):\n  list_observed_ip.append(match)\n  for scn_pt in findall(':', body):\n  for match in ipv6_regex.findall(body[scn_pt-4:scn_pt+35]):\n  if not priv_ip_regex.match(match):\n  list_observed_ip.append(match.lower())\n  if include_raw_body:\n  if list_observed_urls:\n  bodie['uri'] = list(set(list_observed_urls))\n  if list_observed_email:\n  bodie['email'] = list(set(list_observed_email))\n  if list_observed_dom:\n  bodie['domain'] = list(set(list_observed_dom))\n   if list_observed_dom:\n  bodie['ip'] = list(set(list_observed_ip))\n   else:\n  if list_observed_urls:\n  bodie['uri_hash'] = []\n  for uri in list(set(list_observed_urls)):\n  bodie['uri_hash'].append(hashlib.sha256(uri.lower()).hexdigest())\n  if list_observed_email:\n  bodie['email_hash'] = []\n  for emel in list(set(list_observed_email)):\n  bodie['email_hash'].append(hashlib.sha256(emel).hexdigest())\n  if list_observed_dom:\n  bodie['domain_hash'] = []\n  for uri in list(set(list_observed_dom)):\n  bodie['domain_hash'].append(hashlib.sha256(uri.lower()).hexdigest())\n  if list_observed_ip:\n  bodie['ip_hash'] = []\n  for fip in list(set(list_observed_ip)):\n  bodie['ip_hash'].append(hashlib.sha256(fip).hexdigest())\n    a: [toto,titi]\n c: [truc]\n  ch = {}\n  for k, v in body_multhead:\n  k = ad(k.lower()).replace('.', ':')\n  if multipart:\n  if k in ch:\n  ch[k].append(ad(v))\n  else:\n  ch[k] = [ad(v)]\n  else:\n  if k.startswith('content'):\n  k = ad(k.lower()).replace('.', ':')\n  if k in ch:\n  ch[k].append(ad(v))\n  else:\n  ch[k] = [ad(v)]\n  bodie['content_header'] = ch\n  if include_raw_body:\n  bodie['content'] = body\n  val = ch.get('content-type')\n  if val:\n  if type(val) == list:\n  val = val[-1]\n  bodie['content_type'] = val.split(';')[0].strip()\n  try:\n  bodie['hash'] = hashlib.sha256(body).hexdigest()\n  except:\n  bodie['hash'] = hashlib.sha256(body.encode('UTF-8')).hexdigest()\n  uid = str(uuid.uuid1())\n  bodys[uid] = bodie\n  bodys_struc = bodys\n    a: [toto,titi]\n c: [truc]\n  for k, v in msg.items():\n  k = ad(k.lower()).replace('.', ':')\n  if k in header:\n  header[k].append(ad(v))\n  else:\n  header[k] = [ad(v)]\n  headers_struc['header'] = header\n  report_struc['attachment'] = traverse_multipart(msg, 0, include_attachment_data)\n  if len(report_struc['attachment']) == 0:\n  report_struc.pop('attachment')\n  else:\n  newattach = []\n  for attachment in report_struc['attachment']:\n  newattach.append(report_struc['attachment'][attachment])\n  report_struc['attachment'] = newattach\n  newbody = []\n  for body in bodys_struc:\n  newbody.append(bodys_struc[body])\n  report_struc['body'] = newbody\n  report_struc['header'] = headers_struc\n  return report_struc", "target": 1, "info": "Null", "idx": 0}
{"func": "def Execute(options):\n  RFLPInfoDirectory = os.path.dirname(options.primerFile)\n  RFLPReference = None\n  RFLPReference = PrimerEngine.RFLPMarker.RFLPReference(RFLPInfoDirectory)\n  if not options.primerFile:\n  print(\"FATAL: No primer file specified.\")\n  exit(1)\n  PrimerTypes = [\"ForwardPrimer\", \"ReversePrimer\"]\n  featureFolderPath = \"annotations\"\n  if os.path.isdir(featureFolderPath):\n  genomeFeatureFiles = [\n  os.path.join(featureFolderPath, File)\n  for File in os.listdir(featureFolderPath)\n  if not File.startswith(\".\")\n  ]\n  genomeFeaturesSet = [\n  list(SeqIO.parse(File, \"genbank\"))\n  for File in genomeFeatureFiles if File.endswith(\".gbff\")\n  ]\n  else:\n  genomeFeaturesSet = []\n  if not genomeFeaturesSet:\n  print(\"Fatal: No features found.\")\n  lociPrimerList = loadPrimerList(options.primerFile)\n  genomeDirectory = \"genomes\"\n  if os.path.isdir(genomeDirectory):\n  genomes = os.listdir(genomeDirectory)\n  genomeFilePaths = [os.path.join(genomeDirectory, genomeFile)\n for genomeFile in genomes\n if genomeFile.endswith(('.fna', '.fasta'))]\n  genomes = [PrimerEngine.GeneticEntities.Genome(genomeFilePath)\n for genomeFilePath in genomeFilePaths]\n  print(\"Loaded %i genomes.\" % len(genomes))\n  maxGenomes = 25\n  if len(genomes) > maxGenomes:\n  print(\"Discarding genomes, max is %i!\" % maxGenomes)\n  genomes = genomes[:maxGenomes]\n  else:\n  genomes = []\n  if not genomes:\n  print(\"Fatal: No genomes found!\")\n  exit(1)\n  if len(genomes) < 4:\n  print(\"Fatal: need at least 4 genomes to proper execute the analysis, got only %i.\" % len(genomes))\n  exit(1)\n  genomeFeatures = annotationManager.loadAnnotation(\"annotations\")\n  bruteForceSearcher =\\\n  PrimerEngine.bruteForcePrimerSearch.bruteForceSearcher(\n  genomeFeatures,\n  genomeFilePaths\n  )\n  if not bruteForceSearcher.matchedGenome:\n  bruteForceSearcher = None\n  AllLociPrimerSet = OrderedDict()\n  RebootLocusTolerance = 13\n  matchedPrimerSequences = []\n  for i in range(lociPrimerList.shape[0]):\n  locus_info = lociPrimerList.iloc[i]\n  locus_name = locus_info[\"LocusName\"]\n  outputFastaName = \"LOCI_%s.fasta\" % locus_name\n   outputFastaPath = os.path.join(options.outputPath, outputFastaName)\n  if os.path.isfile(outputFastaPath):\n  print(\"Skipping locus %s. Already exists...\" % locus_name)\n  continue\n  if options.WantedLoci:\n  WantedLoci = options.WantedLoci.split(',')\n  WantedLoci = [l.strip() for l in WantedLoci]\n  if locus_name not in WantedLoci:\n  continue\n  overallProgress = (i + 1, lociPrimerList.shape[0])\n  (LocusAmpliconSet, matchSuccess, primerPair) =\\\n  PrimerEngine.PrimerDock.matchLocusOnGenomes(\n  locus_name,\n  locus_info,\n  genomes,\n  overallProgress,\n  bruteForceSearcher=bruteForceSearcher\n  )\n  if LocusAmpliconSet is not None:\n  score = PrimerEngine.ampliconSanity.evaluateSetOfAmplicons(\n  LocusAmpliconSet)\n  print(\"\\tAlignment Health = %.2f%%\" % score)\n  print()\n  writeFastaFile(outputFastaPath, locus_name,\n LocusAmpliconSet, RFLPReference=RFLPReference)\n  primerPair[\"AlignmentHealth\"] = score\n  RegionLengths = [len(r) for r in LocusAmpliconSet]\n  primerPair[\"MeanLength\"] = np.mean(RegionLengths)\n  primerPair[\"StdLength\"] = np.std(RegionLengths)\n  matchedPrimerSequences.append(primerPair)\n  AllLociPrimerSet[locus_name] = matchSuccess\n  else:\n  print(\"WARNING: PrimerDock failure.\")\n  if matchedPrimerSequences:\n  outputFilePath = os.path.join(options.outputPath,\n    \"MatchedRegions.csv\")\n  data = pd.DataFrame(matchedPrimerSequences,\n  columns=[\n  \"LocusName\",\n  *PrimerTypes,\n  \"RebootCount\",\n  \"AlignmentHealth\",\n  \"MeanLength\",\n  \"StdLength\"\n  ])\n  data.to_csv(outputFilePath, index=False)\n  PrimerData = []\n  allPrimers = []\n  for Locus in AllLociPrimerSet.keys():\n  for Primer in AllLociPrimerSet[Locus]:\n  row = Primer[0].to_dict(Locus)\n  del row[\"Chromosome\"]\n  PrimerData.append(row)\n  allPrimers.append(Primer)\n  outputFilePath = os.path.join(options.outputPath, \"PrimerData.csv\")\n  data = pd.DataFrame(PrimerData)\n  data.to_csv(outputFilePath, index=False)\n  else:\n  print(\"No regions found, nothing to do.\")\n  return matchedPrimerSequences", "target": 0, "info": "Null", "idx": 0}
{"func": "def Execute(options):\n  RFLPInfoDirectory = os.path.dirname(options.primerFile)\n  RFLPReference = None\n  RFLPReference = PrimerEngine.RFLPMarker.RFLPReference(RFLPInfoDirectory)\n  if not options.primerFile:\n  print(\"FATAL: No primer file specified.\")\n  exit(1)\n  PrimerTypes = [\"ForwardPrimer\", \"ReversePrimer\"]\n  featureFolderPath = \"annotations\"\n  if os.path.isdir(featureFolderPath):\n  genomeFeatureFiles = [\n  os.path.join(featureFolderPath, File)\n  for File in os.listdir(featureFolderPath)\n  if not File.startswith(\".\")\n  ]\n  genomeFeaturesSet = [\n  list(SeqIO.parse(File, \"genbank\"))\n  for File in genomeFeatureFiles if File.endswith(\".gbff\")\n  ]\n  else:\n  genomeFeaturesSet = []\n  if not genomeFeaturesSet:\n  print(\"Fatal: No features found.\")\n  lociPrimerList = loadPrimerList(options.primerFile)\n  genomeDirectory = \"genomes\"\n  if os.path.isdir(genomeDirectory):\n  genomes = os.listdir(genomeDirectory)\n  genomeFilePaths = [os.path.join(genomeDirectory, genomeFile)\n for genomeFile in genomes\n if genomeFile.endswith(('.fna', '.fasta'))]\n  genomes = [PrimerEngine.GeneticEntities.Genome(genomeFilePath)\n for genomeFilePath in genomeFilePaths]\n  print(\"Loaded %i genomes.\" % len(genomes))\n  maxGenomes = 25\n  if len(genomes) > maxGenomes:\n  print(\"Discarding genomes, max is %i!\" % maxGenomes)\n  genomes = genomes[:maxGenomes]\n  else:\n  genomes = []\n  if not genomes:\n  print(\"Fatal: No genomes found!\")\n  exit(1)\n  if len(genomes) < 4:\n  print(\"Fatal: need at least 4 genomes to proper execute the analysis, got only %i.\" % len(genomes))\n  exit(1)\n  genomeFeatures = annotationManager.loadAnnotation(\"annotations\")\n  bruteForceSearcher =\\\n  PrimerEngine.bruteForcePrimerSearch.bruteForceSearcher(\n  genomeFeatures,\n  genomeFilePaths\n  )\n  if not bruteForceSearcher.matchedGenome:\n  bruteForceSearcher = None\n  AllLociPrimerSet = OrderedDict()\n  RebootLocusTolerance = 13\n  matchedPrimerSequences = []\n  for i in range(lociPrimerList.shape[0]):\n  locus_info = lociPrimerList.iloc[i]\n  locus_name = locus_info[\"LocusName\"]\n  outputFastaName = \"LOCI_%s.fasta\" % locus_name\n   outputFastaPath = os.path.join(options.outputPath, outputFastaName)\n  if os.path.isfile(outputFastaName):\n  print(\"Skipping locus %s. Already exists...\" % locus_name)\n  continue\n  if options.WantedLoci:\n  WantedLoci = options.WantedLoci.split(',')\n  WantedLoci = [l.strip() for l in WantedLoci]\n  if locus_name not in WantedLoci:\n  continue\n  overallProgress = (i + 1, lociPrimerList.shape[0])\n  (LocusAmpliconSet, matchSuccess, primerPair) =\\\n  PrimerEngine.PrimerDock.matchLocusOnGenomes(\n  locus_name,\n  locus_info,\n  genomes,\n  overallProgress,\n  bruteForceSearcher=bruteForceSearcher\n  )\n  if LocusAmpliconSet is not None:\n  score = PrimerEngine.ampliconSanity.evaluateSetOfAmplicons(\n  LocusAmpliconSet)\n  print(\"\\tAlignment Health = %.2f%%\" % score)\n  print()\n  writeFastaFile(outputFastaPath, locus_name,\n LocusAmpliconSet, RFLPReference=RFLPReference)\n  primerPair[\"AlignmentHealth\"] = score\n  RegionLengths = [len(r) for r in LocusAmpliconSet]\n  primerPair[\"MeanLength\"] = np.mean(RegionLengths)\n  primerPair[\"StdLength\"] = np.std(RegionLengths)\n  matchedPrimerSequences.append(primerPair)\n  AllLociPrimerSet[locus_name] = matchSuccess\n  else:\n  print(\"WARNING: PrimerDock failure.\")\n  if matchedPrimerSequences:\n  outputFilePath = os.path.join(options.outputPath,\n    \"MatchedRegions.csv\")\n  data = pd.DataFrame(matchedPrimerSequences,\n  columns=[\n  \"LocusName\",\n  *PrimerTypes,\n  \"RebootCount\",\n  \"AlignmentHealth\",\n  \"MeanLength\",\n  \"StdLength\"\n  ])\n  data.to_csv(outputFilePath, index=False)\n  PrimerData = []\n  allPrimers = []\n  for Locus in AllLociPrimerSet.keys():\n  for Primer in AllLociPrimerSet[Locus]:\n  row = Primer[0].to_dict(Locus)\n  del row[\"Chromosome\"]\n  PrimerData.append(row)\n  allPrimers.append(Primer)\n  outputFilePath = os.path.join(options.outputPath, \"PrimerData.csv\")\n  data = pd.DataFrame(PrimerData)\n  data.to_csv(outputFilePath, index=False)\n  else:\n  print(\"No regions found, nothing to do.\")\n  return matchedPrimerSequences", "target": 1, "info": "Null", "idx": 0}
{"func": "def send(self, request, **kwargs):\n  func = super(FuturesSession, self).send\n  if isinstance(self.executor, ProcessPoolExecutor):\n  try:\n  dumps(func)\n  except (TypeError, PickleError):\n  raise RuntimeError(PICKLE_ERROR)\n  return self.executor.submit(func, request, **kwargs)", "target": 0, "info": "Null", "idx": 0}
{"func": "def send(self, request, **kwargs):\n  func = super(FuturesSession, self).send\n  if isinstance(self.executor, ProcessPoolExecutor):\n  try:\n  dumps(request)\n  except (TypeError, PickleError):\n  raise RuntimeError(PICKLE_ERROR)\n  return self.executor.submit(func, request, **kwargs)", "target": 1, "info": "Null", "idx": 0}
{"func": "def classifierTrainer(maxLeafNodes, data=None, data_backup_file='patchesDataSet/Features',\n    filename_to_save_model='PatchClassifierModel'):\n  if data is None:\n  data = pd.read_csv(f\"{package_path()}/data/patchesDataSet/{data_backup_file}.csv\", index_col=False)\n  features = list(data)\n  del features[0]\n  else:\n  features = list(data)\n  del features[-1]\n  trainX = data[features][500:]\n  trainy = data.Result[500:]\n  valX = data[features][:500]\n  valy = data.Result[:500]\n  clf = RandomForestClassifier(max_leaf_nodes=maxLeafNodes, random_state=2)\n  clf.fit(trainX, trainy)\n  if filename_to_save_model:\n  save_model(clf, filename_to_save_model)\n  preds = clf.predict(valX)\n  acc = accuracy_score(valy, preds)\n  return clf, acc", "target": 0, "info": "Null", "idx": 0}
{"func": "def classifierTrainer(maxLeafNodes, data=None, data_backup_file='patchesDataSet/Features',\n    filename_to_save_model='PatchClassifierModel'):\n  if data is not None:\n  data = pd.read_csv(f\"{package_path()}/data/patchesDataSet/{data_backup_file}.csv\", index_col=False)\n  features = list(data)\n  del features[0]\n  else:\n  features = list(data)\n  del features[-1]\n  trainX = data[features][500:]\n  trainy = data.Result[500:]\n  valX = data[features][:500]\n  valy = data.Result[:500]\n  clf = RandomForestClassifier(max_leaf_nodes=maxLeafNodes, random_state=2)\n  clf.fit(trainX, trainy)\n  if filename_to_save_model:\n  save_model(clf, filename_to_save_model)\n  preds = clf.predict(valX)\n  acc = accuracy_score(valy, preds)\n  return clf, acc", "target": 1, "info": "Null", "idx": 0}
{"func": "def classifierTrainer(maxLeafNodes, data=None, data_backup_file='patchesDataSet/Features',\n    filename_to_save_model='PatchClassifierModel'):\n  if data is None:\n  data = pd.read_csv(f\"{package_path()}/data/patchesDataSet/{data_backup_file}.csv\", index_col=False)\n  features = list(data)\n  del features[0]\n  else:\n  features = list(data)\n  del features[-1]\n  trainX = data[features][500:]\n  trainy = data.Result[500:]\n  valX = data[features][:500]\n  valy = data.Result[:500]\n  clf = RandomForestClassifier(max_leaf_nodes=maxLeafNodes, random_state=2)\n  clf.fit(trainX, trainy)\n  if filename_to_save_model:\n  save_model(clf, filename_to_save_model)\n  preds = clf.predict(valX)\n  acc = accuracy_score(valy, preds)\n  return clf, acc", "target": 0, "info": "Null", "idx": 0}
{"func": "def classifierTrainer(maxLeafNodes, data=None, data_backup_file='patchesDataSet/Features',\n    filename_to_save_model='PatchClassifierModel'):\n  if data is not None:\n  data = pd.read_csv(f\"{package_path()}/data/patchesDataSet/{data_backup_file}.csv\", index_col=False)\n  features = list(data)\n  del features[0]\n  else:\n  features = list(data)\n  del features[-1]\n  trainX = data[features][500:]\n  trainy = data.Result[500:]\n  valX = data[features][:500]\n  valy = data.Result[:500]\n  clf = RandomForestClassifier(max_leaf_nodes=maxLeafNodes, random_state=2)\n  clf.fit(trainX, trainy)\n  if filename_to_save_model:\n  save_model(clf, filename_to_save_model)\n  preds = clf.predict(valX)\n  acc = accuracy_score(valy, preds)\n  return clf, acc", "target": 1, "info": "Null", "idx": 0}
{"func": " def _request(self, method, endpoint, params=None, **kwargs):\n  _params = {'access_token': self.access_token}\n  if params:\n  _params.update(params)\n  response = requests.request(method, self.BASE_URL + endpoint, params=_params, **kwargs)\n  return self._parse(response)", "target": 0, "info": "Null", "idx": 0}
{"func": "def _request(self, method, endpoint, params=None, **kwargs):\n  _params = {'access_token': self.access_token}\n  if params:\n  _params.update(params)\n  response = requests.request(method, self.BASE_URL + endpoint, params=params, **kwargs)\n  return self._parse(response)", "target": 1, "info": "Null", "idx": 0}
{"func": " def is_not_regex_match(self, pattern: str) -> StringValidator:\n  if RegexHelper.is_match(pattern, self.value):\n  raise ArgumentPatternError(\n  f'The argument `{self.argument_name}` should match the pattern `{pattern}`',\n  self.value,\n  self.argument_name,\n  pattern\n  )\n  return self", "target": 0, "info": "Null", "idx": 0}
{"func": " def is_not_regex_match(self, pattern: str) -> StringValidator:\n  if not RegexHelper.is_match(pattern, self.value):\n  raise ArgumentPatternError(\n  f'The argument `{self.argument_name}` should match the pattern `{pattern}`',\n  self.value,\n  self.argument_name,\n  pattern\n  )\n  return self", "target": 1, "info": "Null", "idx": 0}
{"func": "def __init__(\n  self, name, radius, coordinates, welldepth=1.0, aquiferdepth=None\n  ):\n  self.name = _formstr(name)\n  if isinstance(radius, Variable):\n  self._radius = dcopy(radius)\n  else:\n  self._radius = Variable(\n  \"radius\",\n  radius,\n  \"r\",\n  \"m\",\n  \"Inner radius of well '\" + str(name) + \"'\",\n  )\n  if not self._radius.scalar:\n  raise ValueError(\"Well: 'radius' needs to be scalar\")\n  if self.radius < 0.0:\n  raise ValueError(\"Well: 'radius' needs to be positiv\")\n   if isinstance(coordinates, Variable):\n  self._coordinates = dcopy(coordinates)\n  else:\n  self._coordinates = Variable(\n  \"coordinates\",\n  coordinates,\n  \"XY\",\n  \"m\",\n  \"coordinates of well '\" + str(name) + \"'\",\n  )\n  if np.shape(self.coordinates) != (2,) and not np.isscalar(\n  self.coordinates\n  ):\n  raise ValueError(\n  \"Well: 'coordinates' should be given as \"\n  + \"[x,y] values or one single distance value\"\n  )\n  if isinstance(welldepth, Variable):\n  self._welldepth = dcopy(welldepth)\n  else:\n  self._welldepth = Variable(\n  \"welldepth\",\n  welldepth,\n  \"L_w\",\n  \"m\",\n  \"depth of well '\" + str(name) + \"'\",\n  )\n  if not self._welldepth.scalar:\n  raise ValueError(\"Well: 'welldepth' needs to be scalar\")\n  if self.welldepth <= 0.0:\n  raise ValueError(\"Well: 'welldepth' needs to be positiv\")\n  if isinstance(aquiferdepth, Variable):\n  self._aquiferdepth = dcopy(aquiferdepth)\n  else:\n  if aquiferdepth is None:\n  self._aquiferdepth = Variable(\n  \"aquiferdepth\",\n  welldepth,\n  \"L_a\",\n  \"m\",\n  \"aquiferdepth at well '\" + str(name) + \"'\",\n  )\n  else:\n  self._aquiferdepth = Variable(\n  \"aquiferdepth\",\n  aquiferdepth,\n  \"L_a\",\n  \"m\",\n  \"aquiferdepth at well '\" + str(name) + \"'\",\n  )\n  if not self._aquiferdepth.scalar:\n  raise ValueError(\"Well: 'aquiferdepth' needs to be scalar\")\n  if self.aquiferdepth <= 0.0:\n  raise ValueError(\"Well: 'aquiferdepth' needs to be positiv\")", "target": 0, "info": "Null", "idx": 0}
{"func": "def __init__(\n  self, name, radius, coordinates, welldepth=1.0, aquiferdepth=None\n  ):\n  self.name = _formstr(name)\n  if isinstance(radius, Variable):\n  self._radius = dcopy(radius)\n  else:\n  self._radius = Variable(\n  \"radius\",\n  radius,\n  \"r\",\n  \"m\",\n  \"Inner radius of well '\" + str(name) + \"'\",\n  )\n  if not self._radius.scalar:\n  raise ValueError(\"Well: 'radius' needs to be scalar\")\n  if self.radius <= 0.0:\n  raise ValueError(\"Well: 'radius' needs to be positiv\")\n   if isinstance(coordinates, Variable):\n  self._coordinates = dcopy(coordinates)\n  else:\n  self._coordinates = Variable(\n  \"coordinates\",\n  coordinates,\n  \"XY\",\n  \"m\",\n  \"coordinates of well '\" + str(name) + \"'\",\n  )\n  if np.shape(self.coordinates) != (2,) and not np.isscalar(\n  self.coordinates\n  ):\n  raise ValueError(\n  \"Well: 'coordinates' should be given as \"\n  + \"[x,y] values or one single distance value\"\n  )\n  if isinstance(welldepth, Variable):\n  self._welldepth = dcopy(welldepth)\n  else:\n  self._welldepth = Variable(\n  \"welldepth\",\n  welldepth,\n  \"L_w\",\n  \"m\",\n  \"depth of well '\" + str(name) + \"'\",\n  )\n  if not self._welldepth.scalar:\n  raise ValueError(\"Well: 'welldepth' needs to be scalar\")\n  if self.welldepth <= 0.0:\n  raise ValueError(\"Well: 'welldepth' needs to be positiv\")\n  if isinstance(aquiferdepth, Variable):\n  self._aquiferdepth = dcopy(aquiferdepth)\n  else:\n  if aquiferdepth is None:\n  self._aquiferdepth = Variable(\n  \"aquiferdepth\",\n  welldepth,\n  \"L_a\",\n  \"m\",\n  \"aquiferdepth at well '\" + str(name) + \"'\",\n  )\n  else:\n  self._aquiferdepth = Variable(\n  \"aquiferdepth\",\n  aquiferdepth,\n  \"L_a\",\n  \"m\",\n  \"aquiferdepth at well '\" + str(name) + \"'\",\n  )\n  if not self._aquiferdepth.scalar:\n  raise ValueError(\"Well: 'aquiferdepth' needs to be scalar\")\n  if self.aquiferdepth <= 0.0:\n  raise ValueError(\"Well: 'aquiferdepth' needs to be positiv\")", "target": 1, "info": "Null", "idx": 0}
{"func": "def add_transient_obs(\n  self,\n  well,\n  time,\n  observation,\n  description=\"Transient Drawdown observation\",\n  ):\n  obs = varlib.DrawdownObs(well, observation, time, description)\n  self.add_observations(obs)", "target": 0, "info": "Null", "idx": 0}
{"func": "def add_transient_obs(\n  self,\n  well,\n  time,\n  observation,\n  description=\"Transient Drawdown observation\",\n  ):\n  obs = varlib.DrawdownObs(well, time, observation, description)\n  self.add_observations(obs)", "target": 1, "info": "Null", "idx": 0}
{"func": " def __read_snapshot(self, network_filename):\n  previous_slice, actual_slice = None, None\n  edge_list = []\n  with open(network_filename) as nf:\n  ln = 0\n  for line in nf:\n  edge = line.rstrip().split(\"\\t\")\n  actual_slice = edge[2]\n  if ln == 0:\n  previous_slice = edge[2]\n  ln += 1\n  if actual_slice != previous_slice:\n  vertices = set()\n  for ev in edge_list:\n  vertices.update(ev)\n  vertices = sorted(vertices)\n  g = igraph.Graph()\n  g.add_vertices(vertices)\n  g.add_edges(edge_list)\n  edge_list = []\n   self.slices_ids.append(actual_slice)\n  yield g, previous_slice\n   previous_slice = actual_slice\n   edge_list.append([edge[0], edge[1]])\n  vertices = set()\n  for line in edge_list:\n  vertices.update(line)\n  vertices = sorted(vertices)\n  g = igraph.Graph()\n  g.add_vertices(vertices)\n  g.add_edges(edge_list)\n  yield g, actual_slice", "target": 0, "info": "Null", "idx": 0}
{"func": " def __read_snapshot(self, network_filename):\n  previous_slice, actual_slice = None, None\n  edge_list = []\n  with open(network_filename) as nf:\n  ln = 0\n  for line in nf:\n  edge = line.rstrip().split(\"\\t\")\n  actual_slice = edge[2]\n  if ln == 0:\n  previous_slice = edge[2]\n  ln += 1\n  if actual_slice != previous_slice:\n  vertices = set()\n  for ev in edge_list:\n  vertices.update(ev)\n  vertices = sorted(vertices)\n  g = igraph.Graph()\n  g.add_vertices(vertices)\n  g.add_edges(edge_list)\n  edge_list = []\n   self.slices_ids.append(actual_slice)\n  yield g, actual_slice\n   previous_slice = actual_slice\n   edge_list.append([edge[0], edge[1]])\n  vertices = set()\n  for line in edge_list:\n  vertices.update(line)\n  vertices = sorted(vertices)\n  g = igraph.Graph()\n  g.add_vertices(vertices)\n  g.add_edges(edge_list)\n  yield g, actual_slice", "target": 1, "info": "Null", "idx": 0}
{"func": " def developer(self, developer, results=None, page=None, detailed=False):\n  results = s.DEV_RESULTS if results is None else results\n  page = 0 if page is None else page\n  page_num = (results // 20) * page\n  if not 0 <= page_num <= 12:\n  raise ValueError('Page out of range. (results // 20) * page must be between 0 - 12')\n  pagtok = self._pagtok[page_num]\n   url = build_url('developer', developer)\n  data = generate_post_data(results, 0, pagtok)\n  response = send_request('POST', url, data)\n  soup = BeautifulSoup(response.content, 'lxml')\n  if detailed:\n  apps = self._parse_multiple_apps(response)\n  else:\n  apps = [self._parse_card_info(app) for app in soup.select('div[data-uitype=500]')]\n  return apps", "target": 0, "info": "Null", "idx": 0}
{"func": "def developer(self, developer, results=None, page=None, detailed=False):\n  results = s.DEV_RESULTS if results is None else results\n  page = 0 if page is None else page\n  page_num = (results // 20) * page\n  if not 0 < page_num <= 12:\n  raise ValueError('Page out of range. (results // 20) * page must be between 0 - 12')\n  pagtok = self._pagtok[page_num]\n   url = build_url('developer', developer)\n  data = generate_post_data(results, 0, pagtok)\n  response = send_request('POST', url, data)\n  soup = BeautifulSoup(response.content, 'lxml')\n  if detailed:\n  apps = self._parse_multiple_apps(response)\n  else:\n  apps = [self._parse_card_info(app) for app in soup.select('div[data-uitype=500]')]\n  return apps", "target": 1, "info": "Null", "idx": 0}
{"func": "def _inside_skip(soup_elem):\n  parent = soup_elem.parent\n  while parent is not None:\n  if parent.name in skip_elements:\n  return True\n  parent = parent.parent\n  return False", "target": 0, "info": "Null", "idx": 0}
{"func": "def _inside_skip(soup_elem):\n  parent = soup_elem.parent\n  while parent is not None:\n  if parent.name not in skip_elements:\n  return True\n  parent = parent.parent\n  return False", "target": 1, "info": "Null", "idx": 0}
{"func": "def main():\n  parser = argparse.ArgumentParser(prog=\"experiments\")\n  parser.add_argument(\"mode\", choices=[\"time_generation\", \"generate_code\", \"jobscripts\"])\n  parser.add_argument(\"experiment\", choices=[\"random\", \"pldi\"])\n  parser.add_argument(\"-m\", \"--merging\", choices=[\"true\", \"false\", \"both\"], default=\"true\")\n  parser.add_argument(\"-j\", \"--jobindex\", help=\"Job index.\", type=int, default=0)\n  parser.add_argument(\"-r\", \"--repetitions\", help=\"Number of repetitions.\", type=int)\n  parser.add_argument(\"-c\", \"--constructive\", action=\"store_true\", help=\"Use constructive strategy.\")\n  parser.add_argument(\"-e\", \"--exhaustive\", action=\"store_true\", help=\"Use exhaustive strategy.\")\n  parser.add_argument(\"-f\", \"--reference\", action=\"store_true\", help=\"Generate reference code.\")\n  parser.add_argument(\"-l\", \"--config\", type=str, default=None, help=\"Specify configuration file.\")\n  args = parser.parse_args()\n  PLDI_examples = [\n  PLDI.Example01(),\n  PLDI.Example02(),\n  PLDI.Example03(),\n  PLDI.Example04(),\n  PLDI.Example05(),\n  PLDI.Example06(),\n  PLDI.Example07(),\n  PLDI.Example08(),\n  PLDI.Example09(),\n  PLDI.Example10(),\n  PLDI.Example11(),\n  PLDI.Example12(),\n  PLDI.Example13(),\n  PLDI.Example14(),\n  PLDI.Example15(),\n  PLDI.Example16(),\n  PLDI.Example17(),\n  PLDI.Example18(),\n  PLDI.Example19(),\n  PLDI.Example20(),\n  PLDI.Example21(),\n  PLDI.Example22(),\n  PLDI.Example23(),\n  PLDI.Example24(),\n  PLDI.Example25(),\n  ]\n  ExampleContainer = collections.namedtuple(\"ExampleContainer\", [\"eqns\"])\n  random.seed(0)\n  rand_exprs = [ExampleContainer(generate_equation(random.randint(4, 7))) for _ in range(100)]\n  if args.config:\n  linnea.config.load_config(config_file=args.config)\n  if args.experiment == \"random\":\n  examples = rand_exprs\n  elif args.experiment == \"pldi\":\n  examples = PLDI_examples\n  else:\n  return\n  JobExample = collections.namedtuple(\"JobExample\", [\"example\", \"name\"])\n  if args.jobindex == 0:\n  job_examples = []\n  for idx, example in enumerate(examples, 1):\n  name = \"{}{:03}\".format(args.experiment, idx)\n  job_examples.append(JobExample(example, name))\n  else:\n  name = \"{}{:03}\".format(args.experiment, args.jobindex)\n  job_examples = [JobExample(examples[args.jobindex-1], name)]\n  strategies = []\n  algorithms = []\n  strategy_strs = []\n  if args.constructive:\n  strategies.append(Strategy.constructive)\n  algorithms.append((\"constructive\", \"algorithm0c\"))\n  strategy_strs.append(\"c\")\n  if args.exhaustive:\n  strategies.append(Strategy.exhaustive)\n  algorithms.append((\"exhaustive\", \"algorithm0e\"))\n  strategy_strs.append(\"e\")\n  if args.mode == \"time_generation\":\n  if not strategies:\n  return\n  linnea.config.set_verbosity(0)\n  merging_args = []\n  merging_labels = []\n  if args.merging == \"true\" or args.merging == \"both\":\n  merging_args.append(True)\n  merging_labels.append(\"merging\")\n  elif args.merging == \"false\" or args.merging == \"both\":\n  merging_args.append(False)\n  merging_labels.append(\"no_merging\")\n  mindex = pd.MultiIndex.from_product([\n  [example.name for example in job_examples],\n  [strategy.name for strategy in strategies],\n  merging_labels],\n  names=[\"example\", \"strategy\", \"merging\"])\n  col_index = pd.Index([\"mean\", \"std\", \"min\", \"max\", \"nodes\", \"solution\"])\n  if args.jobindex == 0:\n  output_file = \"generation.csv\"\n  else:\n  output_file = \"generation{:03}.csv\".format(args.jobindex)\n  data = []\n  for example, name in job_examples:\n  for strategy, merge in itertools.product(strategies, merging_args):\n  data.append(measure(example, name, strategy, merge, args.repetitions))\n  dframe = pd.DataFrame(data, index=mindex[:len(data)], columns=col_index)\n  dframe.to_csv(output_file)\n  elif args.mode == \"generate_code\":\n  linnea.config.set_verbosity(2)\n   for strategy_str in strategy_strs:\n  dir = os.path.join(linnea.config.results_path, args.experiment, \"intensity\", strategy_str)\n  if not os.path.exists(dir):\n  os.makedirs(dir)\n   for example, name in job_examples:\n  for strategy in strategies:\n  generate(args.experiment, example, name, strategy)\n  if args.reference:\n  reference_code.generate_reference_code(name, example.eqns)\n  operand_generation.generate_operand_generator(name, example.eqns)\n  existing_algorithms = []\n  for subdir_name, algorithm_name in [(\"constructive\", \"algorithm0c\"), (\"exhaustive\", \"algorithm0e\")]:\n  file_path = os.path.join(linnea.config.output_code_path, name, Language.Julia.name, subdir_name, algorithm_name + \".jl\")\n  if os.path.exists(file_path):\n  existing_algorithms.append((subdir_name, algorithm_name))\n   runner.generate_runner(name, existing_algorithms)\n  elif args.mode == \"jobscripts\":\n  generate_scripts(args.experiment, len(examples))", "target": 0, "info": "Null", "idx": 0}
{"func": "def main():\n  parser = argparse.ArgumentParser(prog=\"experiments\")\n  parser.add_argument(\"mode\", choices=[\"time_generation\", \"generate_code\", \"jobscripts\"])\n  parser.add_argument(\"experiment\", choices=[\"random\", \"pldi\"])\n  parser.add_argument(\"-m\", \"--merging\", choices=[\"true\", \"false\", \"both\"], default=\"true\")\n  parser.add_argument(\"-j\", \"--jobindex\", help=\"Job index.\", type=int, default=0)\n  parser.add_argument(\"-r\", \"--repetitions\", help=\"Number of repetitions.\", type=int)\n  parser.add_argument(\"-c\", \"--constructive\", action=\"store_true\", help=\"Use constructive strategy.\")\n  parser.add_argument(\"-e\", \"--exhaustive\", action=\"store_true\", help=\"Use exhaustive strategy.\")\n  parser.add_argument(\"-f\", \"--reference\", action=\"store_true\", help=\"Generate reference code.\")\n  parser.add_argument(\"-l\", \"--config\", type=str, default=None, help=\"Specify configuration file.\")\n  args = parser.parse_args()\n  PLDI_examples = [\n  PLDI.Example01(),\n  PLDI.Example02(),\n  PLDI.Example03(),\n  PLDI.Example04(),\n  PLDI.Example05(),\n  PLDI.Example06(),\n  PLDI.Example07(),\n  PLDI.Example08(),\n  PLDI.Example09(),\n  PLDI.Example10(),\n  PLDI.Example11(),\n  PLDI.Example12(),\n  PLDI.Example13(),\n  PLDI.Example14(),\n  PLDI.Example15(),\n  PLDI.Example16(),\n  PLDI.Example17(),\n  PLDI.Example18(),\n  PLDI.Example19(),\n  PLDI.Example20(),\n  PLDI.Example21(),\n  PLDI.Example22(),\n  PLDI.Example23(),\n  PLDI.Example24(),\n  PLDI.Example25(),\n  ]\n  ExampleContainer = collections.namedtuple(\"ExampleContainer\", [\"eqns\"])\n  random.seed(0)\n  rand_exprs = [ExampleContainer(generate_equation(random.randint(4, 7))) for _ in range(100)]\n  if args.config:\n  linnea.config.load_config(config_file=args.config)\n  if args.experiment == \"random\":\n  examples = rand_exprs\n  elif args.experiment == \"pldi\":\n  examples = PLDI_examples\n  else:\n  return\n  JobExample = collections.namedtuple(\"JobExample\", [\"example\", \"name\"])\n  if args.jobindex == 0:\n  job_examples = []\n  for idx, example in enumerate(examples, 1):\n  name = \"{}{:03}\".format(args.experiment, idx)\n  job_examples.append(JobExample(example, name))\n  else:\n  name = \"{}{:03}\".format(args.experiment, args.jobindex)\n  job_examples = [JobExample(examples[args.jobindex-1], name)]\n  strategies = []\n  algorithms = []\n  strategy_strs = []\n  if args.constructive:\n  strategies.append(Strategy.constructive)\n  algorithms.append((\"constructive\", \"algorithm0c\"))\n  strategy_strs.append(\"c\")\n  if args.exhaustive:\n  strategies.append(Strategy.exhaustive)\n  algorithms.append((\"exhaustive\", \"algorithm0e\"))\n  strategy_strs.append(\"e\")\n  if args.mode == \"time_generation\":\n  if not strategies:\n  return\n  linnea.config.set_verbosity(0)\n  merging_args = []\n  merging_labels = []\n  if args.merging == \"true\" or args.merging == \"both\":\n  merging_args.append(True)\n  merging_labels.append(\"merging\")\n  elif args.merging == \"false\" or args.merging == \"both\":\n  merging_args.append(False)\n  merging_labels.append(\"no_merging\")\n  mindex = pd.MultiIndex.from_product([\n  [example.name for example in job_examples],\n  [strategy.name for strategy in strategies],\n  merging_labels],\n  names=[\"example\", \"strategy\", \"merging\"])\n  col_index = pd.Index([\"mean\", \"std\", \"min\", \"max\", \"nodes\", \"solution\"])\n  if args.jobindex == 0:\n  output_file = \"generation.csv\"\n  else:\n  output_file = \"generation{:03}.csv\".format(args.jobindex)\n  data = []\n  for example, name in job_examples:\n  for strategy, merge in itertools.product(strategies, merging_args):\n  data.append(measure(example, name, strategy, merge, args.repetitions))\n  dframe = pd.DataFrame(data, index=mindex[:len(data)], columns=col_index)\n  dframe.to_csv(output_file)\n  elif args.mode == \"generate_code\":\n  linnea.config.set_verbosity(2)\n   for strategy_str in strategy_strs:\n  dir = os.path.join(linnea.config.results_path, args.experiment, strategy_str, \"intensity\")\n  if not os.path.exists(dir):\n  os.makedirs(dir)\n   for example, name in job_examples:\n  for strategy in strategies:\n  generate(args.experiment, example, name, strategy)\n  if args.reference:\n  reference_code.generate_reference_code(name, example.eqns)\n  operand_generation.generate_operand_generator(name, example.eqns)\n  existing_algorithms = []\n  for subdir_name, algorithm_name in [(\"constructive\", \"algorithm0c\"), (\"exhaustive\", \"algorithm0e\")]:\n  file_path = os.path.join(linnea.config.output_code_path, name, Language.Julia.name, subdir_name, algorithm_name + \".jl\")\n  if os.path.exists(file_path):\n  existing_algorithms.append((subdir_name, algorithm_name))\n   runner.generate_runner(name, existing_algorithms)\n  elif args.mode == \"jobscripts\":\n  generate_scripts(args.experiment, len(examples))", "target": 1, "info": "Null", "idx": 0}
{"func": "def get_replacement(self, match_dict, memory):\n  replacement = None\n  matched_operand_name = match_dict[self.operand.variable_name].name\n  try:\n  storage_format = memory.storage_format[matched_operand_name]\n  except KeyError:\n  raise memory_module.OperandNotInMemory()\n   if self.storage_format <= storage_format:\n  replacement = self.values[0]\n  else:\n  replacement = self.values[1]\n  if config.c:\n  return None, \"\".join([\"\\\"\", replacement, \"\\\"\"])\n  elif config.julia:\n  return None, \"\".join([\"\\'\", replacement, \"\\'\"])\n  else:\n  raise config.LanguageOptionNotImplemented()", "target": 0, "info": "Null", "idx": 0}
{"func": "def get_replacement(self, match_dict, memory):\n  replacement = None\n  matched_operand_name = match_dict[self.operand.variable_name].name\n  try:\n  storage_format = memory.storage_format[matched_operand_name]\n  except KeyError:\n  raise memory_module.OperandNotInMemory()\n   if self.storage_format == storage_format:\n  replacement = self.values[0]\n  else:\n  replacement = self.values[1]\n  if config.c:\n  return None, \"\".join([\"\\\"\", replacement, \"\\\"\"])\n  elif config.julia:\n  return None, \"\".join([\"\\'\", replacement, \"\\'\"])\n  else:\n  raise config.LanguageOptionNotImplemented()", "target": 1, "info": "Null", "idx": 0}
{"func": "def measure(experiment, example, name, merging):\n  linnea.config.clear_all()\n  if hasattr(example, \"init\"):\n  example.init()\n  graph = DerivationGraph(example.eqns)\n  trace = graph.derivation(\n  time_limit=30*60,\n  merging=merging,\n  dead_ends=True)\n  df_trace = pd.DataFrame(trace, columns=[\"time\", \"cost\"])\n  t_start = time.perf_counter()\n  graph.write_output(code=True,\n derivation=False,\n output_name=name,\n experiment_code=False,\n algorithms_limit=1,\n graph=False,\n subdir_name=\"time_generation\")\n  t_end = time.perf_counter()\n   df_code_gen_time = pd.DataFrame([t_end-t_start], index=[name], columns=[\"time\"])\n   if merging:\n  subdir = \"merging\"\n  else:\n  subdir = \"no_merging\"\n  file_path = os.path.join(linnea.config.results_path, experiment, \"generation\", subdir, name + \"_trace.csv\")\n  df_trace.to_csv(file_path)\n  if linnea.config.verbosity >= 2:\n  print(\"Generate trace file {}\".format(file_path))\n  file_path = os.path.join(linnea.config.results_path, experiment, \"generation\", subdir, name + \"_code_gen_time.csv\")\n  df_code_gen_time.to_csv(file_path)\n  if linnea.config.verbosity >= 2:\n  print(\"Generate code gen time file {}\".format(file_path))\n  return", "target": 0, "info": "Null", "idx": 0}
{"func": "def measure(experiment, example, name, merging):\n  linnea.config.clear_all()\n  if hasattr(example, \"init\"):\n  example.init()\n  graph = DerivationGraph(example.eqns)\n  trace = graph.derivation(\n  time_limit=30*60,\n  merging=merging,\n  dead_ends=True)\n  df_trace = pd.DataFrame(trace, columns=[\"time\", \"cost\"])\n  t_start = time.perf_counter()\n  graph.write_output(code=True,\n derivation=False,\n output_name=name,\n experiment_code=False,\n algorithms_limit=1,\n graph=False,\n subdir_name=\"time_generation\")\n  t_end = time.perf_counter()\n   df_code_gen_time = pd.DataFrame([t_end-t_start], index=[example], columns=[\"time\"])\n   if merging:\n  subdir = \"merging\"\n  else:\n  subdir = \"no_merging\"\n  file_path = os.path.join(linnea.config.results_path, experiment, \"generation\", subdir, name + \"_trace.csv\")\n  df_trace.to_csv(file_path)\n  if linnea.config.verbosity >= 2:\n  print(\"Generate trace file {}\".format(file_path))\n  file_path = os.path.join(linnea.config.results_path, experiment, \"generation\", subdir, name + \"_code_gen_time.csv\")\n  df_code_gen_time.to_csv(file_path)\n  if linnea.config.verbosity >= 2:\n  print(\"Generate code gen time file {}\".format(file_path))\n  return", "target": 1, "info": "Null", "idx": 0}
{"func": " def write_output(\n  self,\n  code=True,\n  derivation=False,\n  output_name=\"tmp\",\n  experiment_code=False,\n  k_best=False,\n  algorithms_limit=1,\n  pruning_factor=1.0,\n  graph=False,\n  graph_style=config.GraphStyle.full,\n  subdir_name=\"generated\",\n  subdir_name_experiments=\"experiments\",\n  algorithm_name=\"algorithm{}\",\n  no_duplicates=False\n  ):\n  if not config.output_code_path:\n  raise config.OutputPathNotSet(\"Unable to write output: output_code_path not set.\")\n  if graph:\n  self.write_graph(output_name, graph_style)\n   algorithms = []\n  known_algorithms = set()\n  number_of_algorithms = 0\n  min_cost = self.shortest_path()[1]\n  for path, cost in self.k_shortest_paths(math.inf):\n  if k_best and cost > pruning_factor*min_cost:\n  break\n  algorithm = self.path_to_algorithm(path, cost)\n  if no_duplicates:\n  if not algorithm in known_algorithms:\n  known_algorithms.add(algorithm)\n  algorithms.append(algorithm)\n  number_of_algorithms += 1\n  else:\n  algorithms.append(algorithm)\n  number_of_algorithms += 1\n  if number_of_algorithms == algorithms_limit:\n  break\n  self.print_result(\"Number of algorithms:\", number_of_algorithms)\n  if code or derivation or experiment_code:\n  directory_name = os.path.join(config.output_code_path, output_name)\n  if not os.path.exists(directory_name):\n  os.makedirs(directory_name)\n  else:\n  if code:\n  algorithms_dir_name = os.path.join(directory_name, config.language.name, subdir_name)\n  cgu.remove_files(algorithms_dir_name)\n  derivation_dir_name = os.path.join(directory_name, config.language.name, subdir_name, \"derivation\")\n  cgu.remove_files(derivation_dir_name)\n  if experiment_code:\n  algorithms_dir_name = os.path.join(directory_name, config.language.name, subdir_name_experiments)\n  cgu.remove_files(algorithms_dir_name)\n  algorithms_dir_name = os.path.join(directory_name, config.language.name, subdir_name_experiments, \"derivation\")\n  cgu.remove_files(algorithms_dir_name)\n  if code or derivation:\n  for n, algorithm in enumerate(algorithms):\n  if code:\n  cgu.algorithm_to_file(output_name, subdir_name, algorithm_name.format(n), algorithm.code(), algorithm.experiment_input, algorithm.experiment_output)\n  if derivation:\n  cgu.derivation_to_file(output_name, subdir_name, algorithm_name.format(n), algorithm.derivation())\n  if experiment_code:\n  cgu.algorithm_to_file(output_name, subdir_name_experiments, algorithm_name.format(n), algorithm.code(), algorithm.experiment_input, algorithm.experiment_output, experiment=True)\n  if derivation:\n  cgu.derivation_to_file(output_name, subdir_name_experiments, algorithm_name.format(n), algorithm.derivation())\n  if experiment_code:\n  generate_experiment_code(output_name, self.input, algorithm_name, [1, 24], k_best, number_of_algorithms)\n    return algorithms", "target": 0, "info": "Null", "idx": 0}
{"func": "def write_output(\n  self,\n  code=True,\n  derivation=False,\n  output_name=\"tmp\",\n  experiment_code=False,\n  k_best=False,\n  algorithms_limit=1,\n  pruning_factor=1.0,\n  graph=False,\n  graph_style=config.GraphStyle.full,\n  subdir_name=\"generated\",\n  subdir_name_experiments=\"experiments\",\n  algorithm_name=\"algorithm{}\",\n  no_duplicates=False\n  ):\n  if not config.output_code_path:\n  raise config.OutputPathNotSet(\"Unable to write output: output_code_path not set.\")\n  if graph:\n  self.write_graph(output_name, graph_style)\n   algorithms = []\n  known_algorithms = set()\n  number_of_algorithms = 0\n  min_cost = self.shortest_path()[1]\n  for path, cost in self.k_shortest_paths(math.inf):\n  if k_best and cost > pruning_factor*min_cost:\n  break\n  algorithm = self.path_to_algorithm(path, cost)\n  if no_duplicates:\n  if not algorithm in known_algorithms:\n  known_algorithms.add(algorithm)\n  algorithms.append(algorithm)\n  number_of_algorithms += 1\n  else:\n  algorithms.append(algorithm)\n  number_of_algorithms += 1\n  if number_of_algorithms == algorithms_limit:\n  break\n  self.print_result(\"Number of algorithms:\", number_of_algorithms)\n  if code or derivation or experiment_code:\n  directory_name = os.path.join(config.output_code_path, output_name)\n  if not os.path.exists(directory_name):\n  os.makedirs(directory_name)\n  else:\n  if code:\n  algorithms_dir_name = os.path.join(directory_name, config.language.name, subdir_name)\n  cgu.remove_files(algorithms_dir_name)\n  derivation_dir_name = os.path.join(directory_name, config.language.name, subdir_name, \"derivation\")\n  cgu.remove_files(derivation_dir_name)\n  if experiment_code:\n  algorithms_dir_name = os.path.join(directory_name, config.language.name, subdir_name_experiments)\n  cgu.remove_files(algorithms_dir_name)\n  algorithms_dir_name = os.path.join(directory_name, config.language.name, subdir_name_experiments, \"derivation\")\n  cgu.remove_files(algorithms_dir_name)\n  if code or derivation:\n  for n, algorithm in enumerate(algorithms):\n  if code:\n  cgu.algorithm_to_file(output_name, subdir_name, algorithm_name.format(n), algorithm.code(), algorithm.experiment_input, algorithm.experiment_output)\n  if derivation:\n  cgu.derivation_to_file(output_name, subdir_name, algorithm_name.format(n), algorithm.derivation())\n  if experiment_code:\n  cgu.algorithm_to_file(output_name, subdir_name_experiments, algorithm_name.format(n), algorithm.code(), algorithm.experiment_input, algorithm.experiment_output, experiment=True)\n  if derivation:\n  cgu.derivation_to_file(output_name, subdir_name_experiments, algorithm_name.format(n), algorithm.derivation())\n  if experiment_code:\n  generate_experiment_code(output_name, self.input, algorithm_name, [1, 24], k_best, number_of_algorithms)\n    return number_of_algorithms", "target": 1, "info": "Null", "idx": 0}
{"func": "def _build_batch_arrays(self, batch_size):\n  if batch_size > self.num_of_states:\n  batch_size = self.num_of_states\n  if self.num_of_states % batch_size != 0:\n  raise Exception('In exact the batch size must divide the total number of states in the system')\n  self.batch_size = batch_size\n  self.num_of_batch_until_full_cycle = self.num_of_states // self.batch_size\n  self.batch_complex_local_energies = np.zeros((self.batch_size, ), dtype=np.complex128)\n  self.batch_naive_complex_local_energies = np.zeros((self.batch_size, ), dtype=np.complex128)", "target": 0, "info": "Null", "idx": 0}
{"func": "def _build_batch_arrays(self, batch_size):\n  if batch_size > self.num_of_states:\n  batch_size = self.num_of_states\n  if self.num_of_states % batch_size != 0:\n  raise Exception('In exact the batch size must divide the total number of states in the system')\n  self.batch_size = batch_size\n  self.num_of_batch_until_full_cycle = self.num_of_states / self.batch_size\n  self.batch_complex_local_energies = np.zeros((self.batch_size, ), dtype=np.complex128)\n  self.batch_naive_complex_local_energies = np.zeros((self.batch_size, ), dtype=np.complex128)", "target": 1, "info": "Null", "idx": 0}
{"func": "def encode_fp_to_snorm(x, *, dtype=np.uint8, nbits=None):\n  assert (x.dtype.kind == 'f'), '`dtype` of the argument `x` must be floating point types.'\n  assert (dtype().dtype.kind == 'u'), '`dtype` of the argument `dtype` must be unsigned integer types.'\n  max_nbits = dtype().itemsize * 8\n  if nbits is None:\n  nbits = max_nbits\n  assert (0 < nbits <= max_nbits), '`nbits` value is out of range.'\n  mask = np.invert(dtype(np.iinfo(dtype).max) << dtype(nbits))\n  return dtype(np.around(x * x.dtype.type((1 << (nbits-1)) - 1))) & mask", "target": 0, "info": "Null", "idx": 0}
{"func": "def encode_fp_to_snorm(x, *, dtype=np.uint8, nbits=None):\n  assert (x.dtype.kind == 'f'), '`dtype` of the argument `x` must be floating point types.'\n  assert (dtype().dtype.kind == 'u'), '`dtype` of the argument `dtype` must be unsigned integer types.'\n  max_nbits = dtype().itemsize * 8\n  if nbits is None:\n  nbits = max_nbits\n  assert (0 < nbits <= max_nbits), '`nbits` value is out of range.'\n  mask = np.invert(dtype(np.iinfo(nbits).max) << dtype(nbits))\n  return dtype(np.around(x * x.dtype.type((1 << (nbits-1)) - 1))) & mask", "target": 1, "info": "Null", "idx": 0}
{"func": " def _notify_thread_subscribers(self, thread, notification_text, comment=None):\n  forum = thread.getForum()\n  di = self._thread_info(thread)\n  if comment is not None:\n  di['commenturl'] = comment.absolute_url()\n  di['commenttext'] = safe_unicode(comment.getText())\n  subscriptions = getUtility(ISubscriptions)\n  subscribers = set(subscriptions.subscribers_for(thread)) | set(subscriptions.subscribers_for(forum))\n  mdtool = getToolByName(thread, 'portal_memberdata')\n  keys = mdtool.propertyIds()\n  for mdata in subscribers:\n  if (comment is not None) and (mdata.getId() == comment.Creator()):\n  continue\n  di.update([(k, str(mdata.getProperty(k)).decode(self._encoding())) for k in keys])\n  di['salutation'] = self._salutation_for_member(di)\n  self._notify(di, notification_text % di)\n  log.info('notified subscriber {subscriber}'.format(subscriber=di.get('email')))", "target": 0, "info": "Null", "idx": 0}
{"func": "def _notify_thread_subscribers(self, thread, notification_text, comment=None):\n  forum = thread.getForum()\n  di = self._thread_info(thread)\n  if comment is not None:\n  di['commenturl'] = comment.absolute_url()\n  di['commenttext'] = safe_unicode(comment.getText())\n  subscriptions = getUtility(ISubscriptions)\n  subscribers = set(subscriptions.subscribers_for(thread)) | set(subscriptions.subscribers_for(forum))\n  mdtool = getToolByName(comment, 'portal_memberdata')\n  keys = mdtool.propertyIds()\n  for mdata in subscribers:\n  if (comment is not None) and (mdata.getId() == comment.Creator()):\n  continue\n  di.update([(k, str(mdata.getProperty(k)).decode(self._encoding())) for k in keys])\n  di['salutation'] = self._salutation_for_member(di)\n  self._notify(di, notification_text % di)\n  log.info('notified subscriber {subscriber}'.format(subscriber=di.get('email')))", "target": 1, "info": "Null", "idx": 0}
{"func": "def request(self, method):\n  m_type = method.m_type\n  auth_ = method.auth\n  url = self.__get_url(method)\n  if m_type == 'GET':\n  assert method.body is None, 'For GET method body must be empty'\n  if self.proxies is None:\n  r = requests.get(url=url, params=method.params, headers=method.headers, auth=auth_)\n  else:\n  r = requests.get(url=url, params=method.params, headers=method.headers, proxies=self.proxies, auth=auth_)\n  elif m_type == 'FILE':\n  assert method.files is not None, 'For FILE attribute file must not be empty'\n  if self.proxies is not None:\n  r = requests.post(url=url, params=method.params, data=method.body, headers=method.headers, auth=auth_,\n    files=method.files)\n  else:\n  r = requests.post(url=url, params=method.params, data=method.body, headers=method.headers,\n    proxies=self.proxies, auth=auth_, files=method.files)\n  elif m_type == 'POST':\n  if self.proxies is None:\n  r = requests.post(url=url, params=method.params, data=method.body, headers=method.headers, auth=auth_)\n  else:\n  r = requests.post(url=url, params=method.params, data=method.body, headers=method.headers,\n    proxies=self.proxies, auth=auth_)\n  elif m_type == 'DELETE':\n  if self.proxies is None:\n  r = requests.delete(url=url, params=method.params, data=method.body, headers=method.headers, auth=auth_)\n  else:\n  r = requests.delete(url=url, params=method.params, data=method.body, headers=method.headers,\n  proxies=self.proxies, auth=auth_)\n  elif m_type == 'PATCH':\n  if self.proxies is None:\n  r = requests.patch(url=url, params=method.params, data=method.body, headers=method.headers, auth=auth_)\n  else:\n  r = requests.patch(url=url, params=method.params, data=method.body, headers=method.headers,\n  proxies=self.proxies, auth=auth_)\n  elif m_type == 'PUT':\n  if self.proxies is None:\n  r = requests.put(url=url, params=method.params, data=method.body, headers=method.headers, auth=auth_)\n  else:\n  r = requests.put(url=url, params=method.params, data=method.body, headers=method.headers,\n  proxies=self.proxies, auth=auth_)\n  else:\n  raise Exception(\"\\nnot implemented method request: %s\" % method.m_type)\n  try:\n  if r is None or len(r.content) == 0:\n  return method.response_process({})\n  return method.response_process(r.json(), r.status_code)\n  except Exception as e:\n  logging.info(f'not a json response: {e}')\n  return method.response_process({}, r.status_code)", "target": 0, "info": "Null", "idx": 0}
{"func": "def request(self, method):\n  m_type = method.m_type\n  auth_ = method.auth\n  url = self.__get_url(method)\n  if m_type == 'GET':\n  assert method.body is None, 'For GET method body must be empty'\n  if self.proxies is None:\n  r = requests.get(url=url, params=method.params, headers=method.headers, auth=auth_)\n  else:\n  r = requests.get(url=url, params=method.params, headers=method.headers, proxies=self.proxies, auth=auth_)\n  elif m_type == 'FILE':\n  assert method.files is None, 'For FILE attribute file must not be empty'\n  if self.proxies is not None:\n  r = requests.post(url=url, params=method.params, data=method.body, headers=method.headers, auth=auth_,\n    files=method.files)\n  else:\n  r = requests.post(url=url, params=method.params, data=method.body, headers=method.headers,\n    proxies=self.proxies, auth=auth_, files=method.files)\n  elif m_type == 'POST':\n  if self.proxies is None:\n  r = requests.post(url=url, params=method.params, data=method.body, headers=method.headers, auth=auth_)\n  else:\n  r = requests.post(url=url, params=method.params, data=method.body, headers=method.headers,\n    proxies=self.proxies, auth=auth_)\n  elif m_type == 'DELETE':\n  if self.proxies is None:\n  r = requests.delete(url=url, params=method.params, data=method.body, headers=method.headers, auth=auth_)\n  else:\n  r = requests.delete(url=url, params=method.params, data=method.body, headers=method.headers,\n  proxies=self.proxies, auth=auth_)\n  elif m_type == 'PATCH':\n  if self.proxies is None:\n  r = requests.patch(url=url, params=method.params, data=method.body, headers=method.headers, auth=auth_)\n  else:\n  r = requests.patch(url=url, params=method.params, data=method.body, headers=method.headers,\n  proxies=self.proxies, auth=auth_)\n  elif m_type == 'PUT':\n  if self.proxies is None:\n  r = requests.put(url=url, params=method.params, data=method.body, headers=method.headers, auth=auth_)\n  else:\n  r = requests.put(url=url, params=method.params, data=method.body, headers=method.headers,\n  proxies=self.proxies, auth=auth_)\n  else:\n  raise Exception(\"\\nnot implemented method request: %s\" % method.m_type)\n  try:\n  if r is None or len(r.content) == 0:\n  return method.response_process({})\n  return method.response_process(r.json(), r.status_code)\n  except Exception as e:\n  logging.info(f'not a json response: {e}')\n  return method.response_process({}, r.status_code)", "target": 1, "info": "Null", "idx": 0}
{"func": "def __init__(self, target): \t\tsystem.System.__init__(self)\n \"/usr/include/X11/*\"\n ],\n destination_path=\"X11\",\n recursive=True)", "target": 0, "info": "Null", "idx": 0}
{"func": "def __init__(self, target): \t\tsystem.System.__init__(self)\n \"/usr/include/X11/*\"\n ],\n destination_path=\"X11\",\n recursive=True)", "target": 1, "info": "Null", "idx": 0}
{"func": " def get_controller_name():\n  x_router_controller = operation.router_controller\n  name = self.default_module_name\n  resource_name = ''\n  split = re.split(\"/\", operation.path)\n  for s in split:\n  pattern = re.compile(r\"\\{[a-zA-Z-_]+\\}\")\n  if s and pattern.search(s) is None:\n  resource_name += s.title()\n   if x_router_controller:\n  name = x_router_controller\n  elif resource_name:\n  resource_controller_name = resource_name.replace('-', '_')\n  name += '.' + resource_controller_name\n  return name", "target": 0, "info": "Null", "idx": 0}
{"func": "def get_controller_name():\n  x_router_controller = operation.router_controller\n  name = self.default_module_name\n  resource_name = ''\n  split = re.split(\"/\", operation.path)\n  for s in split:\n  pattern = re.compile(r\"\\{[a-zA-Z-_]+\\}\")\n  if not s and pattern.search(s) is None:\n  resource_name += s.title()\n   if x_router_controller:\n  name = x_router_controller\n  elif resource_name:\n  resource_controller_name = resource_name.replace('-', '_')\n  name += '.' + resource_controller_name\n  return name", "target": 1, "info": "Null", "idx": 0}
{"func": "def setup_trinity_logging(level):\n  from .mp import ctx\n  log_queue = ctx.Queue()\n  logging.basicConfig(level=level)\n  logger = logging.getLogger('trinity')\n  handler = logging.StreamHandler(sys.stdout)\n  formatter = logging.Formatter(\n  '%(levelname)s %(name)s %(asctime)s - %(message)s'\n  )\n  handler.setFormatter(formatter)\n  logger.setLevel(logging.DEBUG)\n  logger.addHandler(handler)\n   listener = handlers.QueueListener(log_queue, handler)\n   return logger, log_queue, listener", "target": 0, "info": "Null", "idx": 0}
{"func": "def setup_trinity_logging(level):\n  from .mp import ctx\n  log_queue = ctx.Queue()\n  logging.basicConfig(level=level)\n  logger = logging.getLogger('trinity')\n  handler = logging.StreamHandler(sys.stdout)\n  formatter = logging.Formatter(\n  '%(levelname)s %(name)s %(asctime)s - %(message)s'\n  )\n  handler.setFormatter(formatter)\n  logger.setLevel(logging.DEBUG)\n  logger.addHandler(handler)\n   listener = handlers.QueueListener(log_queue, logger)\n   return logger, log_queue, listener", "target": 1, "info": "Null", "idx": 0}
{"func": "def __call__(self, transaction):\n  valid_transaction = self.validate_transaction(transaction)\n  message = self.build_evm_message(valid_transaction)\n  computation = self.build_computation(message, valid_transaction)\n  finalized_computation = self.finalize_computation(valid_transaction, computation)\n  return finalized_computation", "target": 0, "info": "Null", "idx": 0}
{"func": "def __call__(self, transaction):\n  valid_transaction = self.validate_transaction(transaction)\n  message = self.build_evm_message(valid_transaction)\n  computation = self.build_computation(message, valid_transaction)\n  finalized_computation = self.finalize_computation(computation, valid_transaction)\n  return finalized_computation", "target": 1, "info": "Null", "idx": 0}
{"func": "def remove_bundle(self, bundle: Bundle, tombstone: Tombstone):\n  elasticsearch_retry.add_context(tombstone=tombstone, bundle=bundle)\n  doc = BundleDocument.from_bundle(bundle)\n  tombstone_doc = BundleTombstoneDocument.from_tombstone(tombstone)\n  modified, index_name = doc.entomb(tombstone_doc, dryrun=self.dryrun)\n  if self.notify or modified and self.notify is None:\n  self._notify(tombstone_doc, index_name)", "target": 0, "info": "Null", "idx": 0}
{"func": "def remove_bundle(self, bundle: Bundle, tombstone: Tombstone):\n  elasticsearch_retry.add_context(tombstone=tombstone, bundle=bundle)\n  doc = BundleDocument.from_bundle(bundle)\n  tombstone_doc = BundleTombstoneDocument.from_tombstone(tombstone)\n  modified, index_name = doc.entomb(tombstone_doc, dryrun=self.dryrun)\n  if self.notify or modified and self.notify is None:\n  self._notify(doc, index_name)", "target": 1, "info": "Null", "idx": 0}
{"func": " def accumulate_grads(self, grads):\n  for (_, g_dst, _), g_src in zip(self.tuples, grads):\n  if isinstance(g_dst, numpy.ndarray):\n  g_dst += cuda.to_cpu(g_src)\n  continue\n  with cuda.get_device(g_dst):\n  if (isinstance(g_src, cuda.ndarray) and\n  g_dst.gpudata.device != g_src.gpudata.device):\n  g_dst += cuda.copy(g_src, out_device=g_dst.gpudata.device)\n  else:\n  g_dst += cuda.to_gpu(g_src)", "target": 0, "info": "Null", "idx": 0}
{"func": " def accumulate_grads(self, grads):\n  for (_, g_dst, _), g_src in zip(self.tuples, grads):\n  if isinstance(g_dst, numpy.ndarray):\n  g_dst += cuda.to_cpu(g_src)\n  continue\n  with cuda.get_device(g_dst):\n  if (isinstance(g_src, cuda.ndarray) and\n  g_dst.gpudata.device != g_src.gpudata.device):\n  g_dst += cuda.copy(g_src, out_device=g_src.gpudata.device)\n  else:\n  g_dst += cuda.to_gpu(g_src)", "target": 1, "info": "Null", "idx": 0}
{"func": "def exec_ultima(source, _clpy_header=''):\n  source = \\\n  'typedef ' + clpy.backend.opencl.utility.typeof_size() \\\n  + ' __kernel_arg_size_t;\\n' \\\n  + _clpy_header + '\\n' \\\n  'static void __clpy_begin_print_out() ' \\\n  '__attribute__((annotate(\"clpy_begin_print_out\")));\\n' \\\n  + source + '\\n' \\\n  'static void __clpy_end_print_out()' \\\n  '__attribute__((annotate(\"clpy_end_print_out\")));\\n'\n  filename = tempfile.gettempdir() + \"/\" + str(time.monotonic()) + \".cpp\"\n  with TempFile(filename, source):\n  root_dir = os.path.join(clpy.__path__[0], \"..\")\n  proc = subprocess.Popen('{} {} -- -I {}'\n  .format(os.path.join(root_dir,\n   \"ultima\",\n   \"ultima\"),\n  filename,\n  os.path.join(root_dir,\n   \"clpy\",\n   \"core\",\n   \"include\"))\n  .strip().split(\" \"),\n  stdout=subprocess.PIPE,\n  stderr=subprocess.PIPE,\n  universal_newlines=True)\n  try:\n  source, errstream = proc.communicate(timeout=15)\n  proc.wait()\n  except subprocess.TimeoutExpired:\n  proc.kill()\n  source, errstream = proc.communicate()\n   if proc.returncode != 0 or len(errstream) > 0:\n  raise clpy.backend.ultima.exceptions.UltimaRuntimeError(\n  proc.returncode, errstream)\n   return source", "target": 0, "info": "Null", "idx": 0}
{"func": "def exec_ultima(source, _clpy_header=''):\n  source = \\\n  'typedef ' + clpy.backend.opencl.utility.typeof_size() \\\n  + ' __kernel_arg_size_t;\\n' \\\n  + _clpy_header + '\\n' \\\n  'static void __clpy_begin_print_out() ' \\\n  '__attribute__((annotate(\"clpy_begin_print_out\")));\\n' \\\n  + source + '\\n' \\\n  'static void __clpy_end_print_out()' \\\n  '__attribute__((annotate(\"clpy_end_print_out\")));\\n'\n  filename = tempfile.gettempdir() + \"/\" + str(time.monotonic()) + \".cpp\"\n  with TempFile(filename, source):\n  root_dir = os.path.join(clpy.__path__[0], \"..\")\n  proc = subprocess.Popen('{} {} -- -I {}'\n  .format(os.path.join(root_dir,\n   \"ultima\",\n   \"ultima\"),\n  filename,\n  os.path.join(root_dir,\n   \"clpy\",\n   \"core\",\n   \"include\"))\n  .strip().split(\" \"),\n  stdout=subprocess.PIPE,\n  stderr=subprocess.PIPE,\n  universal_newlines=True)\n  try:\n  source, errstream = proc.communicate(timeout=15)\n  proc.wait()\n  except subprocess.TimeoutExpired:\n  proc.kill()\n  source, errstream = proc.communicate()\n   if proc.returncode != 0 and len(errstream) > 0:\n  raise clpy.backend.ultima.exceptions.UltimaRuntimeError(\n  proc.returncode, errstream)\n   return source", "target": 1, "info": "Null", "idx": 0}
{"func": "def hstack(tup):\n  arrs = [cupy.atleast_1d(a) for a in tup]\n  axis = 1\n  if arrs[0].ndim == 1:\n  axis = 0\n  return concatenate(arrs, axis)", "target": 0, "info": "Null", "idx": 0}
{"func": "def hstack(tup):\n  arrs = [cupy.atleast_1d(a) for a in tup]\n  axis = 1\n  if arrs[0].ndim == 1:\n  axis = 0\n  return concatenate(tup, axis)", "target": 1, "info": "Null", "idx": 0}
{"func": " def check_type_backward(self, in_types, out_types):\n  type_check.expect(out_types.size() == 1)\n  x_type, = in_types\n  y_type, = out_types\n   type_check.expect(\n  x_type.dtype == y_type.dtype,\n  x_type.ndim == y_type.ndim,\n  x_type.shape == y_type.shape\n  )", "target": 0, "info": "Null", "idx": 0}
{"func": "  def check_type_backward(self, in_types, out_types):\n  type_check.expect(out_types.size() == 1)\n  x_type, = out_types\n  y_type, = out_types\n   type_check.expect(\n  x_type.dtype == y_type.dtype,\n  x_type.ndim == y_type.ndim,\n  x_type.shape == y_type.shape\n  )", "target": 1, "info": "Null", "idx": 0}
{"func": " def __init__(self, bus=0, device=0, speed=1000000, pin_rst=22,\n  pin_ce=0, pin_irq=18, pin_mode=GPIO.BOARD):\n  self.pin_rst = pin_rst\n  self.pin_ce = pin_ce\n  self.pin_irq = pin_irq\n  self.spi = spidev.SpiDev()\n  self.spi.open(bus, device)\n  self.spi.max_speed_hz = speed\n   GPIO.setmode(pin_mode)\n  if pin_rst is not None:\n  GPIO.setup(pin_rst, GPIO.OUT)\n  GPIO.output(pin_rst, 1)\n  GPIO.setup(pin_irq, GPIO.IN, pull_up_down=GPIO.PUD_UP)\n  GPIO.add_event_detect(pin_irq, GPIO.FALLING,\n  callback=self.irq_callback)\n  if pin_ce != 0:\n  GPIO.setup(pin_ce, GPIO.OUT)\n  GPIO.output(pin_ce, 1)\n  self.init()", "target": 0, "info": "Null", "idx": 0}
{"func": "   def __init__(self, bus=0, device=0, speed=1000000, pin_rst=22,\n  pin_ce=0, pin_irq=18, pin_mode=GPIO.BOARD):\n  self.pin_rst = pin_rst\n  self.pin_ce = pin_ce\n  self.pin_irq = pin_irq\n  self.spi = spidev.SpiDev()\n  self.spi.open(bus, device)\n  self.spi.max_speed_hz = speed\n   GPIO.setmode(pin_mode)\n  if pin_rst is None:\n  GPIO.setup(pin_rst, GPIO.OUT)\n  GPIO.output(pin_rst, 1)\n  GPIO.setup(pin_irq, GPIO.IN, pull_up_down=GPIO.PUD_UP)\n  GPIO.add_event_detect(pin_irq, GPIO.FALLING,\n  callback=self.irq_callback)\n  if pin_ce != 0:\n  GPIO.setup(pin_ce, GPIO.OUT)\n  GPIO.output(pin_ce, 1)\n  self.init()", "target": 1, "info": "Null", "idx": 0}
{"func": "def _yfinance_request(names, start_date=None, end_date=None):\n  try:\n  import yfinance as yf\n  except ImportError:\n  print(\n  \"The following package is required:\\n - `yfinance`\\n\"\n  + \"Please make sure that it is installed.\"\n  )\n  try:\n  import datetime\n  if isinstance(start_date, str):\n  start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n  if isinstance(end_date, str):\n  end_date = datetime.datetime.strptime(end_date, \"%Y-%m-%d\")\n  except ImportError:\n  print(\n  \"The following package is required:\\n - `datetime`\\n\"\n  + \"Please make sure that it is installed.\"\n  )\n  except Exception:\n  raise Exception(\"Please provide valid values for <start_date> and <end_date>\")\n  try:\n  resp = yf.download(names, start=start_date, end=end_date)\n  except Exception:\n  raise Exception(\"Error during download of stock data from Yahoo Finance with `yfinance`.\")\n  return resp", "target": 0, "info": "Null", "idx": 0}
{"func": "def _yfinance_request(names, start_date=None, end_date=None):\n  try:\n  import yfinance as yf\n  except ImportError:\n  print(\n  \"The following package is required:\\n - `yfinance`\\n\"\n  + \"Please make sure that it is installed.\"\n  )\n  try:\n  import datetime\n  if isinstance(start_date, str):\n  start_date = datetime.datetime.strptime(end_date, \"%Y-%m-%d\")\n  if isinstance(end_date, str):\n  end_date = datetime.datetime.strptime(end_date, \"%Y-%m-%d\")\n  except ImportError:\n  print(\n  \"The following package is required:\\n - `datetime`\\n\"\n  + \"Please make sure that it is installed.\"\n  )\n  except Exception:\n  raise Exception(\"Please provide valid values for <start_date> and <end_date>\")\n  try:\n  resp = yf.download(names, start=start_date, end=end_date)\n  except Exception:\n  raise Exception(\"Error during download of stock data from Yahoo Finance with `yfinance`.\")\n  return resp", "target": 1, "info": "Null", "idx": 0}
{"func": " def inject_line(self, line):\n  assert isinstance(line, Line)\n  self.recvq.put(line)", "target": 0, "info": "Null", "idx": 0}
{"func": "def inject_line(self, line):\n  assert isinstance(Line, line)\n  self.recvq.put(line)", "target": 1, "info": "Null", "idx": 0}
{"func": "def join(self, event):\n  target = event.target\n  channel = event.scope\n  modes = {m[0] for m in event.modes} if event.modes else set()\n  self.u_expire_timers.pop(target.nick, None)\n  user = self.get_user(target.nick)\n  if not user:\n  user = self.add_user(target.nick, username=target.username,\n   host=target.host, gecos=event.gecos,\n   account=event.account)\n  else:\n  self.update_username_host(target)\n   basicrfc = self.get_extension(\"BasicRFC\")\n   if self.casecmp(target.nick, basicrfc.nick):\n  isupport = self.get_extension(\"ISupport\")\n   params = [channel]\n  if isupport.get(\"WHOX\"):\n  num = ''.join(str(randint(0, 9)) for x in range(randint(1, 3)))\n  params.append(\"%tcuihsnflar,\" + num)\n  self.whox_send.append(num)\n  sched = self.schedule(2, partial(self.send, \"WHO\", params))\n  self.who_timers[channel] = sched\n  user.channels[channel] = modes", "target": 0, "info": "Null", "idx": 0}
{"func": "def join(self, event):\n  target = event.target\n  channel = event.scope\n  modes = {m[0] for m in event.modes} if event.modes else set()\n  self.u_expire_timers.pop(target.nick, None)\n  user = self.get_user(target.nick)\n  if not user:\n  user = self.add_user(target.nick, username=target.username,\n   host=target.host, gecos=event.gecos,\n   account=event.account)\n  else:\n  self.update_username_host(target)\n   basicrfc = self.get_extension(\"BasicRFC\")\n   if self.casecmp(user.nick, basicrfc.nick):\n  isupport = self.get_extension(\"ISupport\")\n   params = [channel]\n  if isupport.get(\"WHOX\"):\n  num = ''.join(str(randint(0, 9)) for x in range(randint(1, 3)))\n  params.append(\"%tcuihsnflar,\" + num)\n  self.whox_send.append(num)\n  sched = self.schedule(2, partial(self.send, \"WHO\", params))\n  self.who_timers[channel] = sched\n  user.channels[channel] = modes", "target": 1, "info": "Null", "idx": 0}
{"func": "def do_colour(self):\n  ret = []\n  if not (self.foreground and self.background):\n  ret.extend((self.fmt_resetforeground, self.fmt_resetbackground))\n  else:\n  if self.foreground is not None:\n  fgc = ColoursANSI[self.foreground.name].value\n  ret.append(str(fgc.foreground_16))\n  else:\n  ret.append(self.fmt_resetforeground)\n   if self.background is not None:\n  bgc = ColoursANSI[self.background.name].value\n  ret.append(str(bgc.background_16))\n  else:\n  ret.append(self.fmt_resetbackground)\n  return self.sgr.format(';'.join(ret))", "target": 0, "info": "Null", "idx": 0}
{"func": "def do_colour(self):\n  ret = []\n  if not (self.foreground and self.background):\n  ret.extend((self.fmt_resetforeground, self.fmt_resetbackground))\n  else:\n  if self.foreground is not None:\n  fgc = ColoursANSI[self.foreground.name].value\n  ret.append(str(fgc.foreground_16))\n  else:\n  ret.append(self.fmt_resetforeground)\n   if self.background is not None:\n  bgc = ColoursANSI[self.background.name].value\n  ret.append(str(fgc.background_16))\n  else:\n  ret.append(self.fmt_resetbackground)\n  return self.sgr.format(';'.join(ret))", "target": 1, "info": "Null", "idx": 0}
{"func": "def convert(self, case):\n  return IRCString(case, self)", "target": 0, "info": "Null", "idx": 0}
{"func": "def convert(self, case):\n  return IRCString(self, case)", "target": 1, "info": "Null", "idx": 0}
{"func": " def launch_task(self, task_obj, *args, **kwargs):\n  tid_obj = self.register_task(task_obj)\n  task_obj.run(*args, **kwargs)\n  return task_obj", "target": 0, "info": "Null", "idx": 0}
{"func": "def launch_task(self, task_obj, *args, **kwargs):\n  tid_obj = self.register_task(task_obj)\n  task_obj.run(*args, **kwargs)\n  return tid_obj", "target": 1, "info": "Null", "idx": 0}
{"func": " def setup_country(self):\n  if not hasattr(self, 'exchange') or self.exchange is None:\n  return\n  exch_country = find_country_for_exchange(self.exchange)\n  if hasattr(self, 'country') and self.country:\n  if self.country == exch_country:\n  return\n  self.country = exch_country", "target": 0, "info": "Null", "idx": 0}
{"func": "def setup_country(self):\n  if not hasattr(self, 'exchange') and self.exchange is None:\n  return\n  exch_country = find_country_for_exchange(self.exchange)\n  if hasattr(self, 'country') and self.country:\n  if self.country == exch_country:\n  return\n  self.country = exch_country", "target": 1, "info": "Null", "idx": 0}
{"func": "def setUp(self):\n  self.directory = tempfile.mkdtemp()\n  self.source_path = os.path.join(self.directory, '_test.py')\n  self.bc_path = importlib.util.cache_from_source(self.source_path)\n  with open(self.source_path, 'w') as file:\n  file.write('x = 123\\n')\n  self.source_path2 = os.path.join(self.directory, '_test2.py')\n  self.bc_path2 = importlib.util.cache_from_source(self.source_path2)\n  shutil.copyfile(self.source_path, self.source_path2)\n  self.subdirectory = os.path.join(self.directory, '_subdir')\n  os.mkdir(self.subdirectory)\n  self.source_path3 = os.path.join(self.subdirectory, '_test3.py')\n  shutil.copyfile(self.source_path, self.source_path3)\n  many_directories = [str(number) for number in range(1, 100)]\n  self.long_path = os.path.join(self.directory,\n    \"long\",\n    *many_directories)\n  os.makedirs(self.long_path)\n  self.source_path_long = os.path.join(self.long_path, '_test4.py')\n  shutil.copyfile(self.source_path, self.source_path_long)\n  self.bc_path_long = importlib.util.cache_from_source(\n  self.source_path_long\n  )", "target": 0, "info": "Null", "idx": 0}
{"func": "def setUp(self):\n  self.directory = tempfile.mkdtemp()\n  self.source_path = os.path.join(self.directory, '_test.py')\n  self.bc_path = importlib.util.cache_from_source(self.source_path)\n  with open(self.source_path, 'w') as file:\n  file.write('x = 123\\n')\n  self.source_path2 = os.path.join(self.directory, '_test2.py')\n  self.bc_path2 = importlib.util.cache_from_source(self.source_path2)\n  shutil.copyfile(self.source_path, self.source_path2)\n  self.subdirectory = os.path.join(self.directory, '_subdir')\n  os.mkdir(self.subdirectory)\n  self.source_path3 = os.path.join(self.subdirectory, '_test3.py')\n  shutil.copyfile(self.source_path, self.source_path3)\n  many_directories = [str(number) for number in range(1, 100)]\n  self.long_path = os.path.join(\"long\",\n    self.directory,\n    *many_directories)\n  os.makedirs(self.long_path)\n  self.source_path_long = os.path.join(self.long_path, '_test4.py')\n  shutil.copyfile(self.source_path, self.source_path_long)\n  self.bc_path_long = importlib.util.cache_from_source(\n  self.source_path_long\n  )", "target": 1, "info": "Null", "idx": 0}
{"func": "def prompt_hex(item):\n  if not isinstance(item, JConfigHex):\n  return\n  if not item.is_visible():\n  return\n  print('\\nCONFIG_{0}'.format(item.get_name()))\n  val = 'h'\n  while val == 'h' or val == '':\n  val = raw_input('{0} : '.format(item.get_prompt()))\n  if val == 'h':\n  print_help(item)\n  elif val == '':\n  val = item.get_default_value()\n  if val is not '':\n  item.set_user_value(val)\n  else:\n  print('No default value')\n  print_help(item)\n  else:\n  try:\n  item.set_user_value(val)\n  except ValueError as ve:\n  print(ve)\n  val = 'h'\n  print('entered value is {}\\n'.format(item.get_user_value()))", "target": 0, "info": "Null", "idx": 0}
{"func": "def prompt_hex(item):\n  if not isinstance(item, JConfigHex):\n  return\n  if item.is_visible():\n  return\n  print('\\nCONFIG_{0}'.format(item.get_name()))\n  val = 'h'\n  while val == 'h' or val == '':\n  val = raw_input('{0} : '.format(item.get_prompt()))\n  if val == 'h':\n  print_help(item)\n  elif val == '':\n  val = item.get_default_value()\n  if val is not '':\n  item.set_user_value(val)\n  else:\n  print('No default value')\n  print_help(item)\n  else:\n  try:\n  item.set_user_value(val)\n  except ValueError as ve:\n  print(ve)\n  val = 'h'\n  print('entered value is {}\\n'.format(item.get_user_value()))", "target": 1, "info": "Null", "idx": 0}
{"func": " def send_card(self, wxid, friend_or_mp):\n   card_name = friend_or_mp.nickname if isinstance(friend_or_mp, Chat) else friend_or_mp\n  logger.info('sending {} to {}: {}'.format(CARD, self, card_name))\n   return self.core.send(\n  self, content='<msg username=\"{}\" nickname=\"{}\"/>'.format(wxid, card_name),\n  msg_type=CARD, url=self.core.uris.send_msg, text=card_name\n  )", "target": 0, "info": "Null", "idx": 0}
{"func": " def send_card(self, wxid, friend_or_mp):\n   card_name = friend_or_mp.nickname if isinstance(Chat, friend_or_mp) else friend_or_mp\n  logger.info('sending {} to {}: {}'.format(CARD, self, card_name))\n   return self.core.send(\n  self, content='<msg username=\"{}\" nickname=\"{}\"/>'.format(wxid, card_name),\n  msg_type=CARD, url=self.core.uris.send_msg, text=card_name\n  )", "target": 1, "info": "Null", "idx": 0}
{"func": " def member(self):\n  from wxpy import Group\n   if isinstance(self.receiver, Group):\n  return self.receiver.self", "target": 0, "info": "Null", "idx": 0}
{"func": " def member(self):\n  from wxpy import Group\n   if isinstance(Group, self.receiver):\n  return self.receiver.self", "target": 1, "info": "Null", "idx": 0}
{"func": "def to_train(self) -> bool:\n  if self.name == \"compress\":\n  return True\n  return False", "target": 0, "info": "Null", "idx": 0}
{"func": "def to_train(self) -> bool:\n  if self.name == \"compress\":\n  return True\n  return True", "target": 1, "info": "Null", "idx": 0}
{"func": " def dumpPkl(obj,path): \twith open(path,'wb') as pf: \t\tpickle.dump(obj,pf)", "target": 0, "info": "Null", "idx": 0}
{"func": "def dumpPkl(obj,path): \twith open(path,'wb') as pf: \t\tpickle.dump(obj,path)", "target": 1, "info": "Null", "idx": 0}
{"func": "def PdfFileRead(file):\n  i = 0\n  text = ''\n  if 'fitz' in dir():\n  pdf_file = fitz.open(file)\n  while i < len(pdf_file):\n  text += pdf_file[i].getText('text')\n  i += 1\n  else:\n  pdf_file = open(file, 'rb')\n  pdf_reader = PyPDF2.PdfFileReader(pdf_file)\n  while i < pdf_reader.numPages:\n  payload = pdf_reader.getPage(i).extractText().replace('\\n', '')\n  text += payload.encode('ascii', 'ignore').decode('unicode_escape')\n  i += 1\n  return text", "target": 0, "info": "Null", "idx": 0}
{"func": "def PdfFileRead(file):\n  i = 0\n  text = ''\n  if 'fitz' in dir():\n  pdf_file = fitz.open(file)\n  while i < len(pdf_file):\n  text += pdf_file[i].getText('text')\n  i += 1\n  else:\n  pdf_file = open(file, 'rb')\n  pdf_reader = PyPDF2.PdfFileReader(file)\n  while i < pdf_reader.numPages:\n  payload = pdf_reader.getPage(i).extractText().replace('\\n', '')\n  text += payload.encode('ascii', 'ignore').decode('unicode_escape')\n  i += 1\n  return text", "target": 1, "info": "Null", "idx": 0}
{"func": "def dict_flatten(dict_in: dict):\n  dict_out = dict()\n  for k in list(dict_in.keys()):\n  if isinstance(dict_in[k], dict):\n  for kk, vv in dict_in[k].items():\n  dict_out[f'{k}:{kk}'] = vv\n  else:\n  dict_out[k] = dict_in[k]\n   return dict_out", "target": 0, "info": "Null", "idx": 0}
{"func": "def dict_flatten(dict_in: dict):\n  dict_out = dict()\n  for k in list(dict_in.keys()):\n  if isinstance(dict_in[k], dict):\n  for kk, vv in dict_in[k].items():\n  dict_out[f'{k}:{kk}'] = vv\n  else:\n  dict_out[k] = dict_in[k]\n   return dict_in", "target": 1, "info": "Null", "idx": 0}
{"func": "def __init__(self, path_to_config):\n  self.path_to_config = path_to_config\n   if not path_to_config.endswith(directory_separator):\n  self.path_to_config += directory_separator\n   self.path_to_config += PyFunceble.CONFIGURATION_FILENAME\n  try:\n  self._load_config_file()\n  self._install_iana_config()\n  self._install_psl_config()\n  except FileNotFoundError:\n  if \"PYFUNCEBLE_AUTO_CONFIGURATION\" not in environ:\n  while True:\n  response = input(\n  \"%s was not found.\\n\\ Install the default configuration in the current directory ? [y/n] \"\n  % (Style.BRIGHT + path_to_config + Style.RESET_ALL)\n  )\n  if isinstance(response, str):\n  if response.lower() == \"y\":\n  self._install_production_config()\n  self._load_config_file()\n  self._install_iana_config()\n  self._install_psl_config()\n  break\n  elif response.lower() == \"n\":\n  raise Exception(\"Unable to find the configuration file.\")\n  else:\n  self._install_production_config()\n  self._load_config_file()\n  self._install_iana_config()\n  self._install_psl_config()\n  for main_key in [\"domains\", \"hosts\", \"splited\"]:\n  PyFunceble.CONFIGURATION[\"outputs\"][main_key][\"directory\"] = Directory(\n  PyFunceble.CONFIGURATION[\"outputs\"][main_key][\"directory\"]\n  ).fix_path()\n  for main_key in [\"http_analytic\", \"logs\"]:\n  for key, value in PyFunceble.CONFIGURATION[\"outputs\"][main_key][\n  \"directories\"\n  ].items():\n  PyFunceble.CONFIGURATION[\"outputs\"][main_key][\"directories\"][\n  key\n  ] = Directory(\n  value\n  ).fix_path()\n  PyFunceble.CONFIGURATION[\"outputs\"][\"main\"] = Directory(\n  PyFunceble.CONFIGURATION[\"outputs\"][\"main\"]\n  ).fix_path()\n  PyFunceble.STATUS.update(PyFunceble.CONFIGURATION[\"status\"])\n  PyFunceble.OUTPUTS.update(PyFunceble.CONFIGURATION[\"outputs\"])\n  PyFunceble.HTTP_CODE.update(PyFunceble.CONFIGURATION[\"http_codes\"])\n  PyFunceble.LINKS.update(PyFunceble.CONFIGURATION[\"links\"])\n  PyFunceble.CONFIGURATION.update(\n  {\"done\": Fore.GREEN + \"\u2714\", \"error\": Fore.RED + \"\u2718\"}\n  )", "target": 0, "info": "Null", "idx": 0}
{"func": "def __init__(self, path_to_config):\n  self.path_to_config = path_to_config\n   if path_to_config.endswith(directory_separator):\n  self.path_to_config += directory_separator\n   self.path_to_config += PyFunceble.CONFIGURATION_FILENAME\n  try:\n  self._load_config_file()\n  self._install_iana_config()\n  self._install_psl_config()\n  except FileNotFoundError:\n  if \"PYFUNCEBLE_AUTO_CONFIGURATION\" not in environ:\n  while True:\n  response = input(\n  \"%s was not found.\\n\\ Install the default configuration in the current directory ? [y/n] \"\n  % (Style.BRIGHT + path_to_config + Style.RESET_ALL)\n  )\n  if isinstance(response, str):\n  if response.lower() == \"y\":\n  self._install_production_config()\n  self._load_config_file()\n  self._install_iana_config()\n  self._install_psl_config()\n  break\n  elif response.lower() == \"n\":\n  raise Exception(\"Unable to find the configuration file.\")\n  else:\n  self._install_production_config()\n  self._load_config_file()\n  self._install_iana_config()\n  self._install_psl_config()\n  for main_key in [\"domains\", \"hosts\", \"splited\"]:\n  PyFunceble.CONFIGURATION[\"outputs\"][main_key][\"directory\"] = Directory(\n  PyFunceble.CONFIGURATION[\"outputs\"][main_key][\"directory\"]\n  ).fix_path()\n  for main_key in [\"http_analytic\", \"logs\"]:\n  for key, value in PyFunceble.CONFIGURATION[\"outputs\"][main_key][\n  \"directories\"\n  ].items():\n  PyFunceble.CONFIGURATION[\"outputs\"][main_key][\"directories\"][\n  key\n  ] = Directory(\n  value\n  ).fix_path()\n  PyFunceble.CONFIGURATION[\"outputs\"][\"main\"] = Directory(\n  PyFunceble.CONFIGURATION[\"outputs\"][\"main\"]\n  ).fix_path()\n  PyFunceble.STATUS.update(PyFunceble.CONFIGURATION[\"status\"])\n  PyFunceble.OUTPUTS.update(PyFunceble.CONFIGURATION[\"outputs\"])\n  PyFunceble.HTTP_CODE.update(PyFunceble.CONFIGURATION[\"http_codes\"])\n  PyFunceble.LINKS.update(PyFunceble.CONFIGURATION[\"links\"])\n  PyFunceble.CONFIGURATION.update(\n  {\"done\": Fore.GREEN + \"\u2714\", \"error\": Fore.RED + \"\u2718\"}\n  )", "target": 1, "info": "Null", "idx": 0}
{"func": "def save_to_env_file(cls, environment_variables, env_file_path):\n  if environment_variables:\n  file_instance = PyFunceble.helpers.File(env_file_path)\n  try:\n  content = file_instance.read()\n  except FileNotFoundError:\n  content = \"\"\n  for name, value in environment_variables.items():\n  to_write = f\"{name}={value}\"\n  regex = f\"{name}=.*\"\n   if not content:\n  content = f\"{to_write}\\n\"\n  continue\n   if PyFunceble.helpers.Regex(f\"^{regex}\").get_matching_list(\n  content.splitlines()\n  ):\n  content = PyFunceble.helpers.Regex(regex).replace_match(\n  content, to_write\n  )\n  continue\n  if not content.endswith(\"\\n\"):\n  content += f\"\\n{to_write}\\n\"\n  continue\n  content += f\"{to_write}\\n\"\n  file_instance.write(content, overwrite=True)", "target": 0, "info": "Null", "idx": 0}
{"func": "def save_to_env_file(cls, environment_variables, env_file_path):\n  if environment_variables:\n  file_instance = PyFunceble.helpers.File(env_file_path)\n  try:\n  content = file_instance.read()\n  except FileNotFoundError:\n  content = \"\"\n  for name, value in environment_variables.items():\n  to_write = f\"{name}={value}\"\n  regex = f\"{name}=.*\"\n   if not content:\n  content += f\"{to_write}\\n\"\n  continue\n   if PyFunceble.helpers.Regex(f\"^{regex}\").get_matching_list(\n  content.splitlines()\n  ):\n  content = PyFunceble.helpers.Regex(regex).replace_match(\n  content, to_write\n  )\n  continue\n  if not content.endswith(\"\\n\"):\n  content += f\"\\n{to_write}\\n\"\n  continue\n  content += f\"{to_write}\\n\"\n  file_instance.write(content, overwrite=True)", "target": 1, "info": "Null", "idx": 0}
{"func": "def __init__(self):\n  is_cloned_version = PyFunceble.abstracts.Version.is_local_cloned()\n  destination_directory = (\n  f\"{PyFunceble.CONFIG_DIRECTORY}\"\n  f\"{PyFunceble.CONFIGURATION.outputs.db_type.directory}\"\n  )\n  if not destination_directory.endswith(directory_separator):\n  destination_directory += directory_separator\n  destination_dir_instance = PyFunceble.helpers.Directory(destination_directory)\n  not_supported_db_types = [\"json\"]\n  self.destination = (\n  f\"{destination_directory}\"\n  f\"{PyFunceble.OUTPUTS.db_type.files[PyFunceble.CONFIGURATION.db_type]}\"\n  )\n   if not is_cloned_version and (\n  PyFunceble.CONFIGURATION.db_type not in not_supported_db_types\n  ):\n  destination_dir_instance.delete()\n  if PyFunceble.CONFIGURATION.db_type not in not_supported_db_types:\n  destination_dir_instance.create()\n  self.DOWNTIME_INDEX += f\"_{PyFunceble.CONFIGURATION.db_type}\"\n  self.download_link = PyFunceble.converter.InternalUrl(\n  PyFunceble.CONFIGURATION.links[PyFunceble.CONFIGURATION.db_type]\n  ).get_converted()\n  super().__init__()\n  self.process()", "target": 0, "info": "Null", "idx": 0}
{"func": "def __init__(self):\n  is_cloned_version = PyFunceble.abstracts.Version.is_local_cloned()\n  destination_directory = (\n  f\"{PyFunceble.CONFIG_DIRECTORY}\"\n  f\"{PyFunceble.CONFIGURATION.outputs.db_type.directory}\"\n  )\n  if not destination_directory.endswith(directory_separator):\n  destination_directory += directory_separator\n  destination_dir_instance = PyFunceble.helpers.Directory(destination_directory)\n  not_supported_db_types = [\"json\"]\n  self.destination = (\n  f\"{destination_directory}\"\n  f\"{PyFunceble.OUTPUTS.db_type.files[PyFunceble.CONFIGURATION.db_type]}\"\n  )\n   if is_cloned_version and (\n  PyFunceble.CONFIGURATION.db_type not in not_supported_db_types\n  ):\n  destination_dir_instance.delete()\n  if PyFunceble.CONFIGURATION.db_type not in not_supported_db_types:\n  destination_dir_instance.create()\n  self.DOWNTIME_INDEX += f\"_{PyFunceble.CONFIGURATION.db_type}\"\n  self.download_link = PyFunceble.converter.InternalUrl(\n  PyFunceble.CONFIGURATION.links[PyFunceble.CONFIGURATION.db_type]\n  ).get_converted()\n  super().__init__()\n  self.process()", "target": 1, "info": "Null", "idx": 0}
{"func": "def special_blogspot(self):\n  regex_blogspot = \".blogspot.\"\n  regex_blogger = [\"create-blog.g?\", \"87065\", \"doesn&\n   if self.tested == PyFunceble.CONFIGURATION[\"domain\"]:\n  url_to_get = \"http://%s\" % self.tested\n  else:\n  url_to_get = self.tested\n   if Regex(self.tested, regex_blogspot, return_data=False, escape=True).match():\n  blogger_content_request = requests.get(url_to_get)\n  for regx in regex_blogger:\n  if regx in blogger_content_request.text or Regex(\n  blogger_content_request.text, regx, return_data=False, escape=False\n  ).match():\n  self.source = \"SPECIAL\"\n  self.domain_status = PyFunceble.STATUS[\"official\"][\"down\"]\n  self.output = self.output_parent_dir + PyFunceble.OUTPUTS[\n  \"splited\"\n  ][\n  \"directory\"\n  ] + self.domain_status\n  break", "target": 0, "info": "Null", "idx": 0}
{"func": "def special_blogspot(self):\n  regex_blogspot = \".blogspot.\"\n  regex_blogger = [\"create-blog.g?\", \"87065\", \"doesn&\n   if self.tested == PyFunceble.CONFIGURATION[\"domain\"]:\n  url_to_get = \"http://%s\" & self.tested\n  else:\n  url_to_get = self.tested\n   if Regex(self.tested, regex_blogspot, return_data=False, escape=True).match():\n  blogger_content_request = requests.get(url_to_get)\n  for regx in regex_blogger:\n  if regx in blogger_content_request.text or Regex(\n  blogger_content_request.text, regx, return_data=False, escape=False\n  ).match():\n  self.source = \"SPECIAL\"\n  self.domain_status = PyFunceble.STATUS[\"official\"][\"down\"]\n  self.output = self.output_parent_dir + PyFunceble.OUTPUTS[\n  \"splited\"\n  ][\n  \"directory\"\n  ] + self.domain_status\n  break", "target": 1, "info": "Null", "idx": 0}
{"func": "def __init__(self, list_to_test, clean_all=False):\n  if list_to_test:\n  try:\n  number_of_tested = PyFunceble.CONFIGURATION[\"counter\"][\"number\"][\n  \"tested\"\n  ]\n  if (\n  number_of_tested == 0\n  or list_to_test[number_of_tested - 1] == list_to_test[-1]\n  or number_of_tested >= len(list_to_test)\n  ):\n  Core.reset_counters()\n  self.almost_everything(clean_all)\n  except IndexError:\n  Core.reset_counters()\n  self.almost_everything(clean_all)\n  else:\n  self.almost_everything(clean_all)", "target": 0, "info": "Null", "idx": 0}
{"func": "def __init__(self, list_to_test, clean_all=False):\n  if list_to_test:\n  try:\n  number_of_tested = PyFunceble.CONFIGURATION[\"counter\"][\"number\"][\n  \"tested\"\n  ]\n  if (\n  number_of_tested == 0\n  or list_to_test[number_of_tested - 1] == list_to_test[-1]\n  or number_of_tested == len(list_to_test)\n  ):\n  Core.reset_counters()\n  self.almost_everything(clean_all)\n  except IndexError:\n  Core.reset_counters()\n  self.almost_everything(clean_all)\n  else:\n  self.almost_everything(clean_all)", "target": 1, "info": "Null", "idx": 0}
{"func": "def process_view(self, request, view_func, view_args, view_kwargs):\n  try:\n  realm = view_func._rated_realm\n  except AttributeError:\n  try:\n  realm = settings.REALM_MAP[request.resolver_match.url_name]\n  except KeyError:\n  return None\n  conf = settings.REALMS.get(realm, {})\n  key = 'rated:%s:%s' % (realm, request.META['REMOTE_ADDR'],)\n  now = time.time()\n  client = redis.Redis(connection_pool=POOL)\n  pipe = client.pipeline()\n  pipe.zadd(key, now, now)\n  pipe.expireat(key, now + conf.get('timeout', settings.DEFAULT_TIMEOUT))\n  pipe.zremrangebyscore(key, '-inf', now - settings.DEFAULT_TIMEOUT)\n  pipe.zcard(key)\n  size = pipe.execute()[-1]\n  if size > conf.get('limit', settings.DEFAULT_LIMIT):\n  return HttpResponse(status=501)\n  return None", "target": 0, "info": "Null", "idx": 0}
{"func": "def process_view(self, request, view_func, view_args, view_kwargs):\n  try:\n  realm = request._rated_realm\n  except AttributeError:\n  try:\n  realm = settings.REALM_MAP[request.resolver_match.url_name]\n  except KeyError:\n  return None\n  conf = settings.REALMS.get(realm, {})\n  key = 'rated:%s:%s' % (realm, request.META['REMOTE_ADDR'],)\n  now = time.time()\n  client = redis.Redis(connection_pool=POOL)\n  pipe = client.pipeline()\n  pipe.zadd(key, now, now)\n  pipe.expireat(key, now + conf.get('timeout', settings.DEFAULT_TIMEOUT))\n  pipe.zremrangebyscore(key, '-inf', now - settings.DEFAULT_TIMEOUT)\n  pipe.zcard(key)\n  size = pipe.execute()[-1]\n  if size > conf.get('limit', settings.DEFAULT_LIMIT):\n  return HttpResponse(status=501)\n  return None", "target": 1, "info": "Null", "idx": 0}
{"func": "def load_template(name, paths=None, raw=False):\n  if paths is None:\n  paths = PATHS[:]\n  for path in paths:\n  full_name = os.path.abspath(os.path.join(path, name))\n  if not full_name.startswith(path):\n  continue\n  try:\n  with open(full_name, encoding='utf-8') as fin:\n  src = fin.read()\n   return kompile(src, raw=raw, filename=full_name)\n  except FileNotFoundError:\n  pass\n  else:\n  raise TemplateNotFound(name)", "target": 0, "info": "Null", "idx": 0}
{"func": "def load_template(name, paths=None, raw=False):\n  if paths is None:\n  paths = PATHS[:]\n  for path in paths:\n  full_name = os.path.abspath(os.path.join(path, name))\n  if not full_name.startswith(path):\n  continue\n  try:\n  with open(full_name, encoding='utf-8') as fin:\n  src = fin.read()\n   return kompile(src, raw=raw, filename=name)\n  except FileNotFoundError:\n  pass\n  else:\n  raise TemplateNotFound(name)", "target": 1, "info": "Null", "idx": 0}
{"func": "def elfinder(request):\n  options = default_settings.ELFINDER_DEFAULT_OPTIONS.copy()\n  options['url'] = reverse('cked_elfinder_connector')\n   user_options = getattr(settings, 'ELFINDER_OPTIONS', None)\n   if options != None:\n  if isinstance(options, dict):\n  user_options.update(options)\n  else:\n  raise ImproperlyConfigured('CKEDITOR_OPTIONS setting must be a '\n 'dictionary type.')\n   return render(request, 'cked/elfinder.html', {\n  'options': json_encode(user_options),\n  })", "target": 0, "info": "Null", "idx": 0}
{"func": "def elfinder(request):\n  options = default_settings.ELFINDER_DEFAULT_OPTIONS.copy()\n  options['url'] = reverse('cked_elfinder_connector')\n   user_options = getattr(settings, 'ELFINDER_OPTIONS', None)\n   if options != None:\n  if isinstance(options, dict):\n  user_options.update(options)\n  else:\n  raise ImproperlyConfigured('CKEDITOR_OPTIONS setting must be a '\n 'dictionary type.')\n   return render(request, 'cked/elfinder.html', {\n  'options': json_encode(options),\n  })", "target": 1, "info": "Null", "idx": 0}
{"func": " async def set(self):\n  info = ParamsQueryInfo.new(self, self.params, self.ability)\n  self._check_handle_result(await async_call(self.handle_query, info))\n  if self.is_finished: return\n  raw_post = await self.post_data()\n  values = await self._post_data_check(info, raw_post, A.WRITE)\n  if self.is_finished: return\n  self._check_handle_result(await async_call(self.before_update, raw_post, values))\n  if self.is_finished: return\n  logger.debug('set data: %s' % values)\n  code, data = await self._sql.update(info, values)\n  if code == RETCODE.SUCCESS:\n  await async_call(self.after_update, values)\n  self.finish(code, data)", "target": 0, "info": "Null", "idx": 0}
{"func": "async def set(self):\n  info = ParamsQueryInfo.new(self, self.params, self.ability)\n  self._check_handle_result(await async_call(self.handle_query, info))\n  if self.is_finished: return\n  raw_post = await self.post_data()\n  values = await self._post_data_check(info, raw_post, A.WRITE)\n  if self.is_finished: return\n  self._check_handle_result(await async_call(self.before_update, raw_post, values))\n  if self.is_finished: return\n  logger.debug('set data: %s' % values)\n  code, data = await self._sql.update(info, values)\n  if code == RETCODE.SUCCESS:\n  await async_call(self.after_update, data)\n  self.finish(code, data)", "target": 1, "info": "Null", "idx": 0}
{"func": "async def __(view: AbstractSQLView, *args, **kwargs):\n  if role != view.current_request_role:\n  return view.finish(RETCODE.INVALID_ROLE)\n  return await func(view, *args, **kwargs)\n  return __", "target": 0, "info": "Null", "idx": 0}
{"func": "def _(func):\n  async def __(view: AbstractSQLView, *args, **kwargs):\n  if role == view.current_request_role:\n  return view.finish(RETCODE.INVALID_ROLE)\n  return await func(view, *args, **kwargs)\n  return __", "target": 1, "info": "Null", "idx": 0}
{"func": "def waitOrgCreated(self, org_id):\n   org_info = wa.organisms.show_organism(org_id)\n  if 'directory' not in org_info:\n  time.sleep(1)\n  org_info = wa.organisms.show_organism(org_id)\n   return org_info", "target": 0, "info": "Null", "idx": 0}
{"func": "def waitOrgCreated(self, org_id):\n   org_info = wa.organisms.show_organism(org_id)\n  if 'directory' in org_info:\n  time.sleep(1)\n  org_info = wa.organisms.show_organism(org_id)\n   return org_info", "target": 1, "info": "Null", "idx": 0}
{"func": " def create_model(self, model_id, model_name, description, author, tokenizer_exceptions_path, analyzer_rule_set):\n  if self.get_model(model_id) is not None:\n  Logger.log('L-0019')\n  return False\n  try:\n  Logger.log('L-0021')\n  custom_model = self.__initialize_custom_model()\n  new_model = Model(model_id, model_name, description, author, model_id, analyzer_rule_set)\n  new_model.set_reference(custom_model)\n  Logger.log('L-0022')\n  self.__apply_tokenizer_exceptions(new_model, tokenizer_exceptions_path)\n  ModelDataManager.save_model_data(model_id, model_name, description, author, model_id, analyzer_rule_set)\n  ModelLoader.save_model(custom_model, model_id, tokenizer_exceptions_path)\n  self.__models.append(new_model)\n  Logger.log('L-0025')\n  return True\n  except Exception as e:\n  Logger.log('L-0020', [{'text': e, 'color': ERROR_COLOR}])\n  return False", "target": 0, "info": "Null", "idx": 0}
{"func": "def create_model(self, model_id, model_name, description, author, tokenizer_exceptions_path, analyzer_rule_set):\n  if self.get_model(model_id) is not None:\n  Logger.log('L-0019')\n  return False\n  try:\n  Logger.log('L-0021')\n  custom_model = self.__initialize_custom_model()\n  new_model = Model(model_id, model_name, description, author, model_name, analyzer_rule_set)\n  new_model.set_reference(custom_model)\n  Logger.log('L-0022')\n  self.__apply_tokenizer_exceptions(new_model, tokenizer_exceptions_path)\n  ModelDataManager.save_model_data(model_id, model_name, description, author, model_id, analyzer_rule_set)\n  ModelLoader.save_model(custom_model, model_id, tokenizer_exceptions_path)\n  self.__models.append(new_model)\n  Logger.log('L-0025')\n  return True\n  except Exception as e:\n  Logger.log('L-0020', [{'text': e, 'color': ERROR_COLOR}])\n  return False", "target": 1, "info": "Null", "idx": 0}
{"func": " def __process_tokenizer_results(self, doc, only_positives=False):\n  results = list([])\n  if doc is None:\n  return results\n  token_analyzer = Analyzer(self.__analyzer_rules_set)\n  for sent in doc.sents:\n  for token in sent:\n  generated_token = Token(token.lemma_, token.is_oov, token.pos_, token.sent, token.sentiment, token.tag_, token.text)\n  token_analyzer.analyze_token(generated_token)\n  if not only_positives or generated_token.is_positive():\n  results.append(generated_token)\n  return results", "target": 0, "info": "Null", "idx": 0}
{"func": "def __process_tokenizer_results(self, doc, only_positives=False):\n  results = list([])\n  if doc is None:\n  return results\n  token_analyzer = Analyzer(self.__analyzer_rules_set)\n  for sent in doc.sents:\n  for token in sent:\n  generated_token = Token(token.lemma_, token.is_oov, token.pos_, token.sent, token.sentiment, token.tag_, sent.text)\n  token_analyzer.analyze_token(generated_token)\n  if not only_positives or generated_token.is_positive():\n  results.append(generated_token)\n  return results", "target": 1, "info": "Null", "idx": 0}
{"func": "def estimate_stem_wmvnp(features_train,\n  responses_train,\n  features_test=None,\n  responses_test=None,\n  ridges=np.logspace(0,3,10),\n  normalize_hyparams=False,\n  normalize_kernel=False,\n  temporal_prior=None,\n  feature_priors=None,\n  weights=False,\n  predictions=False,\n  performance=False,\n  folds=(1,5),\n  method='SVD',\n  verbosity=1,\n  cvresults=None,\n  population_optimal=False,\n  population_mean=False,\n  chunklen=True,\n  ):\n  delays = temporal_prior.delays\n  ndelays = len(delays)\n  if features_test is None:\n  features_test = [features_test]*len(features_train)\n  if cvresults is None:\n  cvresults = crossval_stem_wmvnp(features_train,\n  responses_train,\n  ridges=ridges,\n  normalize_hyparams=normalize_hyparams,\n  normalize_kernel=normalize_kernel,\n  temporal_prior=temporal_prior,\n  feature_priors=feature_priors,\n  population_mean=population_mean,\n  folds=folds,\n  method=method,\n  verbosity=verbosity,\n  chunklen=chunklen,\n  )\n  if (weights is False) and (performance is False) and (prediction is False):\n  return cvresults\n  dims = cvresults['dims']\n  cvmean = cvresults['cvresults'].mean(0)\n  if population_optimal is True and (dims.nresponses > 1):\n  cvmean = np.nan_to_num(cvmean).mean(-1)[...,None]\n  nresponses = int(dims.nresponses)\n  nfspaces = int(dims.nfspaces)\n  ntspaces = 1\n  ncvresponses = 1 if population_mean else nresponses\n  optima = np.zeros((ncvresponses, nfspaces + ntspaces + 1))\n  for idx in range(ncvresponses):\n  temporal_opt, spatial_opt, ridge_opt = find_optimum_mvn(cvmean[...,idx],\n  cvresults['temporal'],\n  cvresults['spatial'],\n  cvresults['ridges'])\n  optima[idx] = tuple([temporal_opt])+tuple(spatial_opt)+tuple([ridge_opt])\n  cvresults['optima'] = optima\n  unique_optima = np.vstack(set(tuple(row) for row in optima))\n  solutions = [[]]*nresponses\n  for idx in range(unique_optima.shape[0]):\n  uopt = unique_optima[idx][0], unique_optima[idx][1:-1], unique_optima[idx][-1]\n  temporal_opt, spatial_opt, ridge_opt = uopt\n  if population_mean:\n  train_responses = responses_train\n  test_responses = responses_test\n  else:\n  responses_mask = np.asarray([np.allclose(row, unique_optima[idx]) for row in optima])\n  train_responses = responses_train[:, responses_mask]\n  test_responses = None if responses_test is None else responses_test[:, responses_mask]\n  response_solution = estimate_simple_stem_wmvnp(features_train,\n train_responses,\n features_test=features_test,\n responses_test=test_responses,\n temporal_prior=temporal_prior,\n temporal_hhparam=temporal_opt,\n feature_priors=feature_priors,\n feature_hyparams=spatial_opt,\n weights=weights,\n performance=performance,\n predictions=predictions,\n ridge_scale=ridge_opt,\n verbosity=verbosity,\n method=method,\n )\n  if population_mean:\n  solutions = response_solution\n  else:\n  for rdx, response_index in enumerate(responses_mask.nonzero()[0]):\n  solutions[response_index] = {k:v[...,rdx] for k,v in response_solution.items()}\n  if verbosity:\n  if population_mean:\n  itxt = '%i responses:'%(nresponses)\n  else:\n  itxt = '%i responses:'%(responses_mask.sum())\n  ttxt = \"ridge=%9.03f, temporal=%0.03f,\" % (ridge_opt, temporal_opt)\n  stxt = \"spatial=(\"\n  stxt += ', '.join([\"%0.03f\"]*(len(spatial_opt)))\n  stxt = stxt%tuple(spatial_opt) + ')'\n  perf = 'perf=%0.04f'%response_solution['performance'].mean()\n  print(' '.join([itxt, ttxt, stxt, perf]))\n  if population_mean:\n  for k,v in solutions.items():\n  cvresults[k] = v\n  else:\n  fits = ddict(list)\n  for solution in solutions:\n  for k,v in solution.items():\n  fits[k].append(v)\n  for k,v in fits.items():\n  v = np.asarray(v).T\n  cvresults[k] = v\n  del fits, solutions\n  return cvresults", "target": 0, "info": "Null", "idx": 0}
{"func": "def estimate_stem_wmvnp(features_train,\n  responses_train,\n  features_test=None,\n  responses_test=None,\n  ridges=np.logspace(0,3,10),\n  normalize_hyparams=False,\n  normalize_kernel=False,\n  temporal_prior=None,\n  feature_priors=None,\n  weights=False,\n  predictions=False,\n  performance=False,\n  folds=(1,5),\n  method='SVD',\n  verbosity=1,\n  cvresults=None,\n  population_optimal=False,\n  population_mean=False,\n  chunklen=True,\n  ):\n  delays = temporal_prior.delays\n  ndelays = len(delays)\n  if features_test is None:\n  features_test = [features_test]*len(features_train)\n  if cvresults is None:\n  cvresults = crossval_stem_wmvnp(features_train,\n  responses_train,\n  ridges=ridges,\n  normalize_hyparams=normalize_hyparams,\n  normalize_kernel=normalize_kernel,\n  temporal_prior=temporal_prior,\n  feature_priors=feature_priors,\n  population_mean=population_mean,\n  folds=folds,\n  method=method,\n  verbosity=verbosity,\n  chunklen=chunklen,\n  )\n  if (weights is False) and (performance is False) and (prediction is False):\n  return cvresults\n  dims = cvresults['dims']\n  cvmean = cvresults['cvresults'].mean(0)\n  if population_optimal is True and (dims.nresponses > 1):\n  cvmean = np.nan_to_num(cvmean).mean(-1)[...,None]\n  nresponses = int(dims.nresponses)\n  nfspaces = int(dims.nfspaces)\n  ntspaces = 1\n  ncvresponses = 1 if population_mean else nresponses\n  optima = np.zeros((ncvresponses, nfspaces + ntspaces + 1))\n  for idx in range(ncvresponses):\n  temporal_opt, spatial_opt, ridge_opt = find_optimum_mvn(cvmean[...,idx],\n  cvresults['temporal'],\n  cvresults['spatial'],\n  cvresults['ridges'])\n  optima[idx] = tuple([temporal_opt])+tuple(spatial_opt)+tuple([ridge_opt])\n  cvresults['optima'] = optima\n  unique_optima = np.vstack(set(tuple(row) for row in optima))\n  solutions = [[]]*nresponses\n  for idx in range(unique_optima.shape[0]):\n  uopt = unique_optima[idx][0], unique_optima[idx][1:-1], unique_optima[idx][-1]\n  temporal_opt, spatial_opt, ridge_opt = uopt\n  if population_mean:\n  train_responses = responses_train\n  test_responses = responses_test\n  else:\n  responses_mask = np.asarray([np.allclose(row, unique_optima[idx]) for row in optima])\n  train_responses = responses_train[:, responses_mask]\n  test_responses = None if responses_test is None else responses_test[:, responses_mask]\n  response_solution = estimate_simple_stem_wmvnp(features_train,\n train_responses,\n features_test=features_test,\n responses_test=test_responses,\n temporal_prior=temporal_prior,\n temporal_hhparam=temporal_opt,\n feature_priors=feature_priors,\n feature_hyparams=spatial_opt,\n weights=weights,\n performance=weights,\n predictions=predictions,\n ridge_scale=ridge_opt,\n verbosity=verbosity,\n method=method,\n )\n  if population_mean:\n  solutions = response_solution\n  else:\n  for rdx, response_index in enumerate(responses_mask.nonzero()[0]):\n  solutions[response_index] = {k:v[...,rdx] for k,v in response_solution.items()}\n  if verbosity:\n  if population_mean:\n  itxt = '%i responses:'%(nresponses)\n  else:\n  itxt = '%i responses:'%(responses_mask.sum())\n  ttxt = \"ridge=%9.03f, temporal=%0.03f,\" % (ridge_opt, temporal_opt)\n  stxt = \"spatial=(\"\n  stxt += ', '.join([\"%0.03f\"]*(len(spatial_opt)))\n  stxt = stxt%tuple(spatial_opt) + ')'\n  perf = 'perf=%0.04f'%response_solution['performance'].mean()\n  print(' '.join([itxt, ttxt, stxt, perf]))\n  if population_mean:\n  for k,v in solutions.items():\n  cvresults[k] = v\n  else:\n  fits = ddict(list)\n  for solution in solutions:\n  for k,v in solution.items():\n  fits[k].append(v)\n  for k,v in fits.items():\n  v = np.asarray(v).T\n  cvresults[k] = v\n  del fits, solutions\n  return cvresults", "target": 1, "info": "Null", "idx": 0}
{"func": "def _tagReplace(soup, args = {}): \twrap_with_i = [ \t\tsoup.find_all(\"div\"), \t\tsoup.find_all(\"span\"), \t\tsoup.find_all(\"p\"), \t] \tfor l in wrap_with_i: \t\tfor item in l: \t\t\tattrs = str(item.attrs) \t\t\tattrs = attrs.replace(\": \", \":\") \t\t\tif 'font-style:italic' in attrs: \t\t\t\twrapper = fact().new_tag(\"i\") \t\t\t\twrapper.append(_copyB(item)) \t\t\t\titem.replace_with(wrapper) \twrap_with_p = [ \t\tsoup.find_all(\"div\", class_=\"article-paragraph\"), \t\tsoup.find_all(\"section\"), \t\t[x for x in soup.children if isinstance(x, str)], \t] \tfor l in wrap_with_p: \t\tfor item in l: \t\t\twrapper = fact().new_tag(\"p\") \t\t\twrapper.append(_copyB(item)) \t\t\titem.replace_with(wrapper) \tfor l in soup.find_all(\"p\"): \t\tchildren = list(l.children) \t\tif len(children) != 1 or not isinstance(children[0], str): \t\t\tcontinue \t\tto_replace = None \t\tif l.parent.name == 'blockquote':", "target": 0, "info": "Null", "idx": 0}
{"func": "def _tagReplace(soup, args = {}): \twrap_with_i = [ \t\tsoup.find_all(\"div\"), \t\tsoup.find_all(\"span\"), \t\tsoup.find_all(\"p\"), \t] \tfor l in wrap_with_i: \t\tfor item in l: \t\t\tattrs = str(item.attrs) \t\t\tattrs = attrs.replace(\": \", \":\") \t\t\tif 'font-style:italic' in attrs: \t\t\t\twrapper = fact().new_tag(\"i\") \t\t\t\twrapper.append(_copyB(item)) \t\t\t\titem.replace_with(wrapper) \twrap_with_p = [ \t\tsoup.find_all(\"div\", class_=\"article-paragraph\"), \t\tsoup.find_all(\"section\"), \t\t[x for x in soup.children if isinstance(x, str)], \t] \tfor l in wrap_with_p: \t\tfor item in l: \t\t\twrapper = fact().new_tag(\"p\") \t\t\twrapper.append(_copyB(item)) \t\t\titem.replace_with(wrapper) \tfor l in soup.find_all(\"p\"): \t\tchildren = list(l.children) \t\tif len(children) != 1 or not isinstance(children[0], str): \t\t\tcontinue \t\tto_replace = None \t\tif l.parent.name != 'blockquote':", "target": 1, "info": "Null", "idx": 0}
{"func": " def __next__(self):\n  row = next(self._consumer)\n  decoded_row = unpackb(row.value)\n  if self._end_time:\n  self.verbose(\"searching forward from:\")\n  self.verbose(decoded_row)\n  count = 0\n  while True:\n  count += 1\n  decoded_time = decoded_row[self._time_column]\n  decoded_time = self.decode_item(decoded_time)\n  decoded_time = self.parse_time(decoded_time)\n  if decoded_time >= self._kafka_end_time:\n  self.verbose(\"kafka end time reached: \" + str(count)+ \" rows\")\n  raise StopIteration()\n  if decoded_time >= self._begin_time and decoded_time <= self._end_time:\n  self.verbose(\"row found after\" + str(count)+ \" rows\")\n  return decoded_row\n  row = next(self._consumer)\n  decoded_row = unpackb(row.value)\n   return decoded_row", "target": 0, "info": "Null", "idx": 0}
{"func": " def __next__(self):\n  row = next(self._consumer)\n  decoded_row = unpackb(row.value)\n  if self._end_time:\n  self.verbose(\"searching forward from:\")\n  self.verbose(decoded_row)\n  count += 0\n  while True:\n  count += 1\n  decoded_time = decoded_row[self._time_column]\n  decoded_time = self.decode_item(decoded_time)\n  decoded_time = self.parse_time(decoded_time)\n  if decoded_time >= self._kafka_end_time:\n  self.verbose(\"kafka end time reached: \" + str(count)+ \" rows\")\n  raise StopIteration()\n  if decoded_time >= self._begin_time and decoded_time <= self._end_time:\n  self.verbose(\"row found after\" + str(count)+ \" rows\")\n  return decoded_row\n  row = next(self._consumer)\n  decoded_row = unpackb(row.value)\n   return decoded_row", "target": 1, "info": "Null", "idx": 0}
{"func": " def gather(self, count, row, match, enrichment_data):\n  if self._enrichment_key not in enrichment_data:\n  return (None, None)\n  conns = {}\n  ports = {}\n  for row in enrichment_data[self._enrichment_key]:\n  orig = row[self._orig_key]\n  resp = row[self._resp_key]\n  port = row[self._port_key]\n  if orig not in conns:\n  conns[orig] = {resp: {port: 1}}\n  continue\n  if resp not in conns[orig]:\n  conns[orig][resp] = {port: 1}\n  continue\n  if port not in conns[orig]:\n  conns[orig][resp][port] = 1\n  else:\n  conns[orig][resp][port] += 1\n  if port not in ports:\n  ports[port] = 1\n  else:\n  ports[port] += 1\n   results = {'connections': conns,\n 'ports': ports}\n  return (self._output_key, results)", "target": 0, "info": "Null", "idx": 0}
{"func": "def gather(self, count, row, match, enrichment_data):\n  if self._enrichment_key not in enrichment_data:\n  return (None, None)\n  conns = {}\n  ports = {}\n  for row in enrichment_data[self._enrichment_key]:\n  orig = row[self._orig_key]\n  resp = row[self._resp_key]\n  port = row[self._port_key]\n  if orig not in conns:\n  conns[orig] = {resp: {port: 1}}\n  continue\n  if resp not in conns[orig]:\n  conns[orig][resp] = {port: 1}\n  continue\n  if port not in conns[orig]:\n  conns[orig][resp][port] = 1\n  else:\n  conns[orig][resp][port] += 1\n  if port not in ports:\n  ports[port] = 1\n  else:\n  ports[port] += 1\n   results = {'connections': conns,\n 'ports': ports}\n  return (self._output_key, conns)", "target": 1, "info": "Null", "idx": 0}
{"func": "def read(self, max_records = None):\n  array = []\n  dictionary = {}\n  index_column = self.index_column_number()\n  for (count,entry) in enumerate(self._tfh):\n  array.append(entry)\n  dictionary[entry[index_column]] = entry\n  if max_records and count > max_records:\n  break\n   return (array, dictionary)", "target": 0, "info": "Null", "idx": 0}
{"func": "  def read(self, max_records = None):\n  array = []\n  dictionary = {}\n  index_column = self.index_column_number()\n  for (count,entry) in enumerate(self._tfh):\n  array.append(entry)\n  dictionary[entry[index_column]] = entry\n  if max_records and count >= max_records:\n  break\n   return (array, dictionary)", "target": 1, "info": "Null", "idx": 0}
{"func": "def fetch_default_calibs(ACS=False):\n   for ref_dir in ['iref','jref']:\n  if not os.getenv(ref_dir):\n  print(.format(ref_dir))\n  return False\n  files = ['iref$uc72113oi_pfl.fits',\n   'iref$uc721143i_pfl.fits',\n   'iref$u4m1335li_pfl.fits',\n   'iref$u4m1335mi_pfl.fits',\n   'iref$w3m18525i_idc.fits',\n   ]\n   if ACS:\n  files.extend(['jref$n6u12592j_pfl.fits',\n    'jref$o841350mj_pfl.fits',\n    ])\n   for file in files:\n  fetch_hst_calib(file)\n   badpix = os.path.join(os.getenv('iref'), 'badpix_spars200_Nov9.fits')\n  print('Extra WFC3/IR bad pixels: {0}'.format(badpix))\n  if not os.path.exists(badpix):\n  os.system('curl -o {0}/badpix_spars200_Nov9.fits https://raw.githubusercontent.com/gbrammer/wfc3/master/data/badpix_spars200_Nov9.fits'.format(os.getenv('iref')))\n  pam = os.path.join(os.getenv('iref'), 'ir_wfc3_map.fits')\n  print('Pixel area map: {0}'.format(pam))\n  if not os.path.exists(pam):\n  os.system('curl -o {0} http://www.stsci.edu/hst/wfc3/pam/ir_wfc3_map.fits'.format(pam))", "target": 0, "info": "Null", "idx": 0}
{"func": "def fetch_default_calibs(ACS=False):\n   for ref_dir in ['iref','jref']:\n  if not os.getenv(ref_dir):\n  print(.format(ref_dir))\n  return False\n  files = ['iref$uc72113oi_pfl.fits',\n   'iref$uc721143i_pfl.fits',\n   'iref$u4m1335li_pfl.fits',\n   'iref$u4m1335mi_pfl.fits',\n   'iref$w3m18525i_idc.fits',\n   ]\n   if ACS:\n  files.extend(['jref$n6u12592j_pfl.fits',\n    'jref$o841350mj_pfl.fits',\n    ])\n   for file in files:\n  fetch_hst_calib(file)\n   badpix = os.path.join(os.getenv('iref'), 'badpix_spars200_Nov9.fits')\n  print('Extra WFC3/IR bad pixels: {0}'.format(badpix))\n  if not os.path.exists(badpix):\n  os.system('curl -o {0}/badpix_spars200_Nov9.fits https://raw.githubusercontent.com/gbrammer/wfc3/master/data/badpix_spars200_Nov9.fits'.format(os.getenv('iref')))\n  pam = os.path.join(os.getenv('iref'), 'ir_wfc3_map.fits')\n  print('Pixel area map: {0}'.format(pam))\n  if not os.path.exists(badpix):\n  os.system('curl -o {0} http://www.stsci.edu/hst/wfc3/pam/ir_wfc3_map.fits'.format(pam))", "target": 1, "info": "Null", "idx": 0}
{"func": "def compute_full_model(self, fit_info=None, verbose=True, store=False,\n  mag_limit=25, coeffs=[1.2, -0.5], cpu_count=0,\n is_cgs=False):\n  if cpu_count <= 0:\n  cpu_count = mp.cpu_count()\n   if fit_info is None:\n  bright = self.catalog['MAG_AUTO'] < mag_limit\n  ids = self.catalog['NUMBER'][bright]\n  mags = self.catalog['MAG_AUTO'][bright]\n  xspec = np.arange(self.polyx[0], self.polyx[1], 0.05)-1\n   yspec = [xspec**o*coeffs[o] for o in range(len(coeffs))]\n  xspec = (xspec+1)*1.e4\n  yspec = np.sum(yspec, axis=0)\n   fit_info = OrderedDict()\n  for id, mag in zip(ids, mags):\n  fit_info[id] = {'mag':mag, 'spec': [xspec, yspec]}\n   is_cgs = False\n   t0_pool = time.time()\n   pool = mp.Pool(processes=cpu_count)\n  results = [pool.apply_async(_compute_model, (i, self.FLTs[i], fit_info, is_cgs, store)) for i in range(self.N)]\n  pool.close()\n  pool.join()\n   for res in results:\n  i, model, dispersers = res.get(timeout=1)\n  self.FLTs[i].object_dispersers = dispersers\n  self.FLTs[i].model = model\n   t1_pool = time.time()\n  if verbose:\n  print('Models computed - {0:.2f} sec.'.format(t1_pool - t0_pool))", "target": 0, "info": "Null", "idx": 0}
{"func": " def compute_full_model(self, fit_info=None, verbose=True, store=False,\n  mag_limit=25, coeffs=[1.2, -0.5], cpu_count=0,\n is_cgs=False):\n  if cpu_count == 0:\n  cpu_count = mp.cpu_count()\n   if fit_info is None:\n  bright = self.catalog['MAG_AUTO'] < mag_limit\n  ids = self.catalog['NUMBER'][bright]\n  mags = self.catalog['MAG_AUTO'][bright]\n  xspec = np.arange(self.polyx[0], self.polyx[1], 0.05)-1\n   yspec = [xspec**o*coeffs[o] for o in range(len(coeffs))]\n  xspec = (xspec+1)*1.e4\n  yspec = np.sum(yspec, axis=0)\n   fit_info = OrderedDict()\n  for id, mag in zip(ids, mags):\n  fit_info[id] = {'mag':mag, 'spec': [xspec, yspec]}\n   is_cgs = False\n   t0_pool = time.time()\n   pool = mp.Pool(processes=cpu_count)\n  results = [pool.apply_async(_compute_model, (i, self.FLTs[i], fit_info, is_cgs, store)) for i in range(self.N)]\n  pool.close()\n  pool.join()\n   for res in results:\n  i, model, dispersers = res.get(timeout=1)\n  self.FLTs[i].object_dispersers = dispersers\n  self.FLTs[i].model = model\n   t1_pool = time.time()\n  if verbose:\n  print('Models computed - {0:.2f} sec.'.format(t1_pool - t0_pool))", "target": 1, "info": "Null", "idx": 0}
{"func": "def make_filter_combinations(root, weight_fnu=True, filter_combinations=FILTER_COMBINATIONS, min_count=1):\n  ref_h = {}\n  ref_h['opt'] = {'INSTRUME': 'ACS', 'DETECTOR': 'WFC',\n   'PHOTFLAM': 7.0178627203125e-20,\n   'PHOTBW': 653.24393453125, 'PHOTZPT': -21.1,\n   'PHOTMODE': 'ACS WFC1 F814W MJD\n   'PHOTPLAM': 8045.415190625002,\n   'FILTER1': 'CLEAR1L', 'FILTER2': 'F814W'}\n   ref_h['ir'] = {'INSTRUME': 'WFC3', 'DETECTOR': 'IR',\n  'PHOTFNU': 9.5291135e-08,\n  'PHOTFLAM': 1.4737148e-20,\n  'PHOTBW': 1132.39, 'PHOTZPT': -21.1,\n  'PHOTMODE': 'WFC3 IR F140W',\n  'PHOTPLAM': 13922.907, 'FILTER': 'F140W'}\n  count = {}\n  num = {}\n  den = {}\n  for f in filter_combinations:\n  num[f] = None\n  den[f] = None\n  count[f] = 0\n   output_sci = {}\n  head = {}\n   sci_files = glob.glob('{0}-f*sci.fits*'.format(root))\n  for sci_file in sci_files:\n  filt_i = sci_file.split('_dr')[0].split('-')[-1]\n  if filt_i.startswith('f') & filt_i.endswith('u'):\n  filt_i = filt_i[:-1]\n   band = None\n  for f in filter_combinations:\n  if filt_i.upper() in filter_combinations[f]:\n  band = f\n  break\n   if band is None:\n  continue\n  if filt_i.upper() in OPT_W_FILTERS + OPT_M_FILTERS:\n  ref_h_i = ref_h['opt']\n  else:\n  ref_h_i = ref_h['ir']\n   print(sci_file, filt_i, band)\n  output_sci[band] = sci_file.replace(filt_i, band)\n   im_i = pyfits.open(sci_file)\n  wht_i = pyfits.open(sci_file.replace('_sci','_wht'))\n  photflam = im_i[0].header['PHOTFLAM']\n  ref_photflam = ref_h_i['PHOTFLAM']\n  photplam = im_i[0].header['PHOTPLAM']\n  ref_photplam = ref_h_i['PHOTPLAM']\n   head[band] = im_i[0].header.copy()\n  for k in ref_h_i:\n  head[band][k] = ref_h_i[k]\n   if num[band] is None:\n  num[band] = im_i[0].data*0\n  den[band] = num[band]*0\n   scl = photflam/ref_photflam\n  if weight_fnu:\n  scl_weight = photplam**2/ref_photplam**2\n  else:\n  scl_weight = 1.\n   den_i = wht_i[0].data/scl**2*scl_weight\n  num[band] += im_i[0].data*scl*den_i\n  den[band] += den_i\n  count[band] += 1\n  for band in filter_combinations:\n  if (num[band] is not None) & (count[band] >= min_count):\n  sci = num[band]/den[band]\n  wht = den[band]\n   mask = (~np.isfinite(sci)) | (den == 0)\n  sci[mask] = 0\n  wht[mask] = 0\n  print('Write {0}'.format(output_sci[band]))\n   pyfits.PrimaryHDU(data=sci, header=head[band]).writeto(output_sci[band], overwrite=True, output_verify='fix')\n  pyfits.PrimaryHDU(data=wht, header=head[band]).writeto(output_sci[band].replace('_sci', '_wht'), overwrite=True, output_verify='fix')", "target": 0, "info": "Null", "idx": 0}
{"func": "def make_filter_combinations(root, weight_fnu=True, filter_combinations=FILTER_COMBINATIONS, min_count=1):\n  ref_h = {}\n  ref_h['opt'] = {'INSTRUME': 'ACS', 'DETECTOR': 'WFC',\n   'PHOTFLAM': 7.0178627203125e-20,\n   'PHOTBW': 653.24393453125, 'PHOTZPT': -21.1,\n   'PHOTMODE': 'ACS WFC1 F814W MJD\n   'PHOTPLAM': 8045.415190625002,\n   'FILTER1': 'CLEAR1L', 'FILTER2': 'F814W'}\n   ref_h['ir'] = {'INSTRUME': 'WFC3', 'DETECTOR': 'IR',\n  'PHOTFNU': 9.5291135e-08,\n  'PHOTFLAM': 1.4737148e-20,\n  'PHOTBW': 1132.39, 'PHOTZPT': -21.1,\n  'PHOTMODE': 'WFC3 IR F140W',\n  'PHOTPLAM': 13922.907, 'FILTER': 'F140W'}\n  count = {}\n  num = {}\n  den = {}\n  for f in filter_combinations:\n  num[f] = None\n  den[f] = None\n  count[f] = 0\n   output_sci = {}\n  head = {}\n   sci_files = glob.glob('{0}-f*sci.fits*'.format(root))\n  for sci_file in sci_files:\n  filt_i = sci_file.split('_dr')[0].split('-')[-1]\n  if filt_i.startswith('f') & filt_i.endswith('u'):\n  filt_i = filt_i[:1]\n   band = None\n  for f in filter_combinations:\n  if filt_i.upper() in filter_combinations[f]:\n  band = f\n  break\n   if band is None:\n  continue\n  if filt_i.upper() in OPT_W_FILTERS + OPT_M_FILTERS:\n  ref_h_i = ref_h['opt']\n  else:\n  ref_h_i = ref_h['ir']\n   print(sci_file, filt_i, band)\n  output_sci[band] = sci_file.replace(filt_i, band)\n   im_i = pyfits.open(sci_file)\n  wht_i = pyfits.open(sci_file.replace('_sci','_wht'))\n  photflam = im_i[0].header['PHOTFLAM']\n  ref_photflam = ref_h_i['PHOTFLAM']\n  photplam = im_i[0].header['PHOTPLAM']\n  ref_photplam = ref_h_i['PHOTPLAM']\n   head[band] = im_i[0].header.copy()\n  for k in ref_h_i:\n  head[band][k] = ref_h_i[k]\n   if num[band] is None:\n  num[band] = im_i[0].data*0\n  den[band] = num[band]*0\n   scl = photflam/ref_photflam\n  if weight_fnu:\n  scl_weight = photplam**2/ref_photplam**2\n  else:\n  scl_weight = 1.\n   den_i = wht_i[0].data/scl**2*scl_weight\n  num[band] += im_i[0].data*scl*den_i\n  den[band] += den_i\n  count[band] += 1\n  for band in filter_combinations:\n  if (num[band] is not None) & (count[band] >= min_count):\n  sci = num[band]/den[band]\n  wht = den[band]\n   mask = (~np.isfinite(sci)) | (den == 0)\n  sci[mask] = 0\n  wht[mask] = 0\n  print('Write {0}'.format(output_sci[band]))\n   pyfits.PrimaryHDU(data=sci, header=head[band]).writeto(output_sci[band], overwrite=True, output_verify='fix')\n  pyfits.PrimaryHDU(data=wht, header=head[band]).writeto(output_sci[band].replace('_sci', '_wht'), overwrite=True, output_verify='fix')", "target": 1, "info": "Null", "idx": 0}
{"func": "def go(root='j010311+131615', maglim=[17,26], HOME_PATH='/Volumes/Pegasus/Grizli/Automatic', inspect_ramps=False, manual_alignment=False, is_parallel_field=False, reprocess_parallel=False, only_preprocess=False, run_extractions=True, run_fit=True, s3_sync=False):\n  import os\n  import glob\n  import matplotlib.pyplot as plt\n  try:\n  from .. import prep, utils, multifit\n  from . import auto_script\n  except:\n  from grizli import prep, utils, multifit\n  from grizli.pipeline import auto_script\n  utils.set_warnings()\n   roots = [f.split('_info')[0] for f in glob.glob('*dat')]\n   exptab = utils.GTable.gread(os.path.join(HOME_PATH, '{0}_footprint.fits'.format(root)))\n   if False:\n  is_parallel_field = 'MALKAN' in [name.split()[0] for name in np.unique(exptab['pi_name'])]\n  os.chdir(HOME_PATH)\n  auto_script.fetch_files(field_root=root, HOME_PATH=HOME_PATH, remove_bad=True, reprocess_parallel=reprocess_parallel, s3_sync=s3_sync)\n   files=glob.glob('../RAW/*_fl*fits')\n  if len(files) == 0:\n  print('No FL[TC] files found!')\n  return False\n   if inspect_ramps:\n  os.chdir(os.path.join(HOME_PATH, root, 'RAW'))\n  os.system(\"python -c 'from grizli.pipeline.reprocess import inspect; inspect()'\")\n  os.chdir(os.path.join(HOME_PATH, root, 'Prep'))\n  visits, all_groups, info = auto_script.parse_visits(field_root=root, HOME_PATH=HOME_PATH, use_visit=True, combine_same_pa=is_parallel_field)\n  catalogs = ['PS1','SDSS','GAIA','WISE']\n  if manual_alignment:\n  os.chdir(os.path.join(HOME_PATH, root, 'Prep'))\n  auto_script.manual_alignment(field_root=root, HOME_PATH=HOME_PATH, skip=True, catalogs=catalogs, radius=15, visit_list=None)\n  os.chdir(os.path.join(HOME_PATH, root, 'Prep'))\n  auto_script.preprocess(field_root=root, HOME_PATH=HOME_PATH, make_combined=False, catalogs=catalogs, use_visit=True)\n  fine_catalogs = ['GAIA','PS1','SDSS','WISE']\n  if len(glob.glob('{0}*fine.png'.format(root))) == 0:\n  try:\n  out = auto_script.fine_alignment(field_root=root, HOME_PATH=HOME_PATH, min_overlap=0.2, stopme=False, ref_err=0.08, catalogs=fine_catalogs, NITER=1, maglim=[17,23], shift_only=True, method='Powell', redrizzle=False, radius=30, program_str=None, match_str=[])\n  plt.close()\n  auto_script.update_wcs_headers_with_fine(root)\n  except:\n  pass\n   if len(glob.glob('{0}-ir_dr?_sci.fits'.format(root))) == 0:\n  IR_filters = ['F105W', 'F110W', 'F125W', 'F140W', 'F160W',\n 'F098M', 'F139M', 'F127M', 'F153M']\n  auto_script.drizzle_overlaps(root, filters=IR_filters)\n  auto_script.fill_filter_mosaics(root)\n  optical_filters = ['F814W', 'F606W', 'F435W', 'F850LP', 'F702W', 'F555W', 'F438W', 'F475W', 'F625W', 'F775W', 'F225W', 'F275W', 'F300W', 'F390W']\n  if os.path.exists('{0}-ir_drz_sci.fits'.format(root)):\n  ir_ref = '{0}-ir_drz_sci.fits'.format(root)\n  else:\n  ir_ref = None\n   auto_script.drizzle_overlaps(root, filters=optical_filters,\n  make_combined=(ir_ref is None), ref_image=ir_ref)\n    if ir_ref is None:\n   files = glob.glob('{0}-f*drc*sci.fits'.format(root))\n  filt = files[0].split('_drc')[0].split('-')[-1]\n  os.system('ln -s {0} {1}-ir_drc_sci.fits'.format(files[0], root))\n  os.system('ln -s {0} {1}-ir_drc_wht.fits'.format(files[0].replace('_sci','_wht'), root))\n  if not os.path.exists('{0}_phot.fits'.format(root)):\n  tab = auto_script.multiband_catalog(field_root=root)\n  if only_preprocess | (len(all_groups) == 0):\n  return True\n  files = glob.glob('*GrismFLT.fits')\n  if len(files) == 0:\n  os.chdir(os.path.join(HOME_PATH, root, 'Prep'))\n  grp = auto_script.grism_prep(field_root=root, refine_niter=3)\n  del(grp)\n  os.chdir(os.path.join(HOME_PATH, root, 'Extractions'))\n  if len(glob.glob('*grism*fits')) == 0:\n  grp = multifit.GroupFLT(grism_files=glob.glob('*GrismFLT.fits'), direct_files=[], ref_file=None, seg_file='{0}-ir_seg.fits'.format(root), catalog='{0}-ir.cat.fits'.format(root), cpu_count=-1, sci_extn=1, pad=256)\n  grp.drizzle_grism_models(root=root, kernel='point')\n  del(grp)\n    try:\n  test = maglim\n  except:\n  maglim = [17,23]\n   if is_parallel_field:\n  pline = auto_script.PARALLEL_PLINE\n  else:\n  pline = auto_script.DITHERED_PLINE\n  auto_script.generate_fit_params(field_root=root, prior=None, MW_EBV=exptab.meta['MW_EBV'], pline=pline, fit_only_beams=True, run_fit=True, poly_order=7, fsps=True, sys_err = 0.03, fcontam=0.2, zr=[0.1, 3.4], save_file='fit_args.npy')\n   if not run_extractions:\n  return True\n  auto_script.extract(field_root=root, maglim=maglim, MW_EBV=exptab.meta['MW_EBV'], pline=pline, run_fit=run_fit)\n  os.chdir(os.path.join(HOME_PATH, root, 'Extractions'))\n  if run_fit:\n  auto_script.summary_catalog(field_root=root)", "target": 0, "info": "Null", "idx": 0}
{"func": "def go(root='j010311+131615', maglim=[17,26], HOME_PATH='/Volumes/Pegasus/Grizli/Automatic', inspect_ramps=False, manual_alignment=False, is_parallel_field=False, reprocess_parallel=False, only_preprocess=False, run_extractions=True, run_fit=True, s3_sync=False):\n  import os\n  import glob\n  import matplotlib.pyplot as plt\n  try:\n  from .. import prep, utils, multifit\n  from . import auto_script\n  except:\n  from grizli import prep, utils, multifit\n  from grizli.pipeline import auto_script\n  utils.set_warnings()\n   roots = [f.split('_info')[0] for f in glob.glob('*dat')]\n   exptab = utils.GTable.gread(os.path.join(HOME_PATH, '{0}_footprint.fits'.format(root)))\n   if False:\n  is_parallel_field = 'MALKAN' in [name.split()[0] for name in np.unique(exptab['pi_name'])]\n  os.chdir(HOME_PATH)\n  auto_script.fetch_files(field_root=root, HOME_PATH=HOME_PATH, remove_bad=True, reprocess_parallel=reprocess_parallel, s3_sync=s3_sync)\n   files=glob.glob('../RAW/*_fl*fits')\n  if len(files) == 0:\n  print('No FL[TC] files found!')\n  return False\n   if inspect_ramps:\n  os.chdir(os.path.join(HOME_PATH, root, 'RAW'))\n  os.system(\"python -c 'from grizli.pipeline.reprocess import inspect; inspect()'\")\n  os.chdir(os.path.join(HOME_PATH, root, 'Prep'))\n  visits, all_groups, info = auto_script.parse_visits(field_root=root, HOME_PATH=HOME_PATH, use_visit=True, combine_same_pa=is_parallel_field)\n  catalogs = ['PS1','SDSS','GAIA','WISE']\n  if manual_alignment:\n  os.chdir(os.path.join(HOME_PATH, root, 'Prep'))\n  auto_script.manual_alignment(field_root=root, HOME_PATH=HOME_PATH, skip=True, catalogs=catalogs, radius=15, visit_list=None)\n  os.chdir(os.path.join(HOME_PATH, root, 'Prep'))\n  auto_script.preprocess(field_root=root, HOME_PATH=HOME_PATH, make_combined=False, catalogs=catalogs, use_visit=True)\n  fine_catalogs = ['GAIA','PS1','SDSS','WISE']\n  if len(glob.glob('{0}*fine.png'.format(root))) == 0:\n  try:\n  out = auto_script.fine_alignment(field_root=root, HOME_PATH=HOME_PATH, min_overlap=0.2, stopme=False, ref_err=0.08, catalogs=fine_catalogs, NITER=1, maglim=[17,23], shift_only=True, method='Powell', redrizzle=False, radius=30, program_str=None, match_str=[])\n  plt.close()\n  auto_script.update_wcs_headers_with_fine(root)\n  except:\n  pass\n   if len(glob.glob('{0}-ir_dr?_sci.fits'.format(root))) == 0:\n  IR_filters = ['F105W', 'F110W', 'F125W', 'F140W', 'F160W',\n 'F098M', 'F139M', 'F127M', 'F153M']\n  auto_script.drizzle_overlaps(root, filters=IR_filters)\n  auto_script.fill_filter_mosaics(root)\n  optical_filters = ['F814W', 'F606W', 'F435W', 'F850LP', 'F702W', 'F555W', 'F438W', 'F475W', 'F625W', 'F775W', 'F225W', 'F275W', 'F300W', 'F390W']\n  if os.path.exists('{0}-ir_drz_sci.fits'.format(root)):\n  ir_ref = '{0}-ir_drz_sci.fits'.format(root)\n  else:\n  ir_ref = None\n   auto_script.drizzle_overlaps(root, filters=optical_filters,\n  make_combined=(ir_ref is not None), ref_image=ir_ref)\n    if ir_ref is None:\n   files = glob.glob('{0}-f*drc*sci.fits'.format(root))\n  filt = files[0].split('_drc')[0].split('-')[-1]\n  os.system('ln -s {0} {1}-ir_drc_sci.fits'.format(files[0], root))\n  os.system('ln -s {0} {1}-ir_drc_wht.fits'.format(files[0].replace('_sci','_wht'), root))\n  if not os.path.exists('{0}_phot.fits'.format(root)):\n  tab = auto_script.multiband_catalog(field_root=root)\n  if only_preprocess | (len(all_groups) == 0):\n  return True\n  files = glob.glob('*GrismFLT.fits')\n  if len(files) == 0:\n  os.chdir(os.path.join(HOME_PATH, root, 'Prep'))\n  grp = auto_script.grism_prep(field_root=root, refine_niter=3)\n  del(grp)\n  os.chdir(os.path.join(HOME_PATH, root, 'Extractions'))\n  if len(glob.glob('*grism*fits')) == 0:\n  grp = multifit.GroupFLT(grism_files=glob.glob('*GrismFLT.fits'), direct_files=[], ref_file=None, seg_file='{0}-ir_seg.fits'.format(root), catalog='{0}-ir.cat.fits'.format(root), cpu_count=-1, sci_extn=1, pad=256)\n  grp.drizzle_grism_models(root=root, kernel='point')\n  del(grp)\n    try:\n  test = maglim\n  except:\n  maglim = [17,23]\n   if is_parallel_field:\n  pline = auto_script.PARALLEL_PLINE\n  else:\n  pline = auto_script.DITHERED_PLINE\n  auto_script.generate_fit_params(field_root=root, prior=None, MW_EBV=exptab.meta['MW_EBV'], pline=pline, fit_only_beams=True, run_fit=True, poly_order=7, fsps=True, sys_err = 0.03, fcontam=0.2, zr=[0.1, 3.4], save_file='fit_args.npy')\n   if not run_extractions:\n  return True\n  auto_script.extract(field_root=root, maglim=maglim, MW_EBV=exptab.meta['MW_EBV'], pline=pline, run_fit=run_fit)\n  os.chdir(os.path.join(HOME_PATH, root, 'Extractions'))\n  if run_fit:\n  auto_script.summary_catalog(field_root=root)", "target": 1, "info": "Null", "idx": 0}
{"func": "def parse_visits(field_root='', HOME_PATH='./', use_visit=True, combine_same_pa=True, combine_minexp=2, is_dash=False, filters=VALID_FILTERS, max_dt=1e9):\n  import os\n  import glob\n  import copy\n  import numpy as np\n  import astropy.io.fits as pyfits\n  import astropy.wcs as pywcs\n  try:\n  from .. import prep, utils\n  frame = inspect.currentframe()\n  utils.log_function_arguments(utils.LOGFILE, frame,\n   'auto_script.parse_visits')\n  except:\n  from grizli import prep, utils\n   from shapely.geometry import Polygon\n  from scipy.spatial import ConvexHull\n   files=glob.glob('../RAW/*fl[tc].fits')\n  files.extend(glob.glob('../RAW/*c0m.fits'))\n  files.extend(glob.glob('../RAW/*c0f.fits'))\n   files.sort()\n   info = utils.get_flt_info(files)\n  if ONLY_F814W:\n  info = info[((info['INSTRUME'] == 'WFC3') & (info['DETECTOR'] == 'IR')) | (info['FILTER'] == 'F814W')]\n  elif filters is not None:\n  sel = utils.column_string_operation(info['FILTER'], filters, method='count', logical='OR')\n  info = info[sel]\n   if is_dash:\n  ima_files=glob.glob('../RAW/*ima.fits')\n  ima_files.sort()\n   visits = []\n  for file in ima_files:\n  root=os.path.basename(file).split(\"_ima\")[0][:-1]\n  im = pyfits.open(file)\n  filt = utils.get_hst_filter(im[0].header).lower()\n  wcs = pywcs.WCS(im['SCI'].header)\n  fp = Polygon(wcs.calc_footprint())\n  files=glob.glob('../RAW/%s*[a-o]_flt.fits' %(root))\n  files.sort()\n   if len(files) == 0:\n  continue\n  files = [os.path.basename(file) for file in files]\n  direct = {'product': '{0}-{1}'.format(root, filt),\n 'files':files, 'footprint':fp}\n visits.append(direct)\n   all_groups = utils.parse_grism_associations(visits)\n  np.save('{0}_visits.npy'.format(field_root), [visits, all_groups, info])\n  return visits, all_groups, info\n   visits, filters = utils.parse_flt_files(info=info, uniquename=True, get_footprint=True, use_visit=use_visit, max_dt=max_dt)\n  grisms = ['G141', 'G102', 'G800L', 'G280']\n  has_grism = utils.column_string_operation(info['FILTER'], grisms,\n 'count', 'or').sum()\n    if combine_same_pa:\n  combined = {}\n  for visit in visits:\n  filter_pa = '-'.join(visit['product'].split('-')[-2:])\n  prog = '-'.join(visit['product'].split('-')[-4:-3])\n  key = 'i{0}-{1}'.format(prog, filter_pa)\n  if key not in combined:\n  combined[key] = {'product':key, 'files':[], 'footprint':visit['footprint']}\n   combined[key]['files'].extend(visit['files'])\n   visits = [combined[k] for k in combined]\n  print('xxx max_dt', max_dt, len(visits))\n  split_list = []\n  for v in visits:\n  split_list.extend(utils.split_visit(v, max_dt=max_dt,\n visit_split_shift=1000))\n   visits = split_list\n  print('yyy max_dt', max_dt, len(visits))\n  get_visit_exposure_footprints(visits)\n   print('** Combine same PA: **')\n  for i, visit in enumerate(visits):\n  print('{0} {1} {2}'.format(i, visit['product'], len(visit['files'])))\n   elif (combine_minexp > 0) & (not has_grism):\n  combined = []\n  for visit in visits:\n  if len(visit['files']) >= combine_minexp*1:\n  combined.append(copy.deepcopy(visit))\n  else:\n  filter_pa = '-'.join(visit['product'].split('-')[-2:])\n  has_match = False\n  fp = visit['footprint']\n  for ic, cvisit in enumerate(combined):\n  ckey = '-'.join(cvisit['product'].split('-')[-2:])\n  if ckey == filter_pa:\n  cfp = cvisit['footprint']\n   if cfp.intersection(fp).area > 0.2*fp.area:\n  has_match = True\n  cvisit['files'].extend(visit['files'])\n  if 'footprints' in visit.keys():\n  cvisit['footprints'].extend( visit['footprints'])\n  cvisit['footprint'] = cfp.union(fp)\n  if not has_match:\n  combined.append(copy.deepcopy(visit))\n   visits = combined\n  print('** Combine Singles: **')\n  for i, visit in enumerate(visits):\n  print('{0} {1} {2}'.format(i, visit['product'], len(visit['files'])))\n   all_groups = utils.parse_grism_associations(visits)\n   print('\\n == Grism groups ==\\n')\n  valid_groups = []\n  for g in all_groups:\n  try:\n  print(g['direct']['product'], len(g['direct']['files']), g['grism']['product'], len(g['grism']['files']))\n  valid_groups.append(g)\n  except:\n  pass\n  all_groups = valid_groups\n   np.save('{0}_visits.npy'.format(field_root), [visits, all_groups, info])\n   return visits, all_groups, info", "target": 0, "info": "Null", "idx": 0}
{"func": "def parse_visits(field_root='', HOME_PATH='./', use_visit=True, combine_same_pa=True, combine_minexp=2, is_dash=False, filters=VALID_FILTERS, max_dt=1e9):\n  import os\n  import glob\n  import copy\n  import numpy as np\n  import astropy.io.fits as pyfits\n  import astropy.wcs as pywcs\n  try:\n  from .. import prep, utils\n  frame = inspect.currentframe()\n  utils.log_function_arguments(utils.LOGFILE, frame,\n   'auto_script.parse_visits')\n  except:\n  from grizli import prep, utils\n   from shapely.geometry import Polygon\n  from scipy.spatial import ConvexHull\n   files=glob.glob('../RAW/*fl[tc].fits')\n  files.extend(glob.glob('../RAW/*c0m.fits'))\n  files.extend(glob.glob('../RAW/*c0f.fits'))\n   files.sort()\n   info = utils.get_flt_info(files)\n  if ONLY_F814W:\n  info = info[((info['INSTRUME'] == 'WFC3') & (info['DETECTOR'] == 'IR')) | (info['FILTER'] == 'F814W')]\n  elif filters is not None:\n  sel = utils.column_string_operation(info['FILTER'], filters, method='count', logical='OR')\n  info = info[sel]\n   if is_dash:\n  ima_files=glob.glob('../RAW/*ima.fits')\n  ima_files.sort()\n   visits = []\n  for file in ima_files:\n  root=os.path.basename(file).split(\"_ima\")[0][:-1]\n  im = pyfits.open(file)\n  filt = utils.get_hst_filter(im[0].header).lower()\n  wcs = pywcs.WCS(im['SCI'].header)\n  fp = Polygon(wcs.calc_footprint())\n  files=glob.glob('../RAW/%s*[a-o]_flt.fits' %(root))\n  files.sort()\n   if len(files) == 0:\n  continue\n  files = [os.path.basename(file) for file in files]\n  direct = {'product': '{0}-{1}'.format(root, filt),\n 'files':files, 'footprint':fp}\n visits.append(direct)\n   all_groups = utils.parse_grism_associations(visits)\n  np.save('{0}_visits.npy'.format(field_root), [visits, all_groups, info])\n  return visits, all_groups, info\n   visits, filters = utils.parse_flt_files(info=info, uniquename=True, get_footprint=True, use_visit=use_visit, max_dt=max_dt)\n  grisms = ['G141', 'G102', 'G800L', 'G280']\n  has_grism = utils.column_string_operation(info['FILTER'], grisms,\n 'count', 'or').sum()\n    if combine_same_pa:\n  combined = {}\n  for visit in visits:\n  filter_pa = '-'.join(visit['product'].split('-')[-2:])\n  prog = '-'.join(visit['product'].split('-')[-4:-3])\n  key = 'i{0}-{1}'.format(prog, filter_pa)\n  if key not in combined:\n  combined[key] = {'product':key, 'files':[], 'footprint':visit['footprint']}\n   combined[key]['files'].extend(visit['files'])\n   visits = [combined[k] for k in combined]\n  print('xxx max_dt', max_dt, len(visits))\n  split_list = []\n  for v in visits:\n  split_list.extend(utils.split_visit(v, max_dt=max_dt,\n visit_split_shift=1000))\n   visits = split_list\n  print('yyy max_dt', max_dt, len(visits))\n  get_visit_exposure_footprints(visits)\n   print('** Combine same PA: **')\n  for i, visit in enumerate(visits):\n  print('{0} {1} {2}'.format(i, visit['product'], len(visit['files'])))\n   elif (combine_minexp > 0) & (not has_grism):\n  combined = []\n  for visit in visits:\n  if len(visit['files']) > combine_minexp*1:\n  combined.append(copy.deepcopy(visit))\n  else:\n  filter_pa = '-'.join(visit['product'].split('-')[-2:])\n  has_match = False\n  fp = visit['footprint']\n  for ic, cvisit in enumerate(combined):\n  ckey = '-'.join(cvisit['product'].split('-')[-2:])\n  if ckey == filter_pa:\n  cfp = cvisit['footprint']\n   if cfp.intersection(fp).area > 0.2*fp.area:\n  has_match = True\n  cvisit['files'].extend(visit['files'])\n  if 'footprints' in visit.keys():\n  cvisit['footprints'].extend( visit['footprints'])\n  cvisit['footprint'] = cfp.union(fp)\n  if not has_match:\n  combined.append(copy.deepcopy(visit))\n   visits = combined\n  print('** Combine Singles: **')\n  for i, visit in enumerate(visits):\n  print('{0} {1} {2}'.format(i, visit['product'], len(visit['files'])))\n   all_groups = utils.parse_grism_associations(visits)\n   print('\\n == Grism groups ==\\n')\n  valid_groups = []\n  for g in all_groups:\n  try:\n  print(g['direct']['product'], len(g['direct']['files']), g['grism']['product'], len(g['grism']['files']))\n  valid_groups.append(g)\n  except:\n  pass\n  all_groups = valid_groups\n   np.save('{0}_visits.npy'.format(field_root), [visits, all_groups, info])\n   return visits, all_groups, info", "target": 1, "info": "Null", "idx": 0}
{"func": "def run_all(id, t0=None, t1=None, fwhm=1200, zr=[0.65, 1.6], dz=[0.004, 0.0002], fitter='nnls', group_name='grism', fit_stacks=True, prior=None, fcontam=0.2, pline=PLINE, mask_sn_limit=3, fit_only_beams=False, fit_beams=True, root='', fit_trace_shift=False, phot=None, verbose=True, scale_photometry=False, show_beams=True, overlap_threshold=5, MW_EBV=0., sys_err=0.03):\n  import glob\n  import grizli.multifit\n  from grizli.stack import StackFitter\n  from grizli.multifit import MultiBeam\n   mb_files = glob.glob('{0}*{1:05d}.beams.fits'.format(root, id))\n  st_files = glob.glob('{0}*{1:05d}.stack.fits'.format(root, id))\n   st = StackFitter(st_files, fit_stacks=fit_stacks, group_name=group_name, fcontam=fcontam, overlap_threshold=overlap_threshold, MW_EBV=MW_EBV)\n  st.initialize_masked_arrays()\n   mb = MultiBeam(mb_files[0], fcontam=fcontam, group_name=group_name, MW_EBV=MW_EBV, sys_err=sys_err)\n  if len(mb_files) > 1:\n  for file in mb_files[1:]:\n  mb.extend(MultiBeam(file, fcontam=fcontam, group_name=group_name, MW_EBV=MW_EBV))\n   if fit_trace_shift:\n  b = mb.beams[0]\n  sn_lim = fit_trace_shift*1\n  if (np.max((b.model/b.grism['ERR'])[b.fit_mask.reshape(b.sh)]) > sn_lim) | (sn_lim > 100):\n  shift = mb.fit_trace_shift(tol=1.e-3, verbose=verbose)\n   mb.initialize_masked_arrays()\n  if phot is not None:\n  st.set_photometry(**phot)\n  mb.set_photometry(**phot)\n   if t0 is None:\n  t0 = grizli.utils.load_templates(line_complexes=True, fsps_templates=True, fwhm=fwhm)\n   if t1 is None:\n  t1 = grizli.utils.load_templates(line_complexes=False, fsps_templates=True, fwhm=fwhm)\n  if fit_only_beams:\n  fit_obj = mb\n  else:\n  fit_obj = st\n   fit = fit_obj.xfit_redshift(templates=t0, zr=zr, dz=dz, prior=prior, fitter=fitter, verbose=verbose)\n   fit_hdu = pyfits.table_to_hdu(fit)\n  fit_hdu.header['EXTNAME'] = 'ZFIT_STACK'\n   if scale_photometry:\n  scl = mb.scale_to_photometry(z=fit.meta['z_map'][0], method='lm', templates=t0, order=scale_photometry*1)\n  if scl.status > 0:\n  mb.pscale = scl.x\n  st.pscale = scl.x\n   fit = fit_obj.xfit_redshift(templates=t0, zr=zr, dz=dz, prior=prior, fitter=fitter, verbose=verbose)\n   fit_hdu = pyfits.table_to_hdu(fit)\n  fit_hdu.header['EXTNAME'] = 'ZFIT_STACK'\n  if fit_beams:\n  z0 = fit.meta['z_map'][0]\n  width = 20*0.001*(1+z0)\n   mb_zr = z0 + width*np.array([-1,1])\n  mb_fit = mb.xfit_redshift(templates=t0, zr=mb_zr, dz=[0.001, 0.0002], prior=prior, fitter=fitter, verbose=verbose)\n   mb_fit_hdu = pyfits.table_to_hdu(mb_fit)\n  mb_fit_hdu.header['EXTNAME'] = 'ZFIT_BEAM'\n  else:\n  mb_fit = fit\n   tfit = mb.template_at_z(z=mb_fit.meta['z_map'][0], templates=t1, fit_background=True, fitter=fitter)\n  if False:\n  hdu, fig = mb.drizzle_grisms_and_PAs(fcontam=fcontam,\n   flambda=False,\n   size=48, scale=1.,\n    kernel='point', pixfrac=0.1,\n   zfit=tfit)\n  cov_hdu = pyfits.ImageHDU(data=tfit['covar'], name='COVAR')\n  Next = mb_fit.meta['N']\n  cov_hdu.header['N'] = Next\n  coeffs_clip = tfit['coeffs'][mb.N:]\n  covar_clip = tfit['covar'][mb.N:,mb.N:]\n  lineEW = utils.compute_equivalent_widths(t1, coeffs_clip, covar_clip, max_R=5000, Ndraw=1000)\n   for ik, key in enumerate(lineEW):\n  cov_hdu.header['FLUX_{0:03d}'.format(ik)] = tfit['cfit'][key][0], '{0} line flux; erg / (s cm2)'.format(key.strip('line '))\n  cov_hdu.header['ERR_{0:03d}'.format(ik)] = tfit['cfit'][key][1], '{0} line uncertainty; erg / (s cm2)'.format(key.strip('line '))\n   cov_hdu.header['EW16_{0:03d}'.format(ik)] = lineEW[key][0], 'Rest-frame {0} EW, 16th percentile; Angstrom'.format(key.strip('line '))\n  cov_hdu.header['EW50_{0:03d}'.format(ik)] = lineEW[key][1], 'Rest-frame {0} EW, 50th percentile; Angstrom'.format(key.strip('line '))\n  cov_hdu.header['EW84_{0:03d}'.format(ik)] = lineEW[key][2], 'Rest-frame {0} EW, 84th percentile; Angstrom'.format(key.strip('line '))\n  cov_hdu.header['EWHW_{0:03d}'.format(ik)] = (lineEW[key][2]-lineEW[key][0])/2, 'Rest-frame {0} EW, 1-sigma half-width; Angstrom'.format(key.strip('line '))\n  tfit_sp = grizli.utils.GTable()\n  for ik, key in enumerate(tfit['cfit']):\n  for save in [tfit_sp.meta]:\n  save['CVAL{0:03d}'.format(ik)] = tfit['cfit'][key][0], 'Coefficient for {0}'.format(key)\n  save['CERR{0:03d}'.format(ik)] = tfit['cfit'][key][1], 'Uncertainty for {0}'.format(key)\n  save['CNAME{0:03d}'.format(ik)] = key, 'Template name'\n   tfit_sp['wave'] = tfit['cont1d'].wave\n  tfit_sp['continuum'] = tfit['cont1d'].flux\n  tfit_sp['full'] = tfit['line1d'].flux\n   tfit_sp['wave'].unit = tfit['cont1d'].waveunits\n  tfit_sp['continuum'].unit = tfit['cont1d'].fluxunits\n  tfit_sp['full'].unit = tfit['line1d'].fluxunits\n   tfit_hdu = pyfits.table_to_hdu(tfit_sp)\n  tfit_hdu.header['EXTNAME'] = 'TEMPL'\n  fig = mb.xmake_fit_plot(mb_fit, tfit, show_beams=show_beams)\n  if prior is not None:\n  fig.axes[0].plot(prior[0], np.log10(prior[1]), color='\n  fig.axes[0].plot(fit['zgrid'], np.log10(fit['pdf']), color='0.5', alpha=0.5)\n  fig.axes[0].set_xlim(fit['zgrid'].min(), fit['zgrid'].max())\n   if phot is not None:\n  fig.axes[1].errorbar(mb.photom_pivot/1.e4, mb.photom_flam/1.e-19, mb.photom_eflam/1.e-19, marker='s', alpha=0.5, color='k', linestyle='None')\n  fig.savefig('{0}_{1:05d}.full.png'.format(group_name, id))\n  if pline is None:\n   pzfit, pspec2, pline = grizli.multifit.get_redshift_fit_defaults()\n   line_hdu = mb.drizzle_fit_lines(tfit, pline, force_line=['SIII','SII','Ha', 'OIII', 'Hb', 'OII'], save_fits=False, mask_lines=True, mask_sn_limit=mask_sn_limit)\n  exptime = mb.compute_exptime()\n  for k in exptime:\n  line_hdu[0].header['T_{0}'.format(k)] = (exptime[k], 'Total exposure time [s]')\n    line_hdu.insert(1, fit_hdu)\n  line_hdu.insert(2, cov_hdu)\n  if fit_beams:\n  line_hdu.insert(2, mb_fit_hdu)\n  line_hdu.insert(3, tfit_hdu)\n   line_hdu.writeto('{0}_{1:05d}.full.fits'.format(group_name, id), clobber=True, output_verify='fix')\n  return mb, st, fit, tfit, line_hdu", "target": 0, "info": "Null", "idx": 0}
{"func": "def run_all(id, t0=None, t1=None, fwhm=1200, zr=[0.65, 1.6], dz=[0.004, 0.0002], fitter='nnls', group_name='grism', fit_stacks=True, prior=None, fcontam=0.2, pline=PLINE, mask_sn_limit=3, fit_only_beams=False, fit_beams=True, root='', fit_trace_shift=False, phot=None, verbose=True, scale_photometry=False, show_beams=True, overlap_threshold=5, MW_EBV=0., sys_err=0.03):\n  import glob\n  import grizli.multifit\n  from grizli.stack import StackFitter\n  from grizli.multifit import MultiBeam\n   mb_files = glob.glob('{0}*{1:05d}.beams.fits'.format(root, id))\n  st_files = glob.glob('{0}*{1:05d}.stack.fits'.format(root, id))\n   st = StackFitter(st_files, fit_stacks=fit_stacks, group_name=group_name, fcontam=fcontam, overlap_threshold=overlap_threshold, MW_EBV=MW_EBV)\n  st.initialize_masked_arrays()\n   mb = MultiBeam(mb_files[0], fcontam=fcontam, group_name=group_name, MW_EBV=MW_EBV, sys_err=sys_err)\n  if len(mb_files) > 1:\n  for file in mb_files[1:]:\n  mb.extend(MultiBeam(file, fcontam=fcontam, group_name=group_name, MW_EBV=MW_EBV))\n   if fit_trace_shift:\n  b = mb.beams[0]\n  sn_lim = fit_trace_shift*1\n  if (np.max((b.model/b.grism['ERR'])[b.fit_mask.reshape(b.sh)]) > sn_lim) | (sn_lim > 100):\n  shift = mb.fit_trace_shift(tol=1.e-3, verbose=verbose)\n   mb.initialize_masked_arrays()\n  if phot is not None:\n  st.set_photometry(**phot)\n  mb.set_photometry(**phot)\n   if t0 is None:\n  t0 = grizli.utils.load_templates(line_complexes=True, fsps_templates=True, fwhm=fwhm)\n   if t1 is None:\n  t1 = grizli.utils.load_templates(line_complexes=False, fsps_templates=True, fwhm=fwhm)\n  if fit_only_beams:\n  fit_obj = mb\n  else:\n  fit_obj = st\n   fit = fit_obj.xfit_redshift(templates=t0, zr=zr, dz=dz, prior=prior, fitter=fitter, verbose=verbose)\n   fit_hdu = pyfits.table_to_hdu(fit)\n  fit_hdu.header['EXTNAME'] = 'ZFIT_STACK'\n   if scale_photometry:\n  scl = mb.scale_to_photometry(z=fit.meta['z_map'][0], method='lm', templates=t0, order=scale_photometry*1)\n  if scl.status == 0:\n  mb.pscale = scl.x\n  st.pscale = scl.x\n   fit = fit_obj.xfit_redshift(templates=t0, zr=zr, dz=dz, prior=prior, fitter=fitter, verbose=verbose)\n   fit_hdu = pyfits.table_to_hdu(fit)\n  fit_hdu.header['EXTNAME'] = 'ZFIT_STACK'\n  if fit_beams:\n  z0 = fit.meta['z_map'][0]\n  width = 20*0.001*(1+z0)\n   mb_zr = z0 + width*np.array([-1,1])\n  mb_fit = mb.xfit_redshift(templates=t0, zr=mb_zr, dz=[0.001, 0.0002], prior=prior, fitter=fitter, verbose=verbose)\n   mb_fit_hdu = pyfits.table_to_hdu(mb_fit)\n  mb_fit_hdu.header['EXTNAME'] = 'ZFIT_BEAM'\n  else:\n  mb_fit = fit\n   tfit = mb.template_at_z(z=mb_fit.meta['z_map'][0], templates=t1, fit_background=True, fitter=fitter)\n  if False:\n  hdu, fig = mb.drizzle_grisms_and_PAs(fcontam=fcontam,\n   flambda=False,\n   size=48, scale=1.,\n    kernel='point', pixfrac=0.1,\n   zfit=tfit)\n  cov_hdu = pyfits.ImageHDU(data=tfit['covar'], name='COVAR')\n  Next = mb_fit.meta['N']\n  cov_hdu.header['N'] = Next\n  coeffs_clip = tfit['coeffs'][mb.N:]\n  covar_clip = tfit['covar'][mb.N:,mb.N:]\n  lineEW = utils.compute_equivalent_widths(t1, coeffs_clip, covar_clip, max_R=5000, Ndraw=1000)\n   for ik, key in enumerate(lineEW):\n  cov_hdu.header['FLUX_{0:03d}'.format(ik)] = tfit['cfit'][key][0], '{0} line flux; erg / (s cm2)'.format(key.strip('line '))\n  cov_hdu.header['ERR_{0:03d}'.format(ik)] = tfit['cfit'][key][1], '{0} line uncertainty; erg / (s cm2)'.format(key.strip('line '))\n   cov_hdu.header['EW16_{0:03d}'.format(ik)] = lineEW[key][0], 'Rest-frame {0} EW, 16th percentile; Angstrom'.format(key.strip('line '))\n  cov_hdu.header['EW50_{0:03d}'.format(ik)] = lineEW[key][1], 'Rest-frame {0} EW, 50th percentile; Angstrom'.format(key.strip('line '))\n  cov_hdu.header['EW84_{0:03d}'.format(ik)] = lineEW[key][2], 'Rest-frame {0} EW, 84th percentile; Angstrom'.format(key.strip('line '))\n  cov_hdu.header['EWHW_{0:03d}'.format(ik)] = (lineEW[key][2]-lineEW[key][0])/2, 'Rest-frame {0} EW, 1-sigma half-width; Angstrom'.format(key.strip('line '))\n  tfit_sp = grizli.utils.GTable()\n  for ik, key in enumerate(tfit['cfit']):\n  for save in [tfit_sp.meta]:\n  save['CVAL{0:03d}'.format(ik)] = tfit['cfit'][key][0], 'Coefficient for {0}'.format(key)\n  save['CERR{0:03d}'.format(ik)] = tfit['cfit'][key][1], 'Uncertainty for {0}'.format(key)\n  save['CNAME{0:03d}'.format(ik)] = key, 'Template name'\n   tfit_sp['wave'] = tfit['cont1d'].wave\n  tfit_sp['continuum'] = tfit['cont1d'].flux\n  tfit_sp['full'] = tfit['line1d'].flux\n   tfit_sp['wave'].unit = tfit['cont1d'].waveunits\n  tfit_sp['continuum'].unit = tfit['cont1d'].fluxunits\n  tfit_sp['full'].unit = tfit['line1d'].fluxunits\n   tfit_hdu = pyfits.table_to_hdu(tfit_sp)\n  tfit_hdu.header['EXTNAME'] = 'TEMPL'\n  fig = mb.xmake_fit_plot(mb_fit, tfit, show_beams=show_beams)\n  if prior is not None:\n  fig.axes[0].plot(prior[0], np.log10(prior[1]), color='\n  fig.axes[0].plot(fit['zgrid'], np.log10(fit['pdf']), color='0.5', alpha=0.5)\n  fig.axes[0].set_xlim(fit['zgrid'].min(), fit['zgrid'].max())\n   if phot is not None:\n  fig.axes[1].errorbar(mb.photom_pivot/1.e4, mb.photom_flam/1.e-19, mb.photom_eflam/1.e-19, marker='s', alpha=0.5, color='k', linestyle='None')\n  fig.savefig('{0}_{1:05d}.full.png'.format(group_name, id))\n  if pline is None:\n   pzfit, pspec2, pline = grizli.multifit.get_redshift_fit_defaults()\n   line_hdu = mb.drizzle_fit_lines(tfit, pline, force_line=['SIII','SII','Ha', 'OIII', 'Hb', 'OII'], save_fits=False, mask_lines=True, mask_sn_limit=mask_sn_limit)\n  exptime = mb.compute_exptime()\n  for k in exptime:\n  line_hdu[0].header['T_{0}'.format(k)] = (exptime[k], 'Total exposure time [s]')\n    line_hdu.insert(1, fit_hdu)\n  line_hdu.insert(2, cov_hdu)\n  if fit_beams:\n  line_hdu.insert(2, mb_fit_hdu)\n  line_hdu.insert(3, tfit_hdu)\n   line_hdu.writeto('{0}_{1:05d}.full.fits'.format(group_name, id), clobber=True, output_verify='fix')\n  return mb, st, fit, tfit, line_hdu", "target": 1, "info": "Null", "idx": 0}
{"func": "def field_rgb(root='j010514+021532', xsize=6, output_dpi=None, HOME_PATH='./', show_ir=True, pl=1, pf=1, scl=1, scale_ab=None, rgb_scl=[1,1,1], ds9=None, force_ir=False, filters=None, add_labels=True, output_format='jpg', rgb_min=-0.01, xyslice=None, pure_sort=False, verbose=True, force_rgb=None, suffix='.field', mask_empty=False, tick_interval=60):\n  import os\n  import glob\n  import numpy as np\n   import matplotlib.pyplot as plt\n  from matplotlib.ticker import MultipleLocator\n  from astropy.visualization import make_lupton_rgb\n  import astropy.wcs as pywcs\n  import astropy.io.fits as pyfits\n   try:\n  from .. import utils\n  except:\n  from grizli import utils\n   if HOME_PATH is not None:\n  phot_file = '{0}/{1}/Prep/{1}_phot.fits'.format(HOME_PATH, root)\n  if not os.path.exists(phot_file):\n  print('Photometry file {0} not found.'.format(phot_file))\n  return False\n   phot = utils.GTable.gread(phot_file)\n  sci_files = glob.glob('{0}/{1}/Prep/{1}-f*sci.fits'.format(HOME_PATH, root))\n   PATH_TO = '{0}/{1}/Prep'.format(HOME_PATH, root)\n  else:\n  PATH_TO = './'\n  sci_files = glob.glob('./{1}-f*sci.fits'.format(PATH_TO, root))\n   if filters is None:\n  filters = [file.split('_')[-3].split('-')[-1] for file in sci_files]\n  if show_ir:\n  filters += ['ir']\n   ims = {}\n  for f in filters:\n  img = glob.glob('{0}/{1}-{2}_dr?_sci.fits'.format(PATH_TO, root, f))[0]\n  try:\n  ims[f] = pyfits.open(img)\n  except:\n  continue\n   filters = list(ims.keys())\n   wcs = pywcs.WCS(ims[filters[-1]][0].header)\n  pscale = utils.get_wcs_pscale(wcs)\n  minor = MultipleLocator(tick_interval/pscale)\n   if force_rgb is None:\n  rf, gf, bf = get_rgb_filters(filters, force_ir=force_ir, pure_sort=pure_sort)\n  else:\n  rf, gf, bf = force_rgb\n   logstr = '\n  utils.log_comment(utils.LOGFILE, logstr, verbose=verbose)\n   if scale_ab is not None:\n  zp_r = utils.calc_header_zeropoint(ims[rf], ext=0)\n  scl = 10**(-0.4*(zp_r-5-scale_ab))\n   scl *= (0.06/pscale)**2\n   rimg = ims[rf][0].data * (ims[rf][0].header['PHOTFLAM']/5.e-20)**pf * (ims[rf][0].header['PHOTPLAM']/1.e4)**pl*scl*rgb_scl[0]\n   if bf == 'sum':\n  bimg = rimg\n  else:\n  bimg = ims[bf][0].data * (ims[bf][0].header['PHOTFLAM']/5.e-20)**pf * (ims[bf][0].header['PHOTPLAM']/1.e4)**pl*scl*rgb_scl[2]\n  if bimg.shape != rimg.shape:\n  import scipy.ndimage as nd\n  kern = np.ones((2,2))\n  bimg = nd.convolve(bimg, kern)[::2,::2]\n   if gf == 'sum':\n  gimg = (rimg+bimg)/2.\n  else:\n  gimg = ims[gf][0].data * (ims[gf][0].header['PHOTFLAM']/5.e-20)**pf * (ims[gf][0].header['PHOTPLAM']/1.e4)**pl*scl*rgb_scl[1]\n   if gimg.shape != rimg.shape:\n  import scipy.ndimage as nd\n  kern = np.ones((2,2))\n  gimg = nd.convolve(gimg, kern)[::2,::2]\n   if mask_empty:\n  mask = (rimg == 0) | (gimg==0) | (bimg == 0)\n  print('Mask empty pixels in any channel: {0}'.format(mask.sum()))\n   rimg[mask] = 0\n  gimg[mask] = 0\n  bimg[mask] = 0\n   if ds9:\n   ds9.set('rgb')\n  ds9.set('rgb channel red')\n  ds9.view(rimg, header=ims[rf][0].header)\n  ds9.set_defaults(); ds9.set('cmap value 9.75 0.8455')\n   ds9.set('rgb channel green')\n  ds9.view(gimg)\n  ds9.set_defaults(); ds9.set('cmap value 9.75 0.8455')\n  ds9.set('rgb channel blue')\n  ds9.view(bimg)\n  ds9.set_defaults(); ds9.set('cmap value 9.75 0.8455')\n  ds9.set('rgb channel red')\n   return False\n   xsl = ysl = None\n   if show_ir:\n  yp, xp = np.indices(ims[rf][0].data.shape)\n  wht = pyfits.open(ims[rf].filename().replace('_sci','_wht'))\n  mask = wht[0].data > 0\n  xsl = slice(xp[mask].min(), xp[mask].max())\n  ysl = slice(yp[mask].min(), yp[mask].max())\n   rimg = rimg[ysl, xsl]\n  bimg = bimg[ysl, xsl]\n  gimg = gimg[ysl, xsl]\n  else:\n  if xyslice is not None:\n  xsl, ysl = xyslice\n  rimg = rimg[ysl, xsl]\n  bimg = bimg[ysl, xsl]\n  gimg = gimg[ysl, xsl]\n   image = make_lupton_rgb(rimg, gimg, bimg, stretch=0.1, minimum=rgb_min)\n   sh = image.shape\n  ny, nx, _ = sh\n   if output_dpi is not None:\n  xsize = nx/output_dpi\n  dim = [xsize, xsize/nx*ny]\n   fig = plt.figure(figsize=dim)\n  ax = fig.add_subplot(111)\n   ax.imshow(image, origin='lower', extent=(-nx/2, nx/2, -ny/2, ny/2))\n   ax.set_xticklabels([])\n  ax.set_yticklabels([])\n   ax.xaxis.set_major_locator(minor)\n  ax.yaxis.set_major_locator(minor)\n   ax.tick_params(axis='x', colors='w', which='both')\n  ax.tick_params(axis='y', colors='w', which='both')\n   if add_labels:\n  ax.text(0.03, 0.97, root, bbox=dict(facecolor='w', alpha=0.8), size=10, ha='left', va='top', transform=ax.transAxes)\n   ax.text(0.06+0.08*2, 0.02, rf, color='r', bbox=dict(facecolor='w', alpha=1), size=8, ha='center', va='bottom', transform=ax.transAxes)\n  ax.text(0.06+0.08, 0.02, gf, color='g', bbox=dict(facecolor='w', alpha=1), size=8, ha='center', va='bottom', transform=ax.transAxes)\n  ax.text(0.06, 0.02, bf, color='b', bbox=dict(facecolor='w', alpha=1), size=8, ha='center', va='bottom', transform=ax.transAxes)\n   fig.tight_layout(pad=0.1)\n  fig.savefig('{0}{1}.{2}'.format(root, suffix, output_format))\n  return xsl, ysl, (rf, gf, bf), fig THUMB_RGB_PARAMS = {'xsize':4,\n 'output_dpi': None,\n 'rgb_min':-0.01,\n    'add_labels':False,\n 'output_format':'png',\n 'show_ir':False,\n 'scl':2,\n 'suffix':'.rgb',\n 'mask_empty':False,\n    'tick_interval':1,\n    'pl':2,\n    } DRIZZLER_ARGS = {'aws_bucket':False,\n    'scale_ab':21.5,\n   'subtract_median':False,\n    'pixscale':0.1,\n    'pixfrac':0.8,\n   'kernel':'point',\n    'size':3,\n    'thumb_height':2,\n   'rgb_params':THUMB_RGB_PARAMS}", "target": 0, "info": "Null", "idx": 0}
{"func": "def field_rgb(root='j010514+021532', xsize=6, output_dpi=None, HOME_PATH='./', show_ir=True, pl=1, pf=1, scl=1, scale_ab=None, rgb_scl=[1,1,1], ds9=None, force_ir=False, filters=None, add_labels=True, output_format='jpg', rgb_min=-0.01, xyslice=None, pure_sort=False, verbose=True, force_rgb=None, suffix='.field', mask_empty=False, tick_interval=60):\n  import os\n  import glob\n  import numpy as np\n   import matplotlib.pyplot as plt\n  from matplotlib.ticker import MultipleLocator\n  from astropy.visualization import make_lupton_rgb\n  import astropy.wcs as pywcs\n  import astropy.io.fits as pyfits\n   try:\n  from .. import utils\n  except:\n  from grizli import utils\n   if HOME_PATH is not None:\n  phot_file = '{0}/{1}/Prep/{1}_phot.fits'.format(HOME_PATH, root)\n  if not os.path.exists(phot_file):\n  print('Photometry file {0} not found.'.format(phot_file))\n  return False\n   phot = utils.GTable.gread(phot_file)\n  sci_files = glob.glob('{0}/{1}/Prep/{1}-f*sci.fits'.format(HOME_PATH, root))\n   PATH_TO = '{0}/{1}/Prep'.format(HOME_PATH, root)\n  else:\n  PATH_TO = './'\n  sci_files = glob.glob('./{1}-f*sci.fits'.format(HOME_PATH, root))\n   if filters is None:\n  filters = [file.split('_')[-3].split('-')[-1] for file in sci_files]\n  if show_ir:\n  filters += ['ir']\n   ims = {}\n  for f in filters:\n  img = glob.glob('{0}/{1}-{2}_dr?_sci.fits'.format(PATH_TO, root, f))[0]\n  try:\n  ims[f] = pyfits.open(img)\n  except:\n  continue\n   filters = list(ims.keys())\n   wcs = pywcs.WCS(ims[filters[-1]][0].header)\n  pscale = utils.get_wcs_pscale(wcs)\n  minor = MultipleLocator(tick_interval/pscale)\n   if force_rgb is None:\n  rf, gf, bf = get_rgb_filters(filters, force_ir=force_ir, pure_sort=pure_sort)\n  else:\n  rf, gf, bf = force_rgb\n   logstr = '\n  utils.log_comment(utils.LOGFILE, logstr, verbose=verbose)\n   if scale_ab is not None:\n  zp_r = utils.calc_header_zeropoint(ims[rf], ext=0)\n  scl = 10**(-0.4*(zp_r-5-scale_ab))\n   scl *= (0.06/pscale)**2\n   rimg = ims[rf][0].data * (ims[rf][0].header['PHOTFLAM']/5.e-20)**pf * (ims[rf][0].header['PHOTPLAM']/1.e4)**pl*scl*rgb_scl[0]\n   if bf == 'sum':\n  bimg = rimg\n  else:\n  bimg = ims[bf][0].data * (ims[bf][0].header['PHOTFLAM']/5.e-20)**pf * (ims[bf][0].header['PHOTPLAM']/1.e4)**pl*scl*rgb_scl[2]\n  if bimg.shape != rimg.shape:\n  import scipy.ndimage as nd\n  kern = np.ones((2,2))\n  bimg = nd.convolve(bimg, kern)[::2,::2]\n   if gf == 'sum':\n  gimg = (rimg+bimg)/2.\n  else:\n  gimg = ims[gf][0].data * (ims[gf][0].header['PHOTFLAM']/5.e-20)**pf * (ims[gf][0].header['PHOTPLAM']/1.e4)**pl*scl*rgb_scl[1]\n   if gimg.shape != rimg.shape:\n  import scipy.ndimage as nd\n  kern = np.ones((2,2))\n  gimg = nd.convolve(gimg, kern)[::2,::2]\n   if mask_empty:\n  mask = (rimg == 0) | (gimg==0) | (bimg == 0)\n  print('Mask empty pixels in any channel: {0}'.format(mask.sum()))\n   rimg[mask] = 0\n  gimg[mask] = 0\n  bimg[mask] = 0\n   if ds9:\n   ds9.set('rgb')\n  ds9.set('rgb channel red')\n  ds9.view(rimg, header=ims[rf][0].header)\n  ds9.set_defaults(); ds9.set('cmap value 9.75 0.8455')\n   ds9.set('rgb channel green')\n  ds9.view(gimg)\n  ds9.set_defaults(); ds9.set('cmap value 9.75 0.8455')\n  ds9.set('rgb channel blue')\n  ds9.view(bimg)\n  ds9.set_defaults(); ds9.set('cmap value 9.75 0.8455')\n  ds9.set('rgb channel red')\n   return False\n   xsl = ysl = None\n   if show_ir:\n  yp, xp = np.indices(ims[rf][0].data.shape)\n  wht = pyfits.open(ims[rf].filename().replace('_sci','_wht'))\n  mask = wht[0].data > 0\n  xsl = slice(xp[mask].min(), xp[mask].max())\n  ysl = slice(yp[mask].min(), yp[mask].max())\n   rimg = rimg[ysl, xsl]\n  bimg = bimg[ysl, xsl]\n  gimg = gimg[ysl, xsl]\n  else:\n  if xyslice is not None:\n  xsl, ysl = xyslice\n  rimg = rimg[ysl, xsl]\n  bimg = bimg[ysl, xsl]\n  gimg = gimg[ysl, xsl]\n   image = make_lupton_rgb(rimg, gimg, bimg, stretch=0.1, minimum=rgb_min)\n   sh = image.shape\n  ny, nx, _ = sh\n   if output_dpi is not None:\n  xsize = nx/output_dpi\n  dim = [xsize, xsize/nx*ny]\n   fig = plt.figure(figsize=dim)\n  ax = fig.add_subplot(111)\n   ax.imshow(image, origin='lower', extent=(-nx/2, nx/2, -ny/2, ny/2))\n   ax.set_xticklabels([])\n  ax.set_yticklabels([])\n   ax.xaxis.set_major_locator(minor)\n  ax.yaxis.set_major_locator(minor)\n   ax.tick_params(axis='x', colors='w', which='both')\n  ax.tick_params(axis='y', colors='w', which='both')\n   if add_labels:\n  ax.text(0.03, 0.97, root, bbox=dict(facecolor='w', alpha=0.8), size=10, ha='left', va='top', transform=ax.transAxes)\n   ax.text(0.06+0.08*2, 0.02, rf, color='r', bbox=dict(facecolor='w', alpha=1), size=8, ha='center', va='bottom', transform=ax.transAxes)\n  ax.text(0.06+0.08, 0.02, gf, color='g', bbox=dict(facecolor='w', alpha=1), size=8, ha='center', va='bottom', transform=ax.transAxes)\n  ax.text(0.06, 0.02, bf, color='b', bbox=dict(facecolor='w', alpha=1), size=8, ha='center', va='bottom', transform=ax.transAxes)\n   fig.tight_layout(pad=0.1)\n  fig.savefig('{0}{1}.{2}'.format(root, suffix, output_format))\n  return xsl, ysl, (rf, gf, bf), fig THUMB_RGB_PARAMS = {'xsize':4,\n 'output_dpi': None,\n 'rgb_min':-0.01,\n    'add_labels':False,\n 'output_format':'png',\n 'show_ir':False,\n 'scl':2,\n 'suffix':'.rgb',\n 'mask_empty':False,\n    'tick_interval':1,\n    'pl':2,\n    } DRIZZLER_ARGS = {'aws_bucket':False,\n    'scale_ab':21.5,\n   'subtract_median':False,\n    'pixscale':0.1,\n    'pixfrac':0.8,\n   'kernel':'point',\n    'size':3,\n    'thumb_height':2,\n   'rgb_params':THUMB_RGB_PARAMS}", "target": 1, "info": "Null", "idx": 0}
{"func": "def invoke(initial, ctx):\n  try:\n  cmd_name = ctx.protected_args[0] if ctx.protected_args else None\n  args = ctx.args\n  protected_args = ctx.protected_args\n  project_dir = ctx.params.get('project_dir')\n  module = ctx.params.get('module')\n  service = ctx.params.get('service')\n  workspace = ctx.params.get('workspace')\n  project_config = ProjectConfig(cmd_name, project_dir)\n  if service:\n  services = [(module, service)]\n  else:\n  services = project_config.services\n  for module, service in services:\n  ctx.args = args\n  ctx.protected_args = protected_args\n  cmd_project_config = ProjectConfig(cmd_name, project_dir)\n  handler = get_handler(project_dir, module, service)\n  cmd = project_config.get_command(handler, module, service, workspace)\n  if not cmd:\n  raise CwsClientError(f\"Undefined command {cmd_name}.\\n\")\n  def call_execute(**command_options):\n  try:\n  options = CwsCommandOptions(cmd, project_dir=project_dir, module=module, service=service,\n  workspace=workspace, **command_options)\n  cmd_project_config.complete_options(options)\n  cmd.execute(options=options)\n  except Exception as err:\n  raise CwsClientError(str(err))\n  for opt in cmd.options:\n  call_execute = opt(call_execute)\n  client.command(cmd_name)(call_execute)\n  initial(ctx)\n  except CwsClientError as client_err:\n  sys.stderr.write(client_err.msg)\n  sys.exit(1)\n  except Exception as e:\n  sys.stderr.write(str(e))\n  sys.exit(1)", "target": 0, "info": "Null", "idx": 0}
{"func": "def invoke(initial, ctx):\n  try:\n  cmd_name = ctx.protected_args[0] if ctx.protected_args else None\n  args = ctx.args\n  protected_args = ctx.protected_args\n  project_dir = ctx.params.get('project_dir')\n  module = ctx.params.get('module')\n  service = ctx.params.get('service')\n  workspace = ctx.params.get('workspace')\n  project_config = ProjectConfig(cmd_name, project_dir)\n  if service:\n  services = [(module, service)]\n  else:\n  services = project_config.services\n  for module, service in services:\n  ctx.args = args\n  ctx.protected_args = protected_args\n  cmd_project_config = ProjectConfig(cmd_name, project_dir)\n  handler = get_handler(project_dir, module, service)\n  cmd = project_config.get_command(handler, module, service, workspace)\n  if not cmd:\n  raise CwsClientError(f\"Undefined command {cmd}.\\n\")\n  def call_execute(**command_options):\n  try:\n  options = CwsCommandOptions(cmd, project_dir=project_dir, module=module, service=service,\n  workspace=workspace, **command_options)\n  cmd_project_config.complete_options(options)\n  cmd.execute(options=options)\n  except Exception as err:\n  raise CwsClientError(str(err))\n  for opt in cmd.options:\n  call_execute = opt(call_execute)\n  client.command(cmd_name)(call_execute)\n  initial(ctx)\n  except CwsClientError as client_err:\n  sys.stderr.write(client_err.msg)\n  sys.exit(1)\n  except Exception as e:\n  sys.stderr.write(str(e))\n  sys.exit(1)", "target": 1, "info": "Null", "idx": 0}
{"func": "def add(self, tleads=None, mulst=None, tlst=None, dlst=None, updateq=True, lstq=True):\n  if lstq:\n  self.mulst = self.mulst if mulst is None else self.mulst + make_array(mulst, self.stateind.nleads)\n  self.tlst = self.tlst if tlst is None else self.tlst + make_array(tlst, self.stateind.nleads)\n  self.dlst = self.dlst if dlst is None else self.dlst + make_array_dlst(dlst, self.stateind.nleads)\n  if not tleads is None:\n  tleadsp = tleads if isinstance(tleads, dict) else make_tleads_dict(tleads)\n  self.Tba0 = construct_Tba(tleadsp, self.stateind, self.mtype, self.Tba0)\n  if updateq:\n  for j0 in tleadsp:\n  try:\n self.tleads[j0] += tleadsp[j0]\n  except: self.tleads.update({j0:tleadsp[j0]})", "target": 0, "info": "Null", "idx": 0}
{"func": "def add(self, tleads=None, mulst=None, tlst=None, dlst=None, updateq=True, lstq=True):\n  if lstq:\n  self.mulst = self.mulst if mulst is None else self.mulst + make_array(mulst, self.stateind.nleads)\n  self.tlst = self.tlst if tlst is None else self.tlst + make_array(tlst, self.stateind.nleads)\n  self.dlst = self.dlst if dlst is None else self.dlst + make_array_dlst(dlst, self.stateind.nleads)\n  if not tleads is None:\n  tleadsp = tleads if isinstance(tleads, dict) else make_tleads_dict(tleads)\n  self.Tba0 = construct_Tba(tleadsp, self.stateind, self.mtype, self.Tba0)\n  if updateq:\n  for j0 in tleadsp:\n  try:\n self.tleads[j0] += tleadsp[j0]\n  except: self.tleads.update({j0:tleads[j0]})", "target": 1, "info": "Null", "idx": 0}
{"func": "def updateVaultItems(self, uuid, versions):\n  logger = self.logger\n  logger.debug('updating %d versions for vault %s', len(versions), uuid)\n  assert uuid in self.vaults\n  vault = self.vaults[uuid]\n  name = vault.get('name', '')\n  pos = self.vault_order.find(name, uuid)\n  assert pos != -1\n  unlocker, noitems, items = self.vault_order.dataat(pos)\n  modifications = []\n  current_versions = self.versions[uuid]\n  current_order = self.version_order[uuid]\n  for version in versions:\n  vuuid = version['id']\n  key = sortkey(version)\n  present = not version['_envelope'].get('deleted', False)\n  cur_present = vuuid in current_versions\n  cur_deleted = current_versions.get('vuuid', {}) \\\n  .get('_envelope', {}).get('deleted', False)\n  if present:\n  if not cur_present:\n  modifications.append((key, 'new', version))\n  elif cur_present and not cur_deleted:\n  modifications.append((key, 'update', version))\n  elif cur_deleted:\n  modifications.append((key, 'undelete', version))\n  else:\n  if cur_present and not cur_deleted:\n  modifications.append((key, 'delete', version))\n  modifications.sort()\n  for key,mod,version in modifications:\n  vuuid = version['id']\n  curversion = current_versions.get(vuuid)\n  if mod in ('new', 'update'):\n  group = version.get('group', '')\n  grouppos = current_order.find(group)\n  if grouppos == -1:\n  item = GroupItem(uuid, group)\n  item.openStateChanged.connect(self.setGroupOpenState)\n  pos = current_order.insert(group, None, (item, None))\n  items.insertItem(pos, item)\n  self.groupAdded.emit(uuid, group)\n  if mod == 'new':\n  assert curversion is None\n  pos = current_order.find(key)\n  assert pos == -1\n  item = PasswordItem(uuid, version)\n  item.clicked.connect(self.changeCurrentItem)\n  search = searchkey(version)\n  pos = current_order.insert(key, vuuid, (item, search))\n  items.insertItem(pos, item)\n  elif mod == 'update':\n  assert curversion is not None\n  curkey = sortkey(curversion)\n  curpos = current_order.find(curkey, vuuid)\n  assert curpos != -1\n  item, search = current_order.dataat(curpos)\n  item.updateData(version)\n  if key != curkey:\n  current_order.removeat(curpos)\n  newpos = current_order.insert(key, vuuid, (item, search))\n  items.removeItem(item)\n  items.insertItem(newpos, item)\n  elif mod == 'delete':\n  assert curversion is not None\n  curkey = sortkey(current_versions[vuuid])\n  curpos = current_order.find(curkey, vuuid)\n  assert curpos != -1\n  item, search = current_order.dataat(curpos)\n  current_order.removeat(curpos)\n  items.removeItem(item)\n  item.hide(); item.destroy()\n  if self.current_item[uuid] == vuuid:\n  self.current_item[uuid] = None\n  if mod in ('update', 'delete'):\n  curgroup = curversion.get('group', '')\n  curpos = current_order.find(curgroup)\n  assert curpos != -1\n  prefix = '%s\\x00' % curgroup\n  if curpos == len(current_order)-1 or \\\n  not current_order.keyat(curpos+1).startswith(prefix):\n  item, search = current_order.dataat(curpos)\n  current_order.removeat(curpos)\n  items.removeItem(item)\n  item.hide(); item.destroy()\n  self.groupRemoved.emit(uuid, curgroup)\n  for version in versions:\n  current_versions[version['id']] = version\n  if uuid != self.current_vault:\n  return\n  if len(current_order) > 0:\n  self.stack.setCurrentWidget(items)\n  else:\n  self.stack.setCurrentWidget(items)\n  self.stack.setCurrentWidget(noitems)\n  self.currentVaultItemCountChanged.emit(len(current_order))", "target": 0, "info": "Null", "idx": 0}
{"func": "   def updateVaultItems(self, uuid, versions):\n  logger = self.logger\n  logger.debug('updating %d versions for vault %s', len(versions), uuid)\n  assert uuid in self.vaults\n  vault = self.vaults[uuid]\n  name = vault.get('name', '')\n  pos = self.vault_order.find(name, uuid)\n  assert pos != -1\n  unlocker, noitems, items = self.vault_order.dataat(pos)\n  modifications = []\n  current_versions = self.versions[uuid]\n  current_order = self.version_order[uuid]\n  for version in versions:\n  vuuid = version['id']\n  key = sortkey(version)\n  present = not version['_envelope'].get('deleted', False)\n  cur_present = vuuid in current_versions\n  cur_deleted = current_versions.get('vuuid', {}) \\\n  .get('_envelope', {}).get('deleted', False)\n  if present:\n  if not cur_present:\n  modifications.append((key, 'new', version))\n  elif cur_present and not cur_deleted:\n  modifications.append((key, 'update', version))\n  elif cur_deleted:\n  modifications.append((key, 'undelete', version))\n  else:\n  if cur_present and not cur_deleted:\n  modifications.append((key, 'delete', version))\n  modifications.sort()\n  for key,mod,version in modifications:\n  vuuid = version['id']\n  curversion = current_versions.get(vuuid)\n  if mod in ('new', 'update'):\n  group = version.get('group', '')\n  grouppos = current_order.find(group)\n  if grouppos == -1:\n  item = GroupItem(uuid, group)\n  item.openStateChanged.connect(self.setGroupOpenState)\n  pos = current_order.insert(group, None, (item, None))\n  items.insertItem(pos, item)\n  self.groupAdded.emit(uuid, group)\n  if mod == 'new':\n  assert curversion is None\n  pos = current_order.find(key)\n  assert pos == -1\n  item = PasswordItem(uuid, version)\n  item.clicked.connect(self.changeCurrentItem)\n  search = searchkey(version)\n  pos = current_order.insert(key, vuuid, (item, search))\n  items.insertItem(pos, item)\n  elif mod == 'update':\n  assert curversion is not None\n  curkey = sortkey(curversion)\n  curpos = current_order.find(curkey, vuuid)\n  assert curpos != -1\n  item, search = current_order.dataat(curpos)\n  item.updateData(version)\n  if key != curkey:\n  current_order.removeat(curpos)\n  newpos = current_order.insert(key, vuuid, (item, search))\n  items.removeItem(item)\n  items.insertItem(newpos, item)\n  elif mod == 'delete':\n  assert curversion is not None\n  curkey = sortkey(current_versions[vuuid])\n  curpos = current_order.find(curkey, vuuid)\n  assert curpos != -1\n  item, search = current_order.dataat(curpos)\n  current_order.removeat(curpos)\n  items.removeItem(item)\n  item.hide(); item.destroy()\n  if self.current_item[uuid] == vuuid:\n  self.current_item[uuid] = None\n  if mod in ('update', 'delete'):\n  curgroup = curversion.get('group', '')\n  curpos = current_order.find(curgroup)\n  assert curpos != -1\n  prefix = '%s\\x00' % curgroup\n  if curpos == len(current_order)-1 or \\\n  not current_order.keyat(curpos+1).startswith(prefix):\n  item, search = current_order.dataat(curpos)\n  current_order.removeat(curpos)\n  items.removeItem(item)\n  item.hide(); item.destroy()\n  self.groupRemoved.emit(uuid, group)\n  for version in versions:\n  current_versions[version['id']] = version\n  if uuid != self.current_vault:\n  return\n  if len(current_order) > 0:\n  self.stack.setCurrentWidget(items)\n  else:\n  self.stack.setCurrentWidget(items)\n  self.stack.setCurrentWidget(noitems)\n  self.currentVaultItemCountChanged.emit(len(current_order))", "target": 1, "info": "Null", "idx": 0}
{"func": "def require_relationship(resource_list, data):\n  for resource in resource_list:\n  if resource not in data:\n  raise UnprocessableEntity(\"A valid relationship with {} resource is required\".format(resource),\n    {'pointer': '/data/relationships/{}'.format(resource)})", "target": 0, "info": "Null", "idx": 0}
{"func": "def require_relationship(resource_list, data):\n  for resource in resource_list:\n  if resource not in data:\n  raise UnprocessableEntity({'pointer': '/data/relationships/{}'.format(resource)},\n    \"A valid relationship with {} resource is required\".format(resource))", "target": 1, "info": "Null", "idx": 0}
{"func": "def setup_logging(handler, exclude=['raven', 'sentry.errors']): \tlogger = logging.getLogger()\n  if handler.__class__ in map(type, logger.handlers):\n  return False\n   logger.addHandler(handler)\n  for logger_name in exclude:\n  logger = logging.getLogger(logger_name)\n  logger.propagate = False\n  logger.addHandler(logging.StreamHandler())\n  return True", "target": 0, "info": "Null", "idx": 0}
{"func": "def setup_logging(handler, exclude=['raven', 'sentry.errors']): \tlogger = logging.getLogger()\n  if handler.__class__ not in map(type, logger.handlers):\n  return False\n   logger.addHandler(handler)\n  for logger_name in exclude:\n  logger = logging.getLogger(logger_name)\n  logger.propagate = False\n  logger.addHandler(logging.StreamHandler())\n  return True", "target": 1, "info": "Null", "idx": 0}
{"func": "def register_transport(self, transport):\n  if not hasattr(transport, 'scheme') or not hasattr(transport.scheme, '__iter__'):\n  raise AttributeError('Transport %s must have a scheme list', transport.__class__.__name__)", "target": 0, "info": "Null", "idx": 0}
{"func": "def register_transport(self, transport):\n  if not hasattr(transport, 'scheme') and not hasattr(transport.scheme, '__iter__'):\n  raise AttributeError('Transport %s must have a scheme list', transport.__class__.__name__)\n   for scheme in transport.scheme:\n  self.register_scheme(scheme, transport)", "target": 1, "info": "Null", "idx": 0}
{"func": "def error(\n  self,\n  statement,\n  message=None,\n  variable=None,\n  line=None,\n  column=None,\n  ):\n   if not message:\n   message = self.assign_msg\n   if not variable:\n   variable = statement.id\n   if not line:\n   line = statement.lineno\n   if not column:\n  column = statement.col_offset\n   return (\n  line,\n  column,\n  message.format(variable),\n  type(self),\n  )", "target": 0, "info": "Null", "idx": 0}
{"func": "def error(\n  self,\n  statement,\n  message=None,\n  variable=None,\n  line=None,\n  column=None,\n  ):\n  if not message:\n  message = self.assign_msg\n  if not variable:\n  column = statement.id\n  if not line:\n  line = statement.lineno\n  if not column:\n  column = statement.col_offset\n  return (\n  line,\n  column,\n  message.format(variable),\n  type(self),\n  )", "target": 1, "info": "Null", "idx": 0}
{"func": "   def check_line(self, line, old_approach):\n  found = line.find(old_approach)\n  if found == -1:\n  return found\n   next_character_position = found + len(old_approach) + 1\n  if next_character_position >= len(line):\n  return found", "target": 0, "info": "Null", "idx": 0}
{"func": " def check_line(self, line, old_approach):\n  found = line.find(old_approach)\n  if found == -1:\n  return found\n   next_character_position = found + len(old_approach) + 1\n  if next_character_position > len(line):\n  return found", "target": 1, "info": "Null", "idx": 0}
{"func": "def click_link_in_email(context, address):\n  mails = context.mail.user_messages(address)\n  assert mails, u'message not found'\n  mail = email.message_from_string(mails[0])\n  links = URL_RE.findall(str(mail).replace('=\\n', ''))\n  assert links, u'link not found'\n  url = links[0]\n  context.browser.visit(url)", "target": 0, "info": "Null", "idx": 0}
{"func": "def click_link_in_email(context, address):\n  mails = context.mail.user_messages(address)\n  assert mails, u'message not found'\n  mail = email.message_from_string(mails[-1])\n  links = URL_RE.findall(str(mail).replace('=\\n', ''))\n  assert links, u'link not found'\n  url = links[0]\n  context.browser.visit(url)", "target": 1, "info": "Null", "idx": 0}
{"func": " def percentiles(self, start, percentiles = [10, 25, 75, 90]):\n  end = start + timedelta(days=7)\n  ts_start, ts_end = utils.timestamp_from_datetime([start, end])\n  this_week_indices = where((self._data['timestamp'] < ts_end) & (self._data['timestamp'] >= ts_start))\n  this_week = self._data[this_week_indices]\n  result = {}\n  pred = self.baseline_model.prediction(this_week)\n  for p in percentiles:\n  result[p] = pred + self.baseline_model.percentile_in_place(this_week, p)\n  return result", "target": 0, "info": "Null", "idx": 0}
{"func": "def percentiles(self, start, percentiles = [10, 25, 75, 90]):\n  end = start + timedelta(days=7)\n  ts_start, ts_end = utils.timestamp_from_datetime([start, end])\n  this_week_indices = where((self._data['timestamp'] < ts_end) & (self._data['timestamp'] >= ts_start))\n  this_week = self._data[this_week_indices]\n  result = {}\n  pred = self.baseline_model.prediction(this_week)\n  for p in percentiles:\n  result[p] = pred + self.baseline_model.percentile_in_place(p, this_week)\n  return result", "target": 1, "info": "Null", "idx": 0}
{"func": "def cancelPZ(arg1, tol=1e-6):\n  if not isinstance(arg1, lti):\n  arg1 = lti(*arg1)\n  z = copy.copy(arg1.zeros)\n  p = copy.copy(arg1.poles)\n  k = arg1.gain\n  for i in range(max(z.shape) - 1, -1, -1):\n  d = z[i] - p\n  cancel = np.nonzero(np.abs(d) < tol)[0]\n  if cancel.size:\n  p = np.delete(p, cancel[0])\n  z = np.delete(z, i)\n  return z, p, k", "target": 0, "info": "Null", "idx": 0}
{"func": "def cancelPZ(arg1, tol=1e-6):\n  if not isinstance(arg1, lti):\n  arg1 = lti(*arg1)\n  z = copy.copy(arg1.zeros)\n  p = copy.copy(arg1.poles)\n  k = arg1.gain\n  for i in range(max(z.shape) - 1, 0, -1):\n  d = z[i] - p\n  cancel = np.nonzero(np.abs(d) < tol)[0]\n  if cancel.size:\n  p = np.delete(p, cancel[0])\n  z = np.delete(z, i)\n  return z, p, k", "target": 1, "info": "Null", "idx": 0}
{"func": "def get_entropy_for_each_line(trained_model: TrainedModel,\n    file: str,\n    entropy_aggregator: Callable[[List[float], List[int]], Union[float, List[float]]],\n    verbose: bool = False) -> Union[List[float], List[List[float]]]:\n  prep_lines_and_entropies: List[Dict[str, Union[str, List[str], List[float], float]]] = []\n  with open(file, 'r') as f:\n  _, extension = os.path.splitext(file)[1:]\n  for line in f:\n  time_measurer.tick(\"Inference\")\n  prep_line, entropies, word_boundaries = trained_model.get_entropies_for_text(line, extension[1:])\n  time_measurer.tock(\"Inference\")\n  line_entropy = entropy_aggregator(entropies, word_boundaries)\n  prep_lines_and_entropies.append({\n  'text': line,\n  'prep_text': prep_line,\n  'entropies': entropies,\n  'line_entropy': line_entropy\n  })\n  if verbose:\n  for line in prep_lines_and_entropies:\n  print(line['text'])\n  print(line['line_entropy'])\n  print(f\"{[(prep_token, token_entropy) for prep_token, token_entropy in zip(line['prep_text'], line['entropies'])]}\")\n  print(\"=============\")\n  return list(map(lambda e: e['line_entropy'], prep_lines_and_entropies))", "target": 0, "info": "Null", "idx": 0}
{"func": "def get_entropy_for_each_line(trained_model: TrainedModel,\n    file: str,\n    entropy_aggregator: Callable[[List[float], List[int]], Union[float, List[float]]],\n    verbose: bool = False) -> Union[List[float], List[List[float]]]:\n  prep_lines_and_entropies: List[Dict[str, Union[str, List[str], List[float], float]]] = []\n  with open(file, 'r') as f:\n  _, extension = os.path.splitext(file)[1:]\n  for line in f:\n  time_measurer.tick(\"Inference\")\n  prep_line, entropies, word_boundaries = trained_model.get_entropies_for_text(line, extension[1:])\n  time_measurer.tock(\"Inference\")\n  line_entropy = entropy_aggregator(entropies, word_boundaries)\n  prep_lines_and_entropies.append({\n  'text': line,\n  'prep_text': prep_line,\n  'entropies': entropies,\n  'line_entropy': line_entropy\n  })\n  if not verbose:\n  for line in prep_lines_and_entropies:\n  print(line['text'])\n  print(line['line_entropy'])\n  print(f\"{[(prep_token, token_entropy) for prep_token, token_entropy in zip(line['prep_text'], line['entropies'])]}\")\n  print(\"=============\")\n  return list(map(lambda e: e['line_entropy'], prep_lines_and_entropies))", "target": 1, "info": "Null", "idx": 0}
{"func": "def __call__(self, obj) -> Union['Action', Type]:\n  if isinstance(obj, type):\n  api_type = obj\n  self._api_object = api_type(self.app)\n  predicate = lambda x: (\n  (ismethod(x) and x.__name__[0] != '_')\n  or isinstance(x, Action)\n  )\n  for k, v in getmembers(self.api_object, predicate=predicate):\n  if isinstance(v, Action):\n  existing_action = self.app.actions.get(v.name)\n  if not existing_action:\n  continue\n  kwargs = self.kwargs.copy()\n  kwargs.update(existing_action.decorator.kwargs)\n  new_decorator = type(self)(self.app, *self.args, **kwargs)\n  new_decorator._api_object = self._api_object\n  new_decorator.setup_action(v.target, True)\n  else:\n  self.setup_action(v.__func__, False)\n  return api_type\n  else:\n  func = obj\n  action = self.setup_action(func, False)\n  return func", "target": 0, "info": "Null", "idx": 0}
{"func": "def __call__(self, obj) -> Union['Action', Type]:\n  if isinstance(obj, type):\n  api_type = obj\n  self._api_object = api_type(self.app)\n  predicate = lambda x: (\n  (ismethod(x) and x.__name__[0] != '_')\n  or isinstance(x, Action)\n  )\n  for k, v in getmembers(self.api_object, predicate=predicate):\n  if isinstance(v, Action):\n  existing_action = self.app.actions.get(v.name)\n  if not existing_action:\n  continue\n  kwargs = self.kwargs.copy()\n  kwargs.update(existing_action.decorator.kwargs)\n  new_decorator = type(self)(self.app, *self.args, **kwargs)\n  new_decorator._api_object = self._api_object\n  new_decorator.setup_action(v.target, True)\n  else:\n  self.setup_action(v.__func__, False)\n  return api_type\n  else:\n  func = obj\n  action = self.setup_action(func, False)\n  return action", "target": 1, "info": "Null", "idx": 0}
{"func": "def _aggregate_related(\n  self,\n  bizobj: 'BizType',\n  to_create: Dict,\n  to_update: Dict,\n  ) -> None:\n  for k, v in bizobj.related.items():\n  rel = bizobj.relationships[k]\n  if not v:\n  continue\n  if rel.many:\n  for x in v:\n  if x._id is None:\n  to_create[v.biz_type].append(x)\n  elif x.dirty:\n  to_update[v.biz_type].append(x)\n  for x in v:\n  self._aggregate_related(x, to_create, to_update)\n  else:\n  v_type = v.__class__\n  if v._id is None:\n  to_create[v_type].append(v)\n  elif v.dirty:\n  to_update[v_type].append(x)\n  self._aggregate_related(v, to_create, to_update)", "target": 0, "info": "Null", "idx": 0}
{"func": "def _aggregate_related(\n  self,\n  bizobj: 'BizType',\n  to_create: Dict,\n  to_update: Dict,\n  ) -> None:\n  for k, v in bizobj.related.items():\n  rel = bizobj.relationships[k]\n  if not v:\n  continue\n  if rel.many:\n  for x in v:\n  if x._id is None:\n  to_create[v.biz_type].append(x)\n  elif x.dirty:\n  to_update[v.biz_type].append(x)\n  for x in v:\n  self._aggregate_related(x, to_create, to_update)\n  else:\n  v_type = v.__class__\n  if v._id is None:\n  to_create[v_type].append(v)\n  elif x.dirty:\n  to_update[v_type].append(x)\n  self._aggregate_related(v, to_create, to_update)", "target": 1, "info": "Null", "idx": 0}
{"func": "def update(self, _id, data: dict) -> dict:\n  file_path = self.file_path(_id)\n  if not self.exists(_id):\n  raise Exception('File does not exist, {}'.format(file_path))\n  if '_id' not in data:\n  data['_id'] = _id\n   cur_data = self.fetch(_id)\n  new_data = DictUtils.merge(cur_data, data)\n  Yaml.to_file(file_path=file_path, data=new_data)\n  return new_data", "target": 0, "info": "Null", "idx": 0}
{"func": "def update(self, _id, data: dict) -> dict:\n  file_path = self.file_path(_id)\n  if not self.exists(_id):\n  raise Exception('File does not exist, {}'.format(file_path))\n  if '_id' not in data:\n  data['_id'] = _id\n   cur_data = self.fetch(_id)\n  new_data = DictUtils.merge(cur_data, data)\n  Yaml.to_file(file_path=file_path, data=new_data)\n  return data", "target": 1, "info": "Null", "idx": 0}
{"func": "def _aggregate_related(\n  self,\n  bizobj: 'BizType',\n  to_create: Dict,\n  to_update: Dict,\n  ) -> None:\n  for k, v in bizobj.related.items():\n  rel = bizobj.relationships[k]\n  if not v:\n  continue\n  if rel.many:\n  for x in v:\n  if x._id is None:\n  to_create[v.biz_type].append(x)\n  elif x.dirty:\n  to_update[v.biz_type].append(x)\n  for x in v:\n  self._aggregate_related(x, to_create, to_update)\n  else:\n  v_type = v.__class__\n  if v._id is None:\n  to_create[v_type].append(v)\n  elif v.dirty:\n  to_update[v_type].append(v)\n  self._aggregate_related(v, to_create, to_update)", "target": 0, "info": "Null", "idx": 0}
{"func": "def _aggregate_related(\n  self,\n  bizobj: 'BizType',\n  to_create: Dict,\n  to_update: Dict,\n  ) -> None:\n  for k, v in bizobj.related.items():\n  rel = bizobj.relationships[k]\n  if not v:\n  continue\n  if rel.many:\n  for x in v:\n  if x._id is None:\n  to_create[v.biz_type].append(x)\n  elif x.dirty:\n  to_update[v.biz_type].append(x)\n  for x in v:\n  self._aggregate_related(x, to_create, to_update)\n  else:\n  v_type = v.__class__\n  if v._id is None:\n  to_create[v_type].append(v)\n  elif v.dirty:\n  to_update[v_type].append(x)\n  self._aggregate_related(v, to_create, to_update)", "target": 1, "info": "Null", "idx": 0}
{"func": "def on_authorization(self, context: Dict, arguments: Dict):\n  is_authorized = False\n  lhs_exc = self._lhs(context, arguments)\n  if self._op == OP_CODE.AND:\n  if lhs_exc is None:\n  rhs_exc = self._rhs(context, arguments)\n  if rhs_exc is not None:\n  return rhs_exc\n  else:\n  return lhs_exc\n  elif self._op == OP_CODE.OR:\n  rhs_exc = self._rhs(context, arguments)\n  if rhs_exc is not None and lhs_exc is not None:\n  return CompositeGuardException(lhs_exc, rhs_exc)\n  elif self._op == OP_CODE.NOT:\n  if lhs_exc is None:\n  return lhs_exc\n  else:\n  return ValueError(f'op not recognized, \"{self._op}\"')\n  return None", "target": 0, "info": "Null", "idx": 0}
{"func": "def on_authorization(self, context: Dict, arguments: Dict):\n  is_authorized = False\n  lhs_exc = self._lhs(context, arguments)\n  if self._op == OP_CODE.AND:\n  if lhs_exc is None:\n  rhs_exc = self._rhs(context, arguments)\n  if rhs_exc is not None:\n  return rhs_exc\n  else:\n  return lhs_exc\n  elif self._op == OP_CODE.OR:\n  rhs_exc = self._rhs(context, arguments)\n  if rhs_exc is not None and lhs_exc is not None:\n  return CompositeGuardException(lhs_exc, rhs_exc)\n  elif self._op == OP_CODE.NOT:\n  if lhs_exc is not None:\n  return lhs_exc\n  else:\n  return ValueError(f'op not recognized, \"{self._op}\"')\n  return None", "target": 1, "info": "Null", "idx": 0}
{"func": "def _load(self, data, kwargs_data):\n  data = data or {}\n  data.update(kwargs_data)\n  self._related_bizobjs = {}\n  for rel in self.relationships.values():\n  load_from = rel.load_from or rel.name\n  related_data = data.pop(load_from, None)\n  if related_data is None:\n  continue\n  if rel.many:\n  related_bizobj_list = []\n  for obj in related_data:\n  if isinstance(obj, rel.target):\n  related_bizobj_list.append(obj)\n  else:\n  related_bizobj_list.append(\n  rel.target(obj)\n  )\n   self._related_bizobjs[rel.name] = related_bizobj_list\n  else:\n  if not is_bizobj(related_data):\n  assert isinstance(related_data, dict)\n  related_bizobj = rel.target(related_data)\n  else:\n  related_bizobj = related_data\n  self._related_bizobjs[rel.name] = related_bizobj\n  result, error = self.schema.process(data)\n  if error:\n  raise Exception(str(error))\n  return DirtyDict(result)", "target": 0, "info": "Null", "idx": 0}
{"func": "def _load(self, data, kwargs_data):\n  data = data or {}\n  data.update(kwargs_data)\n  self._related_bizobjs = {}\n  for rel in self.relationships.values():\n  load_from = rel.load_from or rel.name\n  related_data = data.pop(load_from, None)\n  if related_data is None:\n  continue\n  if rel.many:\n  related_bizobj_list = []\n  for obj in related_data:\n  if isinstance(obj, rel.target):\n  related_bizobj_list.append(obj)\n  else:\n  related_bizobj_list.append(\n  rel.target(related_data)\n  )\n   self._related_bizobjs[rel.name] = related_bizobj_list\n  else:\n  if not is_bizobj(related_data):\n  assert isinstance(related_data, dict)\n  related_bizobj = rel.target(related_data)\n  else:\n  related_bizobj = related_data\n  self._related_bizobjs[rel.name] = related_bizobj\n  result, error = self.schema.process(data)\n  if error:\n  raise Exception(str(error))\n  return DirtyDict(result)", "target": 1, "info": "Null", "idx": 0}
{"func": "async def build(self, building: UnitTypeId, near: Union[Point2, Point3], max_distance: int=20, unit: Optional[Unit]=None, random_alternative: bool=True, placement_step: int=2):\n  if isinstance(near, Unit):\n  near = near.position.to2\n  elif near is not None:\n  near = near.to2\n  p = await self.find_placement(building, near.rounded, max_distance, random_alternative, placement_step)\n  if p is None:\n  return ActionResult.CantFindPlacementLocation\n   unit = unit or self.select_build_worker(p)\n  if unit is None or not self.can_afford(building):\n  return ActionResult.Error\n  return await self.do(unit.build(building, p))", "target": 0, "info": "Null", "idx": 0}
{"func": "async def build(self, building: UnitTypeId, near: Union[Point2, Point3], max_distance: int=20, unit: Optional[Unit]=None, random_alternative: bool=True, placement_step: int=2):\n  if isinstance(near, Unit):\n  near = near.position.to2\n  elif near is not None:\n  near = near.to2\n  p = await self.find_placement(building, near.rounded, max_distance, random_alternative, placement_step)\n  if p is None:\n  return ActionResult.CantFindPlacementLocation\n   unit = unit or self.select_build_worker(p)\n  if unit is None or self.can_afford(building):\n  return ActionResult.Error\n  return await self.do(unit.build(building, p))", "target": 1, "info": "Null", "idx": 0}
{"func": "def form(request, template_name='contato/contato_form.html',\n  template_email='contato/email.txt', dict={}):\n  contato = ContatoForm(request.POST or None)\n  if contato.is_valid():\n  nome = contato.data.get('nome')\n  email = contato.data.get('email')\n  assunto = contato.data.get('assunto')\n  mensagem = contato.data.get('mensagem')\n  mensagem = {'nome': nome, 'email': email, 'mensagem': mensagem}\n  if dict:\n  mensagem.update(dict)\n  enviar_email(settings.DEFAULT_FROM_EMAIL, email, nome,\n  assunto, template_email, mensagem)\n  return redirect(reverse('contato_success'))\n  return render_to_response(template_name, {'form': contato},\n  context_instance=RequestContext(request))", "target": 0, "info": "Null", "idx": 0}
{"func": "def form(request, template_name='contato/contato_form.html',\n  template_email='contato/email.txt', dict={}):\n  contato = ContatoForm(request.POST or None)\n  if contato.is_valid():\n  nome = contato.data.get('nome')\n  email = contato.data.get('email')\n  assunto = contato.data.get('assunto')\n  mensagem = contato.data.get('mensagem')\n  mensagem = {'nome': nome, 'email': email, 'mensagem': mensagem}\n  if dict:\n  mensagem.update(dict)\n  enviar_email(email, settings.DEFAULT_FROM_EMAIL, nome,\n  assunto, template_email, mensagem)\n  return redirect(reverse('contato_success'))\n  return render_to_response(template_name, {'form': contato},\n  context_instance=RequestContext(request))", "target": 1, "info": "Null", "idx": 0}
{"func": "\tdef _set_cache_brute_(self, attr):", "target": 0, "info": "Null", "idx": 0}
{"func": "def _set_cache_brute_(self, attr):", "target": 1, "info": "Null", "idx": 0}
{"func": "def convert_all_validator(self, schema_node, validator):\n  converted = None\n  if isinstance(validator, colander.All):\n  converted = {}\n  for v in validator.validators:\n  ret = self(schema_node, v)\n  converted.update(ret)\n  return converted", "target": 0, "info": "Null", "idx": 0}
{"func": "def convert_all_validator(self, schema_node, validator):\n  converted = None\n  if isinstance(validator, colander.All):\n  converted = {}\n  for v in validator.validators:\n  ret = self(schema_node, validator)\n  converted.update(ret)\n  return converted", "target": 1, "info": "Null", "idx": 0}
{"func": "def ddot(L, R, left=True, out=None):\n  r\n  L = asarray(L, float)\n  R = asarray(R, float)\n  ok = min(L.ndim, R.ndim) == 1 and max(L.ndim, R.ndim) == 2\n  if not ok:\n  raise ValueError(\"Wrong array layout. One array should have\" +\n   \" ndim=1 and the other one ndim=2.\")\n  if L.ndim == 1:\n  if out is None:\n  out = copy(R)\n  return multiply(L[:, newaxis], R, out=out)\n  else:\n  if out is None:\n  out = copy(L)\n  return multiply(L, R, out=out)", "target": 0, "info": "Null", "idx": 0}
{"func": "def ddot(L, R, left=True, out=None):\n  r\n  L = asarray(L, float)\n  R = asarray(R, float)\n  ok = min(L.ndim, R.ndim) == 1 and max(L.ndim, R.ndim) == 2\n  if not ok:\n  raise ValueError(\"Wrong array layout. One array should have\" +\n   \" ndim=1 and the other one ndim=2.\")\n  if L.ndim == 1:\n  if out is None:\n  out = copy(R)\n  return multiply(L[:, newaxis], R, out=out)\n  else:\n  if out is None:\n  out = copy(L)\n  return multiply(out, R, out=out)", "target": 1, "info": "Null", "idx": 0}
{"func": "def get_relationship(self, rel_type, rel_key):\n  try:\n  values = self.get(RELATIONSHIP_CF, ENDPOINT_NAME_TEMPLATE % (rel_type, rel_key))\n   except NotFoundException:\n  raise NodeNotFoundException()\n  source_node_key = None\n  source_node_type = None\n  source_attributes = {}\n  for column in values.keys():\n  value = values[column]\n  if column == 'source__type':\n  source_node_type = value\n  elif column == 'source__key':\n  source_node_key = value\n  elif column.startswith('source__'):\n  source_attributes[column[8:]] = value\n  source = prim.Node(self, source_node_type, source_node_key, source_attributes)\n  rel_key = RELATIONSHIP_KEY_PATTERN % (rel_type, rel_key)\n  return self.get_outgoing_relationship(rel_type, source, (rel_key, values))", "target": 0, "info": "Null", "idx": 0}
{"func": "def get_relationship(self, rel_type, rel_key):\n  try:\n  values = self.get(RELATIONSHIP_CF, ENDPOINT_NAME_TEMPLATE % (rel_type, rel_key))\n   except NotFoundException:\n  raise NodeNotFoundException()\n  source_node_key = None\n  source_node_type = None\n  source_attributes = {}\n  for column in values.keys():\n  value = values[column]\n  if column == 'source__type':\n  source_node_type = value\n  elif column == 'source__key':\n  source_node_key = value\n  elif column.startswith('source__'):\n  source_attributes[column[8:]] = value\n  source = prim.Node(self, source_node_type, source_node_key, values)\n  rel_key = RELATIONSHIP_KEY_PATTERN % (rel_type, rel_key)\n  return self.get_outgoing_relationship(rel_type, source, (rel_key, values))", "target": 1, "info": "Null", "idx": 0}
{"func": "def iterrecords(bind=ENGINE, windowsize=WINDOWSIZE, skip_unknown=True):\n  log.info('enter raw records')\n  log.debug('bind: %r', bind)\n  select_files = sa.select([File.path]).order_by(File.id)\n  select_values = sa.select([\n  Value.file_id, Option.section, Option.option,\n  Option.is_lines, Value.value,\n  ])\\\n  .select_from(sa.join(Value, Option))\\\n  .order_by('file_id', 'section', Value.line, 'option')\n  if skip_unknown:\n  select_values.append_whereclause(Option.is_lines != None)\n  groupby = (('file_id',), ('section',), ('option', 'is_lines'))\n  groupby = itertools.starmap(_tools.groupby_attrgetter, groupby)\n  groupby_file, groupby_section, groupby_option = groupby\n  with bind.connect() as conn:\n  select_files.bind = conn\n  select_values.bind = conn\n  for in_slice in window_slices(File.id, size=windowsize, bind=conn):\n  if log.level <= logging.DEBUG:\n  where = literal_compile(in_slice(File.id))\n  log.debug('fetch rows %r', where.string)\n  files = select_files.where(in_slice(File.id)).execute().fetchall()\n  values = select_values.where(in_slice(Value.file_id)).execute().fetchall()\n  for (path,), (_, values) in zip(files, groupby_file(values)):\n  record = {\n  s: {o: [l.value for l in lines] if is_lines else next(lines).value\n for (o, is_lines), lines in groupby_option(sections)}\n  for s, sections in groupby_section(values)}\n  yield tuple(path.split('/')), record\n  log.info('exit raw records')", "target": 0, "info": "Null", "idx": 0}
{"func": "def iterrecords(bind=ENGINE, windowsize=WINDOWSIZE, skip_unknown=True):\n  log.info('enter raw records')\n  log.debug('bind: %r', bind)\n  select_files = sa.select([File.path]).order_by(File.id)\n  select_values = sa.select([\n  Value.file_id, Option.section, Option.option,\n  Option.is_lines, Value.value,\n  ])\\\n  .select_from(sa.join(Value, Option))\\\n  .order_by('file_id', 'section', Value.line, 'option')\n  if skip_unknown:\n  select_values.append_whereclause(Option.is_lines != None)\n  groupby = (('file_id',), ('section',), ('option', 'is_lines'))\n  groupby = itertools.starmap(_tools.groupby_attrgetter, groupby)\n  groupby_file, groupby_section, groupby_option = groupby\n  with bind.connect() as conn:\n  select_files.bind = conn\n  select_values.bind = conn\n  for in_slice in window_slices(File.id, size=windowsize, bind=bind):\n  if log.level <= logging.DEBUG:\n  where = literal_compile(in_slice(File.id))\n  log.debug('fetch rows %r', where.string)\n  files = select_files.where(in_slice(File.id)).execute().fetchall()\n  values = select_values.where(in_slice(Value.file_id)).execute().fetchall()\n  for (path,), (_, values) in zip(files, groupby_file(values)):\n  record = {\n  s: {o: [l.value for l in lines] if is_lines else next(lines).value\n for (o, is_lines), lines in groupby_option(sections)}\n  for s, sections in groupby_section(values)}\n  yield tuple(path.split('/')), record\n  log.info('exit raw records')", "target": 1, "info": "Null", "idx": 0}
{"func": "def hit_highlight(self, hits = [], **kwargs):\n  if not hits and not kwargs.get('query'):\n  return self.text\n  if not hits and kwargs.get('query'):\n  stripped_text   =   strip_tags(self.text)\n  for match in re.finditer(u'%s' % kwargs.get('query'), stripped_text):\n  hits.append(match.span())\n  if hits and not type(hits) == list:\n  raise Exception('The syntax for the hit_highlight method has changed. You must pass in a list of lists containing the indices of the strings you want to match.')\n  tag_name = kwargs.get('tag', DEFAULT_HIGHLIGHT_TAG)\n  tags = [u'<%s>' % tag_name, u'</%s>' % tag_name]\n  text = self.text\n  chunks = re.split(r'[<>]', text)\n  text_chunks = []\n  for index, chunk in enumerate(chunks):\n  if not index % 2:\n  text_chunks.append(chunk)\n  for hit in sorted(hits, key = lambda chunk: chunk[1], reverse = True):\n  hit_start, hit_end = hit\n  placed = 0\n  for index, chunk in enumerate(chunks):\n  if placed == 2:\n  continue\n  if index % 2:\n  continue\n  chunk_start = len(u''.join(text_chunks[0:index // 2]))\n  chunk_end = chunk_start + len(chunk)\n  if hit_start >= chunk_start and hit_start < chunk_end:\n  chunk = chunk[:hit_start - chunk_start] + tags[0] + chunk[hit_start - chunk_start:]\n  if hit_end <= chunk_end:\n  hit_end += len(tags[0])\n  chunk_end += len(tags[0])\n  placed = 1\n  if hit_end > chunk_start and hit_end <= chunk_end:\n  chunk = chunk[:hit_end - chunk_start] + tags[1] + chunk[hit_end - chunk_start:]\n  placed = 2\n  chunks[index] = chunk\n  if placed == 1:\n  chunks[-1] = chunks[-1] + tags[1]\n  result = []\n  for index, chunk in enumerate(chunks):\n  if index % 2:\n  result.append(u'<%s>' % chunk)\n  else:\n  result.append(chunk)\n  self.text = u''.join(result)\n  return self.text", "target": 0, "info": "Null", "idx": 0}
{"func": "def hit_highlight(self, hits = [], **kwargs):\n  if not hits and not kwargs.get('query'):\n  return self.text\n  if not hits and kwargs.get('query'):\n  stripped_text   =   strip_tags(self.text)\n  for match in re.finditer(u'%s' % kwargs.get('query'), stripped_text):\n  hits.append(match.span())\n  if hits and not type(hits) == list:\n  raise Exception('The syntax for the hit_highlight method has changed. You must pass in a list of lists containing the indices of the strings you want to match.')\n  tag_name = kwargs.get('tag', DEFAULT_HIGHLIGHT_TAG)\n  tags = [u'<%s>' % tag_name, u'</%s>' % tag_name]\n  text = self.text\n  chunks = re.split(r'[<>]', text)\n  text_chunks = []\n  for index, chunk in enumerate(chunks):\n  if not index % 2:\n  text_chunks.append(chunk)\n  for hit in sorted(hits, key = lambda chunk: chunk[1], reverse = True):\n  hit_start, hit_end = hit\n  placed = 0\n  for index, chunk in enumerate(chunks):\n  if placed == 2:\n  continue\n  if index % 2:\n  continue\n  chunk_start = len(u''.join(text_chunks[0:index / 2]))\n  chunk_end = chunk_start + len(chunk)\n  if hit_start >= chunk_start and hit_start < chunk_end:\n  chunk = chunk[:hit_start - chunk_start] + tags[0] + chunk[hit_start - chunk_start:]\n  if hit_end <= chunk_end:\n  hit_end += len(tags[0])\n  chunk_end += len(tags[0])\n  placed = 1\n  if hit_end > chunk_start and hit_end <= chunk_end:\n  chunk = chunk[:hit_end - chunk_start] + tags[1] + chunk[hit_end - chunk_start:]\n  placed = 2\n  chunks[index] = chunk\n  if placed == 1:\n  chunks[-1] = chunks[-1] + tags[1]\n  result = []\n  for index, chunk in enumerate(chunks):\n  if index % 2:\n  result.append(u'<%s>' % chunk)\n  else:\n  result.append(chunk)\n  self.text = u''.join(result)\n  return self.text", "target": 1, "info": "Null", "idx": 0}
{"func": "  def _interdistance(self, first: np.ndarray, second: np.ndarray) -> np.ndarray:\n  if first is not self._last:\n  self._last = first\n  self._last_ranks = np.apply_along_axis(st.rankdata, 0, first)\n  second_ranks = np.apply_along_axis(st.rankdata, 0, second)\n  return dist.cdist(self._last_ranks, second_ranks, metric='correlation')", "target": 0, "info": "Null", "idx": 0}
{"func": "def _interdistance(self, first: np.ndarray, second: np.ndarray) -> np.ndarray:\n  if first is not self._last:\n  self._last = first\n  self._last_ranks = np.apply_along_axis(st.rankdata, 0, first)\n  second_ranks = np.apply_along_axis(st.rankdata, 0, first)\n  return dist.cdist(self._last_ranks, second_ranks, metric='correlation')", "target": 1, "info": "Null", "idx": 0}
{"func": "def main(_):   flags.mark_flags_as_required([\"task\"])   if FLAGS.module_import:\n  import_modules(FLAGS.module_import)   gin.parse_config_files_and_bindings(FLAGS.gin_file, FLAGS.gin_param)   total_examples = 0   tf.enable_eager_execution()   task = t5.data.TaskRegistry.get(FLAGS.task)   files = task.tfds_dataset.files(FLAGS.split)   def _example_to_string(ex):\n  key_to_string = {}\n  for k in (\"inputs\", \"targets\"):\n    if k in ex:\n  v = ex[k].numpy()\n  key_to_string[k] = (\n  \" \".join(str(i) for i in v) if FLAGS.tokenize\n  else v.decode(\"utf-8\"))\n    else:\n  key_to_string[k] = \"\"\n  return FLAGS.format_string.format(**key_to_string)", "target": 0, "info": "Null", "idx": 0}
{"func": "def main(_):   flags.mark_flags_as_required([\"task\"])   if FLAGS.module_import:\n  import_modules(FLAGS.module_import)   gin.parse_config_files_and_bindings(FLAGS.gin_file, FLAGS.gin_param)   total_examples = 0   tf.enable_eager_execution()   task = t5.data.TaskRegistry.get(FLAGS.task)   files = task.tfds_dataset.files(FLAGS.split)   def _example_to_string(ex):\n  key_to_string = {}\n  for k in (\"inputs\", \"targets\"):\n    if k in ex:\n  v = ex[k].numpy()\n  key_to_string[k] = (\n  \" \".join(str(i) for i in v) if FLAGS.tokenize\n  else v.decode(\"utf-8\"))\n    else:\n  v[k] = \"\"\n  return FLAGS.format_string.format(**key_to_string)", "target": 1, "info": "Null", "idx": 0}
{"func": "def project_electrodes(electrodes_file, freesurfer_path):\n  chans = read_tsv(electrodes_file.filename)\n  chan = Channels(\n  [x['name'] for x in chans],\n  array([(float(x['x']), float(x['y']), float(x['z'])) for x in chans]))\n  fs = Freesurfer(freesurfer_path / ('sub-' + electrodes_file.subject))\n  if median(chan.return_xyz()[:, 0]) > 0:\n  surf = fs.read_brain().rh\n  else:\n  surf = fs.read_brain().lh\n  chan = snap_to_surface(surf, chan)\n  tsv_electrodes = Path(electrodes_file.filename).parent / f'sub-{electrodes_file.subject}_ses-{electrodes_file.session}_acq-{electrodes_file.acquisition}projected_electrodes.tsv'\n  with tsv_electrodes.open('w') as f:\n  f.write('name\\tx\\ty\\tz\\ttype\\tsize\\tmaterial\\n')\n  for _chan in chan.chan:\n  xyz = \"\\t\".join(f'{x:f}' for x in _chan.xyz)\n  one_chans = [x for x in chans if x['name'] == _chan.label][0]\n  elec_type = one_chans['type']\n  size = one_chans['size']\n  material = one_chans['material']\n  f.write(f'{_chan.label}\\t{xyz}\\t{elec_type}\\t{size}\\t{material}\\n')\n   old_json = replace_underscore(Path(electrodes_file.filename), 'coordframe.json')\n  new_json = replace_underscore(tsv_electrodes, 'coordframe.json')\n  copyfile(old_json, new_json)", "target": 0, "info": "Null", "idx": 0}
{"func": "def project_electrodes(electrodes_file, freesurfer_path):\n  chans = read_tsv(electrodes_file.filename)\n  chan = Channels(\n  [x['name'] for x in chans],\n  array([(float(x['x']), float(x['y']), float(x['z'])) for x in chans]))\n  fs = Freesurfer(freesurfer_path / ('sub-' + electrodes_file.subject))\n  if median(chan.return_xyz()[:, 0]) > 0:\n  surf = fs.read_brain().rh\n  else:\n  surf = fs.read_brain().lh\n  chan = snap_to_surface(surf, chan)\n  tsv_electrodes = Path(electrodes_file.filename).parent / f'sub-{electrodes_file.subject}_ses-{electrodes_file.session}_acq-{electrodes_file.acquisition}projected_electrodes.tsv'\n  with tsv_electrodes.open('w') as f:\n  f.write('name\\tx\\ty\\tz\\ttype\\tsize\\tmaterial\\n')\n  for _chan in chan.chan:\n  xyz = \"\\t\".join(f'{x:f}' for x in _chan.xyz)\n  one_chans = [x for x in chans if x['name'] == _chan.label][0]\n  elec_type = one_chans['type']\n  size = one_chans['size']\n  material = one_chans['material']\n  f.write(f'{_chan.label}\\t{xyz}\\t{elec_type}\\t{size}\\t{material}\\n')\n   old_json = replace_underscore(Path(f.filename), 'coordframe.json')\n  new_json = replace_underscore(tsv_electrodes, 'coordframe.json')\n  copyfile(old_json, new_json)", "target": 1, "info": "Null", "idx": 0}
{"func": "def make_segments(dat):\n  trials = []\n  for d in dat.data:\n  v = _create_subepochs(d, 1023, 1023)\n  for i in range(v.shape[1]):\n  trials.append(v[:, i, :])\n  out = dat._copy(axis=False)\n  out.data = empty(len(trials), dtype='O')\n  out.axis['chan'] = empty(len(trials), dtype='O')\n  out.axis['time'] = empty(len(trials), dtype='O')\n  for i, trial in enumerate(trials):\n  out.data[i] = trial\n  out.axis['time'][i] = dat.axis['time'][0]\n  out.axis['chan'][i] = dat.axis['chan'][0]\n   return out", "target": 0, "info": "Null", "idx": 0}
{"func": "def make_segments(dat):\n  trials = []\n  for d in dat.data:\n  v = _create_subepochs(d, 1023, 1023)\n  for i in range(v.shape[1]):\n  trials.append(v[:, i, :])\n  out = dat._copy(axis=False)\n  out.data = empty(len(trials), dtype='O')\n  out.axis['chan'] = empty(len(trials), dtype='O')\n  out.axis['time'] = empty(len(trials), dtype='O')\n  for i, trial in enumerate(trials):\n  out.data[i] = trial\n  out.axis['time'][i] = dat.axis['time'][0]\n  out.axis['chan'][i] = dat.axis['chan'][0]\n   return dat", "target": 1, "info": "Null", "idx": 0}
{"func": "def main(output_dir):\n  args = []\n  for cond in ('move', 'rest'):\n  for ieeg_file in find_in_bids(output_dir, modality=cond, extension='.pkl', generator=True):\n  args.append((ieeg_file, cond))\n  if PARAMETERS['parallel']:\n  with Pool() as p:\n  p.starmap(save_frequency, args)\n  else:\n  for arg in args:\n  save_frequency(*arg)", "target": 0, "info": "Null", "idx": 0}
{"func": "def main(output_dir):\n  args = []\n  for cond in ('move', 'rest'):\n  for ieeg_file in find_in_bids(output_dir, modality=cond, extension='.pkl', generator=True):\n  args.append((ieeg_file, cond))\n  if PARAMETERS['parallel']:\n  with Pool() as p:\n  p.starmap(save_frequency, args)\n  else:\n  for arg in args:\n  save_frequency(*args)", "target": 1, "info": "Null", "idx": 0}
{"func": " def from_string(cls, s):\n  val = base32hex.b32decode(s.upper())\n  value_check = [0 <= x < 255 for x in val]\n   if not all(value_check):\n  raise InvalidXid(s)\n   return cls(val)", "target": 0, "info": "Null", "idx": 0}
{"func": "def from_string(cls, s):\n  val = base32hex.b32decode(s.upper())\n  value_check = [0 < x < 255 for x in val]\n   if not all(value_check):\n  raise InvalidXid(s)\n   return cls(val)", "target": 1, "info": "Null", "idx": 0}
{"func": "  def __init__(self, partial=False, **kwargs):\n  self._incomplete = set()\n  self.partial = partial\n  simple_fields = sorted(self.SIMPLE_FIELDS, reverse=True)\n  simple_fields = sorted(simple_fields, key=lambda x: x in CivicRecord.SIMPLE_FIELDS, reverse=True)\n  for field in simple_fields:\n  try:\n  self.__setattr__(field, kwargs[field])\n  except KeyError:\n  try:\n  object.__getattribute__(self, field)\n  except AttributeError:\n  if partial and field not in CivicRecord.SIMPLE_FIELDS:\n  self._incomplete.add(field)\n  else:\n  raise AttributeError(f'Expected {field} attribute for {self.type}, none found.')\n  for field in self.COMPLEX_FIELDS:\n  try:\n  v = kwargs[field]\n  except KeyError:\n  if partial:\n  self._incomplete.add(field)\n  continue\n  else:\n  raise AttributeError(f'Expected {field} attribute for {self.type}, none found.')\n  is_compound = isinstance(v, list)\n  cls = get_class(field)\n  if is_compound:\n  result = list()\n  for data in v:\n  try:\n  data['type'] = data.get('type', singularize(field))\n  except AttributeError:\n  result.append(data)\n  else:\n  result.append(cls(partial=True, **data))\n  self.__setattr__(field, result)\n  else:\n  t = v.get('type', field)\n  v['type'] = CIVIC_TO_PYCLASS.get(t, t)\n  self.__setattr__(field, cls(partial=True, **v))\n  self.partial = bool(self._incomplete)\n  if not isinstance(self, Attribute) and not self.partial and self.__class__.__name__ != 'CivicRecord':\n  CACHE[hash(self)] = self", "target": 0, "info": "Null", "idx": 0}
{"func": "def __init__(self, partial=False, **kwargs):\n  self._incomplete = set()\n  self.partial = partial\n  simple_fields = sorted(self.SIMPLE_FIELDS, reverse=True)\n  simple_fields = sorted(simple_fields, key=lambda x: x in CivicRecord.SIMPLE_FIELDS, reverse=True)\n  for field in simple_fields:\n  try:\n  self.__setattr__(field, kwargs[field])\n  except KeyError:\n  try:\n  object.__getattribute__(self, field)\n  except AttributeError:\n  if partial and field not in CivicRecord.SIMPLE_FIELDS:\n  self._incomplete.add(field)\n  else:\n  raise AttributeError(f'Expected {field} attribute for {self.type}, none found.')\n  for field in self.COMPLEX_FIELDS:\n  try:\n  v = kwargs[field]\n  except KeyError:\n  if partial:\n  self._incomplete.add(field)\n  continue\n  else:\n  raise AttributeError(f'Expected {field} attribute for {self.type}, none found.')\n  is_compound = isinstance(v, list)\n  cls = get_class(field)\n  if is_compound:\n  result = list()\n  for data in v:\n  try:\n  data['type'] = data.get('type', singularize(field))\n  except AttributeError:\n  result.append(v)\n  else:\n  result.append(cls(partial=True, **data))\n  self.__setattr__(field, result)\n  else:\n  t = v.get('type', field)\n  v['type'] = CIVIC_TO_PYCLASS.get(t, t)\n  self.__setattr__(field, cls(partial=True, **v))\n  self.partial = bool(self._incomplete)\n  if not isinstance(self, Attribute) and not self.partial and self.__class__.__name__ != 'CivicRecord':\n  CACHE[hash(self)] = self", "target": 1, "info": "Null", "idx": 0}
{"func": "def get_devices(self, location):\n  response_data = self.__call_smart_system_get(\n  f\"{self.SMART_HOST}/v1/locations/{location.data['id']}\"\n  )\n  if len(response_data[\"data\"][\"relationships\"][\"devices\"][\"data\"]) < 1:\n  self.logger.error(\"No device found....\")\n  else:\n  devices_smart_system = {}\n  for device in response_data[\"included\"]:\n  real_id = device[\"id\"].split(\":\")[0]\n  if real_id not in devices_smart_system:\n  devices_smart_system[real_id] = {}\n  if (\n  device[\"type\"] in self.supported_services\n  and device[\"type\"] not in devices_smart_system[real_id]\n  ):\n  devices_smart_system[real_id][device[\"type\"]] = []\n  devices_smart_system[real_id][device[\"type\"]].append(device)\n  for parsed_device in devices_smart_system.values():\n  location.add_device(DeviceFactory.build(self, parsed_device))", "target": 0, "info": "Null", "idx": 0}
{"func": " def get_devices(self, location):\n  response_data = self.__call_smart_system_get(\n  f\"{self.SMART_HOST}/v1/locations/{location.data['id']}\"\n  )\n  if len(response_data[\"data\"][\"relationships\"][\"devices\"][\"data\"]) < 1:\n  self.logger.error(\"No device found....\")\n  else:\n  devices_smart_system = {}\n  for device in response_data[\"included\"]:\n  real_id = device[\"id\"].split(\":\")[0]\n  if real_id not in devices_smart_system:\n  devices_smart_system[real_id] = {}\n  if (\n  device[\"type\"] in self.supported_services\n  and device[\"type\"] not in devices_smart_system[real_id]\n  ):\n  devices_smart_system[real_id][device[\"type\"]] = []\n  devices_smart_system[real_id][device[\"type\"]].append(device)\n  for parsed_device in devices_smart_system.values():\n  location.add_device(DeviceFactory.build(self, device))", "target": 1, "info": "Null", "idx": 0}
{"func": "def isCreate(self):\n  data_id = self.featureServerProxy.getID()\n  return data_id is None and self.request.body != \"\" and self.request.method == \"POST\"", "target": 0, "info": "Null", "idx": 0}
{"func": "  def isCreate(self):\n  data_id = self.featureServerProxy.getID()\n  return data_id is not None and self.request.body != \"\" and self.request.method == \"POST\"", "target": 1, "info": "Null", "idx": 0}
{"func": "def setup_attributes(self) -> None:\n  _POSITION = 0\n  _COLOR = 1\n  self.create_vao_vbos(2)\n  vertices = self.element.vertices.astype(np.float32)\n  colors = self.element.rgba.astype(np.float32)\n  self.num_vertices = len(vertices)\n  glBindVertexArray(self.vao)\n  self.fill_buffer(_POSITION, 3, vertices, GLfloat, GL_FLOAT, self.vbos[_POSITION])\n  self.fill_buffer(_COLOR, 4, colors, GLfloat, GL_FLOAT, self.vbos[_COLOR])\n   glVertexAttribDivisor(_COLOR, 1)\n   glBindVertexArray(0)", "target": 0, "info": "Null", "idx": 0}
{"func": "def setup_attributes(self) -> None:\n  _POSITION = 0\n  _COLOR = 1\n  self.create_vao_vbos(2)\n  vertices = self.element.vertices.astype(np.float32)\n  colors = self.element.rgba.astype(np.float32)\n  self.num_vertices = len(vertices)\n  glBindVertexArray(self.vao)\n  self.fill_buffer(_POSITION, 3, vertices, GLfloat, GL_FLOAT, self.vbos[_POSITION])\n  self.fill_buffer(_COLOR, 4, colors, GLfloat, GL_FLOAT, self.vbos[_POSITION])\n   glVertexAttribDivisor(_COLOR, 1)\n   glBindVertexArray(0)", "target": 1, "info": "Null", "idx": 0}
{"func": " def mouseMoveEvent(self, event, widget):\n  dx = event.x() - self.lastPos.x()\n  dy = event.y() - self.lastPos.y()\n  widget.world.setToIdentity()\n  if event.buttons() == Qt.LeftButton:\n  self.set_x_rotation(widget, widget.xWorldRot + dy)\n  self.set_z_rotation(widget, widget.zWorldRot + dx)\n  elif event.buttons() == Qt.RightButton:\n  self.set_x_rotation(widget, widget.xWorldRot + dy)\n  self.set_y_rotation(widget, widget.yWorldRot + dx)\n  elif event.buttons() == Qt.MiddleButton:\n  distance_x = 200 * abs(widget.zCameraPos + widget.centroid[2]) / widget.width()\n  distance_y = 200 * abs(widget.zCameraPos + widget.centroid[2]) / widget.height()\n  self.set_x_movement(widget, widget.xCameraPos + (distance_x * dx / 200.0))\n  self.set_y_movement(widget, widget.yCameraPos - (distance_y * dy / 200.0))\n  self.lastPos = QPoint(event.pos())", "target": 0, "info": "Null", "idx": 0}
{"func": "def mouseMoveEvent(self, event, widget):\n  dx = event.x() - self.lastPos.x()\n  dy = event.y() - self.lastPos.y()\n  widget.world.setToIdentity()\n  if event.buttons() == Qt.LeftButton:\n  self.set_x_rotation(widget, widget.xWorldRot + dy)\n  self.set_z_rotation(widget, widget.zWorldRot + dx)\n  elif event.buttons() == Qt.RightButton:\n  self.set_x_rotation(widget, widget.xWorldRot + dy)\n  self.set_y_rotation(widget, widget.yWorldRot - dx)\n  elif event.buttons() == Qt.MiddleButton:\n  distance_x = 200 * abs(widget.zCameraPos + widget.centroid[2]) / widget.width()\n  distance_y = 200 * abs(widget.zCameraPos + widget.centroid[2]) / widget.height()\n  self.set_x_movement(widget, widget.xCameraPos + (distance_x * dx / 200.0))\n  self.set_y_movement(widget, widget.yCameraPos - (distance_y * dy / 200.0))\n  self.lastPos = QPoint(event.pos())", "target": 1, "info": "Null", "idx": 0}
{"func": " def verify_result(self, expected):\n  self.assertEqual(expected, self.result)", "target": 0, "info": "Null", "idx": 0}
{"func": " def verify_result(self, expected):\n  self.assertEqual(self.result, expected)", "target": 1, "info": "Null", "idx": 0}
{"func": "def make_gis(parsed_users, output_file_geojson, output_file_kml, send_to_geojsonio):\n  geojson = []\n  kml = Kml()\n  message(\"Making geosjon and kml\")\n  for user in parsed_users:\n  latitude = user[0]\n  longitude = user[1]\n  name = user[2]\n  comment = user[3]\n  point = Point((longitude, latitude))\n  feature = Feature(geometry=point, properties={\"Comment\": comment, \"Name\": name})\n  geojson.append(feature)\n  kml.newpoint(name=name, coords=[(longitude, latitude)], description=comment)\n  geojson_str = (dumps(FeatureCollection(geojson)))\n  if output_file_geojson != \"no\":\n  message(\"Tidying up geojson\")\n  geojson_str_pretty = geojson_str\n  geojson_str_pretty = geojson_str_pretty.replace('\"features\": [', '\"features\": [\\n')\n  geojson_str_pretty = geojson_str_pretty.replace('}}, ', '}},\\n')\n  geojson_str_pretty = geojson_str_pretty.replace('}}]', '}}\\n]')\n   message(\"Writing geojson to \" + output_file_geojson)\n  output = open(output_file_geojson, 'w')\n  output.write(geojson_str_pretty)\n  output.close()\n  if output_file_kml != \"no\":\n  message(\"Writing kml to \" + output_file_kml)\n  kml.save(output_file_kml)\n  if send_to_geojsonio is True:\n  message(\"Sending geojson to geojson.io\")\n  to_geojsonio(geojson_str)", "target": 0, "info": "Null", "idx": 0}
{"func": "def make_gis(parsed_users, output_file_geojson, output_file_kml, send_to_geojsonio):\n  geojson = []\n  kml = Kml()\n  message(\"Making geosjon and kml\")\n  for user in parsed_users:\n  latitude = user[0]\n  longitude = user[1]\n  name = user[2]\n  comment = user[3]\n  point = Point((longitude, latitude))\n  feature = Feature(geometry=point, properties={\"Comment\": comment, \"Name\": name})\n  geojson.append(feature)\n  kml.newpoint(name=name, coords=[(longitude, latitude)], description=comment)\n  geojson_str = (dumps(FeatureCollection(geojson)))\n  if output_file_geojson != \"no\":\n  message(\"Tidying up geojson\")\n  geojson_str_pretty = geojson_str\n  geojson_str_pretty = geojson_str_pretty.replace('\"features\": [', '\"features\": [\\n')\n  geojson_str_pretty = geojson_str_pretty.replace('}}, ', '}},\\n')\n  geojson_str_pretty = geojson_str_pretty.replace('}}]', '}}\\n]')\n   message(\"Writing geojson to \" + output_file_geojson)\n  output = open(output_file_geojson, 'w')\n  output.write(geojson_str)\n  output.close()\n  if output_file_kml != \"no\":\n  message(\"Writing kml to \" + output_file_kml)\n  kml.save(output_file_kml)\n  if send_to_geojsonio is True:\n  message(\"Sending geojson to geojson.io\")\n  to_geojsonio(geojson_str)", "target": 1, "info": "Null", "idx": 0}
{"func": "  def writeSequence(self, data):\n  for chunk in data:\n  self.write(chunk)", "target": 0, "info": "Null", "idx": 0}
{"func": " def writeSequence(self, data):\n  for chunk in data:\n  self.write(data)", "target": 1, "info": "Null", "idx": 0}
{"func": "def do_execute(self, code, silent, store_history=True, user_expressions=None,\n allow_stdin=False):\n  self.execution_count += 1\n  result = yield threads.deferToThread(self.send_message,\n   {\"type\": \"execute\",\n   \"payload\": code})\n   if result[\"payload\"][\"success\"]:\n  if result['payload']['returned'] != \"\" and not silent:\n  self.send_update(\"execute_result\", {\n  'execution_count': self.execution_count,\n  'data': {\n  'text/plain': result['payload']['returned']\n  },\n  'metadata': {}\n  })\n   return {\n  'status': 'ok',\n  'execution_count': self.execution_count,\n  'payload': [],\n  'user_expressions': {},\n  }\n  else:\n  full_traceback = result['payload']['returned'].split(\"\\n\")\n  evalue = full_traceback[0]\n  traceback = full_traceback[2:]\n  if not silent:\n  self.send_update(\"error\", {\n  'execution_count': self.execution_count,\n  'traceback': traceback,\n  'ename': 'n/a',\n  'evalue': evalue\n  })\n  return {\n  'status': 'error',\n  'execution_count': self.execution_count,\n  'traceback': traceback,\n  'ename': 'n/a',\n  'evalue': evalue\n  }", "target": 0, "info": "Null", "idx": 0}
{"func": "def do_execute(self, code, silent, store_history=True, user_expressions=None,\n allow_stdin=False):\n  self.execution_count += 1\n  result = yield threads.deferToThread(self.send_message,\n   {\"type\": \"execute\",\n   \"payload\": code})\n   if result[\"payload\"][\"success\"]:\n  if result['payload']['returned'] == \"\" and not silent:\n  self.send_update(\"execute_result\", {\n  'execution_count': self.execution_count,\n  'data': {\n  'text/plain': result['payload']['returned']\n  },\n  'metadata': {}\n  })\n   return {\n  'status': 'ok',\n  'execution_count': self.execution_count,\n  'payload': [],\n  'user_expressions': {},\n  }\n  else:\n  full_traceback = result['payload']['returned'].split(\"\\n\")\n  evalue = full_traceback[0]\n  traceback = full_traceback[2:]\n  if not silent:\n  self.send_update(\"error\", {\n  'execution_count': self.execution_count,\n  'traceback': traceback,\n  'ename': 'n/a',\n  'evalue': evalue\n  })\n  return {\n  'status': 'error',\n  'execution_count': self.execution_count,\n  'traceback': traceback,\n  'ename': 'n/a',\n  'evalue': evalue\n  }", "target": 1, "info": "Null", "idx": 0}
{"func": "MATCH_ALL = '*'\n  lambda _, dct: dct.values(),\n  AttributeError,\n  dict, )\n  lambda _, iterable: [i for i in iterable],\n  (AttributeError, TypeError),\n  lambda _, value: hasattr(value, '__iter__') or hasattr(value, '__getitem__'), )", "target": 0, "info": "Null", "idx": 0}
{"func": "MATCH_ALL = '*'\n  lambda _, dct: dct.values(),\n  AttributeError,\n  dict, )\n  lambda _, iterable: [i for i in iterable],\n  (AttributeError, TypeError),\n  lambda _, value: hasattr(value, '__iter__') or hasattr(value, '__getitem__'), )", "target": 1, "info": "Null", "idx": 0}
{"func": "def markup(string):\n  colorStack = [9]\n  bgcolorStack = [9]\n  formatStack = [0]\n  while '<' in string and '>' in string:\n  tags = string.split('<', 1)[1].split('>', 1)[0].split()\n  if tags[0][0] == '/':\n  tags[0] = tags[0][1:]\n  if tags[0] == 'color':\n  colorStack.pop()\n  elif tags[0] == 'bgcolor':\n  bgcolorStack.pop()\n  else:\n  formatStack.reverse()\n  formatStack.remove(formatTable[tags[0]])\n  formatStack.reverse()\n  if len(colorStack) is 0 or len(bgcolorStack) is 0 or len(formatStack) is 0:\n  raise SyntaxError(\"Unable to close tag %s\" % tags[0])\n  else:\n  try:\n  if tags[0] == 'color':\n  if len(tags) is 1:\n  tags[1] = \"default\"\n  colorStack.append(colorTable[tags[1]])\n  elif tags[0] == 'bgcolor':\n  if len(tags) is 1:\n  tags[1] = \"default\"\n  bgcolorStack.append(colorTable[tags[1]])\n  else:\n  formatStack.append(formatTable[tags[0]])\n  except:\n  raise SyntaxError(\"Invalid tag or argument: %s %s\" % (tags[0], tags[1]))\n  newString = string.split(\"<\", 1)[0]\n  newString += \"\\033[%d;%d;%dm\" % (\n  formatStack[-1],\n  colorStack[-1]+30,\n  bgcolorStack[-1]+40\n  )\n  newString += string.split(\">\", 1)[1]\n  string = newString\n  return string", "target": 0, "info": "Null", "idx": 0}
{"func": "def markup(string):\n  colorStack = [9]\n  bgcolorStack = [9]\n  formatStack = [0]\n  while '<' in string and '>' in string:\n  tags = string.split('<', 1)[1].split('>', 1)[0].split()\n  if tags[0][0] == '/':\n  tags[0] = tags[0][1:]\n  if tags[0] == 'color':\n  colorStack.pop()\n  elif tags[0] == 'bgcolor':\n  bgcolorStack.pop()\n  else:\n  formatStack.reverse()\n  formatStack.remove(formatTable[tags[0]])\n  formatStack.reverse()\n  if len(colorStack) is 0 or len(bgcolorStack) is 0 or len(formatStack) is 0:\n  raise SyntaxError(\"Unable to close tag %s\" % tags[0])\n  else:\n  try:\n  if tags[0] == 'color':\n  if len(tags) is 1:\n  tags[1] = \"default\"\n  colorStack.append(colorTable[tags[1]])\n  elif tags[0] == 'bgcolor':\n  if len(tags) is 1:\n  tags[1] = \"default\"\n  bgcolorStack.append(colorTable[tags[1]])\n  else:\n  formatStack.append(formatTable[tags[0]])\n  except:\n  raise SyntaxError(\"Invalid tag or argument: %s %s\" % (tags[0], tags[1]))\n  newString = string.split(\"<\", 1)[0]\n  newString += \"\\033[%d;%d;%dm\" % (\n  formatStack[-1],\n  colorStack[-1]+30,\n  bgcolorStack[-1]+40\n  )\n  newString += string.split(\">\", 1)[1]\n  string = newString\n  return newString", "target": 1, "info": "Null", "idx": 0}
{"func": "def example_simple_rpc():\n  def _restore(ref_id):\n  return capability.TestInterface.new_server(Server(100))\n  loop = capnp.EventLoop()\n   read, write = socket.socketpair(socket.AF_UNIX)\n  read_stream = capnp.FdAsyncIoStream(read.fileno())\n  write_stream = capnp.FdAsyncIoStream(write.fileno())\n   restorer = capnp.Restorer(capability.TestSturdyRefObjectId, _restore)\n  server = capnp.RpcServer(loop, write_stream, restorer)\n  client = capnp.RpcClient(loop, read_stream)\n   ref = capability.TestSturdyRefObjectId.new_message()\n  cap = client.restore(ref.as_reader())\n  cap = cap.cast_as(capability.TestInterface)\n  remote = cap.foo(i=5)\n  response = loop.wait(remote)\n  assert response.x == '125'", "target": 0, "info": "Null", "idx": 0}
{"func": "def example_simple_rpc():\n  def _restore(ref_id):\n  return capability.TestInterface.new_server(Server(100))\n  loop = capnp.EventLoop()\n   read, write = socket.socketpair(socket.AF_UNIX)\n  read_stream = capnp.FdAsyncIoStream(read.fileno())\n  write_stream = capnp.FdAsyncIoStream(write.fileno())\n   restorer = capnp.Restorer(capability.TestSturdyRefObjectId, _restore)\n  server = capnp.RpcServer(loop, restorer, write_stream)\n  client = capnp.RpcClient(loop, read_stream)\n   ref = capability.TestSturdyRefObjectId.new_message()\n  cap = client.restore(ref.as_reader())\n  cap = cap.cast_as(capability.TestInterface)\n  remote = cap.foo(i=5)\n  response = loop.wait(remote)\n  assert response.x == '125'", "target": 1, "info": "Null", "idx": 0}
{"func": "def reindex(client, source_index, target_index, target_client=None, chunk_size=500, scroll='5m'):\n  target_client = client if target_client is None else target_client\n   docs = scan(client, index=source_index, scroll=scroll)\n  def _change_doc_index(hits, index):\n  for h in hits:\n  h['_index'] = index\n  yield h\n  return bulk(target_client, _change_doc_index(docs, target_index),\n  chunk_size=chunk_size, stats_only=True)", "target": 0, "info": "Null", "idx": 0}
{"func": "def reindex(client, source_index, target_index, target_client=None, chunk_size=500, scroll='5m'):\n  target_client = client if target_client is None else target_index\n   docs = scan(client, index=source_index, scroll=scroll)\n  def _change_doc_index(hits, index):\n  for h in hits:\n  h['_index'] = index\n  yield h\n  return bulk(target_client, _change_doc_index(docs, target_index),\n  chunk_size=chunk_size, stats_only=True)", "target": 1, "info": "Null", "idx": 0}
{"func": "def get(*paths, **options):  fullpath = []\n  cfg = _config\n  for path in paths:\n  fullpath.append(path)\n  if path in cfg:\n  cfg = cfg[path]\n  else:\n  fullpath = '.'.join(fullpath)\n  raise HaipConfigException(f'path \"{fullpath}\" not found in config')\n  if not options:\n  return cfg\n  result = ConfigContainer()\n  for key, value in options.items():\n  if key in cfg:\n  result[key] = cfg[key]\n  elif value is not MANDATORY:\n  result[key] = value\n  else:\n  path = '/'.join(paths)\n  raise HaipConfigException(f'option \"{key}\" not found in section \"{path}\"')\n  return result", "target": 0, "info": "Null", "idx": 0}
{"func": "def get(*paths, **options): for path in paths:\n  fullpath.append(path)\n  if path in cfg:\n  cfg = cfg[path]\n  else:\n  fullpath = '.'.join(fullpath)\n  raise HaipConfigException(f'path \"{fullpath}\" not found in config')\n  if not options:\n  return cfg\n  result = ConfigContainer()\n  for key, value in options.items():\n  if key in cfg:\n  result[key] = cfg[key]\n  elif value is not MANDATORY:\n  result[key] = value\n  else:\n  path = '/'.join(path)\n  raise HaipConfigException(f'option \"{key}\" not found in section \"{path}\"')\n  return result", "target": 1, "info": "Null", "idx": 0}
{"func": "def mutate_protein_from_transcript(\n  transcript_seq,\n  position,\n  dna_ref,\n  dna_alt,\n  padding = None):\n  transcript_seq = Seq(str(transcript_seq))\n  transcript_ref_base = transcript_seq[position:position+len(dna_ref)]\n  if dna_ref != '.':\n  assert str(transcript_ref_base) == dna_ref, \\\n  \"Transcript reference base %s at position %d != reference %s\" % \\\n  (transcript_ref_base, position, dna_ref)\n  original_protein = transcript_seq.translate()\n  n_original_protein = len(original_protein)\n  mutated_dna = mutate(transcript_seq, position, dna_ref, dna_alt)\n  mutated_protein = mutated_dna.translate()\n  n_mutated_protein = len(mutated_protein)\n  aa_position = int(position / 3)\n  n_dna_ref = len(dna_ref)\n  n_dna_alt = len(dna_alt)\n  if abs(n_dna_ref - n_dna_alt) % 3 != 0:\n  n_aa_deleted = n_original_protein - aa_position\n  n_aa_inserted = n_mutated_protein - aa_position\n  frameshift = True\n  else:\n  n_aa_deleted = int(math.ceil(n_dna_ref / 3.0))\n  n_aa_inserted = int(math.ceil(n_dna_alt / 3.0))\n  frameshift = False\n  if padding is None:\n  start_pos = 0\n  end_pos = n_mutated_protein\n  else:\n  start_pos = max(0, aa_position - padding)\n  end_pos = min(n_mutated_protein, aa_position + padding + 1)\n  prefix_stop_codon = str(mutated_protein[:aa_position]).rfind(\"*\")\n  if prefix_stop_codon != -1:\n  start_pos = max(start_pos, prefix_stop_codon + 1)\n  suffix_stop_codon = str(mutated_protein[aa_position:]).find(\"*\")\n  early_stop = False\n  if suffix_stop_codon != -1:\n  if suffix_stop_codon < n_aa_inserted:\n  n_aa_inserted = suffix_stop_codon\n  early_stop = True\n  end_pos = min(end_pos, suffix_stop_codon + aa_position)\n  seq_region = mutated_protein[start_pos : end_pos]\n  mutation_start_pos_in_region = aa_position - start_pos\n  annot = \\\n  annotate(\n  original_protein,\n  mutated_protein,\n  aa_position,\n  n_aa_deleted,\n  n_aa_inserted,\n  frameshift,\n  early_stop)\n  return Mutation(\n  seq = str(seq_region),\n  start = start_pos,\n  stop = end_pos,\n  mutation_start = mutation_start_pos_in_region,\n  n_removed = n_aa_deleted,\n  n_inserted = n_aa_inserted,\n  annot = annot)", "target": 0, "info": "Null", "idx": 0}
{"func": "def mutate_protein_from_transcript(\n  transcript_seq,\n  position,\n  dna_ref,\n  dna_alt,\n  padding = None):\n  transcript_seq = Seq(str(transcript_seq))\n  transcript_ref_base = transcript_seq[position:position+len(dna_ref)]\n  if dna_ref != '.':\n  assert str(transcript_ref_base) == dna_ref, \\\n  \"Transcript reference base %s at position %d != reference %s\" % \\\n  (transcript_ref_base, position, dna_ref)\n  original_protein = transcript_seq.translate()\n  n_original_protein = len(original_protein)\n  mutated_dna = mutate(transcript_seq, position, dna_ref, dna_alt)\n  mutated_protein = mutated_dna.translate()\n  n_mutated_protein = len(mutated_protein)\n  aa_position = int(position / 3)\n  n_dna_ref = len(dna_ref)\n  n_dna_alt = len(dna_alt)\n  if abs(n_dna_ref - n_dna_alt) % 3 != 0:\n  n_aa_deleted = n_original_protein - aa_position\n  n_aa_inserted = n_mutated_protein - aa_position\n  frameshift = True\n  else:\n  n_aa_deleted = int(math.ceil(n_dna_ref / 3.0))\n  n_aa_inserted = int(math.ceil(n_dna_alt / 3.0))\n  frameshift = False\n  if padding is None:\n  start_pos = 0\n  end_pos = n_mutated_protein\n  else:\n  start_pos = max(0, aa_position - padding)\n  end_pos = min(n_mutated_protein, aa_position + padding + 1)\n  prefix_stop_codon = str(mutated_protein[:aa_position]).rfind(\"*\")\n  if prefix_stop_codon != -1:\n  start_pos = max(start_pos, prefix_stop_codon + 1)\n  suffix_stop_codon = str(mutated_protein[aa_position:]).find(\"*\")\n  early_stop = False\n  if suffix_stop_codon != -1:\n  if suffix_stop_codon < n_aa_inserted:\n  n_aa_inserted = suffix_stop_codon\n  early_stop = True\n  end_pos = min(end_pos, suffix_stop_codon + aa_position)\n  seq_region = mutated_protein[start_pos : end_pos]\n  mutation_start_pos_in_region = aa_position - start_pos\n  annot = \\\n  annotate(\n  original_protein,\n  mutated_protein,\n  aa_position,\n  n_aa_deleted,\n  n_aa_inserted,\n  frameshift,\n  early_stop)\n  return Mutation(\n  seq = str(seq_region),\n  start = start_pos,\n  stop = end_pos,\n  mutation_start = aa_position,\n  n_removed = n_aa_deleted,\n  n_inserted = n_aa_inserted,\n  annot = annot)", "target": 1, "info": "Null", "idx": 0}
{"func": "def positive_only_mse(y_true, y_pred):\n   diff = y_pred - y_true\n   mask = y_pred >= 0\n   diff *= mask\n   return K.mean(K.square(diff), axis=-1)", "target": 0, "info": "Null", "idx": 0}
{"func": "def positive_only_mse(y_true, y_pred):\n  diff = y_pred - y_true\n  mask = y_pred < 0\n  diff *= mask\n  return K.mean(K.square(diff), axis=-1)", "target": 1, "info": "Null", "idx": 0}
{"func": "def op_divl(self):\n  D = self.core.DEBUG\n  D('divl:')\n  lr = self.registers.pop()\n  old_tos = self.registers.tos().value\n  tos = self.registers.tos()\n  i = i32(tos.value).value\n  j = i32(lr.value).value\n  D('  i=%i, j=%i (%s, %s)', i, j, type(i), type(j))\n  i //= j\n  D('  i=%i (%s)', i, type(i))\n  tos.value = i\n   D('divl: %s %s', UINT32_FMT(old_tos), UINT32_FMT(lr.value))\n  self.dump_stack()", "target": 0, "info": "Null", "idx": 0}
{"func": "def op_divl(self):\n  D = self.core.DEBUG\n  D('divl:')\n  lr = self.registers.pop()\n  old_tos = self.registers.tos().value\n  tos = self.registers.tos()\n  i = i32(tos.value).value\n  j = i32(lr.value).value\n  D('  i=%i, j=%i (%s, %s)', i, j, type(i), type(j))\n  i /= j\n  D('  i=%i (%s)', i, type(i))\n  tos.value = i\n   D('divl: %s %s', UINT32_FMT(old_tos), UINT32_FMT(lr.value))\n  self.dump_stack()", "target": 1, "info": "Null", "idx": 0}
{"func": "def show_pages(logger, state):   logger.info('=== Memory pages ===')   for pg in sorted(state.get_child('machine').get_child('memory').get_page_states(), key = lambda x: x.index):\n  pg_addr = pg.index * PAGE_SIZE\n  pg_segment = addr_to_segment(pg_addr)\n  for bs in state.get_child('machine').get_binary_states():\n    if pg_segment == bs.cs:\n  break\n    if pg_segment == bs.ds:\n  break\n  logger.info('  Page\n  logger.info('\n Flags: %s%s%s%s%s', 'r' if pg.read == 1 else '-', 'w' if pg.write == 1 else '-', 'x' if pg.execute == 1 else '-', 'd' if pg.dirty == 1 else '-', 'c' if pg.cache == 1 else '-')\n   CPR = 32\n   for i in range(0, 256 // CPR):\n    s = []\n    t = []\n for b in pg.content[CPR * i:CPR * (i + 1)]:\n  c = '%02X' % b\n  s.append(GREEN(c) if b == 0 else WHITE(c))\n  c = chr(b)\n  if c in string.printable[0:-5]:\n    c = '%%' if c == '%' else (' ' + c)\n    t.append(c)\n  elif b == 10:\n    t.append('\\\\n')\n  elif b == 13:\n    t.append('\\\\r')\n  else:\n    t.append('  ')\n    logger.info('\n ' + ADDR_FMT(pg_addr + i * CPR) + ': ' + ' '.join(s))\n    logger.info('\n   ' + ' '.join(t))\n    row_symbols = []\n    for j in range(pg_addr + i * CPR, pg_addr + i * CPR + CPR):\n  if j not in bs.symbols:\n    continue\n  row_symbols.append((j - (pg_addr + i * CPR)) * 3 * ' ' + '^' + ', '.join(bs.symbols[j]))\n    for symbol in row_symbols:\n  logger.info('\n   ' + symbol)\n  logger.info('')", "target": 0, "info": "Null", "idx": 0}
{"func": "def show_pages(logger, state):   logger.info('=== Memory pages ===')   for pg in sorted(state.get_child('machine').get_child('memory').get_page_states(), key = lambda x: x.index):\n  pg_addr = pg.index * PAGE_SIZE\n  pg_segment = addr_to_segment(pg_addr)\n  for bs in state.get_child('machine').get_binary_states():\n    if pg_segment == bs.cs:\n  break\n    if pg_segment == bs.ds:\n  break\n  logger.info('  Page\n  logger.info('\n Flags: %s%s%s%s%s', 'r' if pg.read == 1 else '-', 'w' if pg.write == 1 else '-', 'x' if pg.execute == 1 else '-', 'd' if pg.dirty == 1 else '-', 'c' if pg.cache == 1 else '-')\n   CPR = 32\n   for i in range(0, 256 / CPR):\n    s = []\n    t = []\n for b in pg.content[CPR * i:CPR * (i + 1)]:\n  c = '%02X' % b\n  s.append(GREEN(c) if b == 0 else WHITE(c))\n  c = chr(b)\n  if c in string.printable[0:-5]:\n    c = '%%' if c == '%' else (' ' + c)\n    t.append(c)\n  elif b == 10:\n    t.append('\\\\n')\n  elif b == 13:\n    t.append('\\\\r')\n  else:\n    t.append('  ')\n    logger.info('\n ' + ADDR_FMT(pg_addr + i * CPR) + ': ' + ' '.join(s))\n    logger.info('\n   ' + ' '.join(t))\n    row_symbols = []\n    for j in range(pg_addr + i * CPR, pg_addr + i * CPR + CPR):\n  if j not in bs.symbols:\n    continue\n  row_symbols.append((j - (pg_addr + i * CPR)) * 3 * ' ' + '^' + ', '.join(bs.symbols[j]))\n    for symbol in row_symbols:\n  logger.info('\n   ' + symbol)\n  logger.info('')", "target": 1, "info": "Null", "idx": 0}
{"func": " def setup_hdt(self):\n  self.DEBUG('Machine.setup_hdt')\n   self.hdt.create()\n   pages = self.memory.alloc_pages(segment = 0x00, count = align(PAGE_SIZE, self.hdt.size()) // PAGE_SIZE)\n  self.memory.update_pages_flags(pages[0].index, len(pages), 'read', True)\n  self.hdt_address = pages[0].base_address\n   self.DEBUG('Machine.setup_hdt: address=%s, size=%s (%s pages)', ADDR_FMT(self.hdt_address), self.hdt.size(), len(pages))\n  self.hdt.write(self.hdt_address)", "target": 0, "info": "Null", "idx": 0}
{"func": " def setup_hdt(self):\n  self.DEBUG('Machine.setup_hdt')\n   self.hdt.create()\n   pages = self.memory.alloc_pages(segment = 0x00, count = align(PAGE_SIZE, self.hdt.size()) / PAGE_SIZE)\n  self.memory.update_pages_flags(pages[0].index, len(pages), 'read', True)\n  self.hdt_address = pages[0].base_address\n   self.DEBUG('Machine.setup_hdt: address=%s, size=%s (%s pages)', ADDR_FMT(self.hdt_address), self.hdt.size(), len(pages))\n  self.hdt.write(self.hdt_address)", "target": 1, "info": "Null", "idx": 0}
{"func": "def _is_wc_root(root, info):\n  root = os.path.normpath(os.path.abspath(root))\n  if os.path.normcase(root) == os.path.normcase(info.get('Working Copy Root Path', '')):\n  return True\n  elif info:\n  p = os.path.dirname(root)\n  return (p == root\n  or not os.path.isdir(os.path.join(p, '.svn'))\n  or _info(p).get('Repository UUID') != info['Repository UUID'])\n  return False", "target": 0, "info": "Null", "idx": 0}
{"func": "def _is_wc_root(root, info):\n  root = os.path.normpath(os.path.abspath(root))\n  if os.path.normcase(root) == os.path.normcase(info.get('Working Copy Root Path', '')):\n  return True\n  elif info:\n  p = os.path.dirname(root)\n  return (p == root\n  or not os.path.isdir(os.path.join(p, '.svn'))\n  or _info(p).get('Repository UUID') == info['Repository UUID'])\n  return False", "target": 1, "info": "Null", "idx": 0}
{"func": " def parse_escape_markers(self):\n  sequence_start = None\n  self.escape_markers = []\n  for index, char in enumerate(self.string):\n  if sequence_start is not None and char in string.letters:\n  self.escape_markers.append(EscapeMarker(sequence_start, index))\n  sequence_start = None\n  elif char == '\\033' and self.string[index+1] == '[':\n  sequence_start = index\n  elif char not in string.printable:\n  self.escape_markers.append(EscapeMarker(index, index))", "target": 0, "info": "Null", "idx": 0}
{"func": "def parse_escape_markers(self):\n  sequence_start = None\n  self.escape_markers = []\n  for index, char in enumerate(self.string):\n  if sequence_start is None and char in string.letters:\n  self.escape_markers.append(EscapeMarker(sequence_start, index))\n  sequence_start = None\n  elif char == '\\033' and self.string[index+1] == '[':\n  sequence_start = index\n  elif char not in string.printable:\n  self.escape_markers.append(EscapeMarker(index, index))", "target": 1, "info": "Null", "idx": 0}
{"func": "def CTSget(source, targets, identifiers, top_only=True, timeout=60, server=\"http://cts.fiehnlab.ucdavis.edu/service/convert\"):\n  result = {}\n  if type(targets) is str:\n  result[targets] = CTS_translate_multi(source, targets, identifiers, top_only, timeout, server)\n  elif type(targets) is list:\n  for i in range(len(targets)):\n  target = targets[i]\n  print ('translating from ' + source + ' to ' + target)\n  result[target] = CTS_translate_multi(source, target, identifiers, top_only, timeout, server)\n  else:\n  raise IOError('Input targets should be string or a list of strings')\n return result", "target": 0, "info": "Null", "idx": 0}
{"func": "def CTSget(source, targets, identifiers, top_only=True, timeout=60, server=\"http://cts.fiehnlab.ucdavis.edu/service/convert\"):\n  result = {}\n  if type(targets) is str:\n  result[targets] = CTS_translate_multi(source, targets, identifiers, top_only, timeout, server)\n  elif type(identifiers) is list:\n  for i in range(len(targets)):\n  target = targets[i]\n  print ('translating from ' + source + ' to ' + target)\n  result[target] = CTS_translate_multi(source, target, identifiers, top_only, timeout, server)\n  else:\n  raise IOError('Input targets should be string or a list of strings')\n return result", "target": 1, "info": "Null", "idx": 0}
{"func": "def keys_load(privkey_file, pubkey_file):\n  privkey_loaded = open(privkey_file, 'rb').read()\n  privkey = SigningKey.from_string(privkey_loaded, curve=SECP256k1)\n  pubkey_loaded = open(pubkey_file, 'rb').read()\n  pubkey = VerifyingKey.from_string(pubkey_loaded, curve=SECP256k1)\n   address = blake2b(pubkey.to_string(), digest_size=20).hexdigest()\n   return privkey, pubkey, address", "target": 0, "info": "Null", "idx": 0}
{"func": "def keys_load(privkey_file, pubkey_file):\n  privkey_loaded = open(privkey_file, 'rb').read()\n  privkey = SigningKey.from_string(privkey_loaded, curve=SECP256k1)\n  pubkey_loaded = open(pubkey_file, 'rb').read()\n  pubkey = VerifyingKey.from_string(pubkey_loaded, curve=SECP256k1)\n   address = blake2b(privkey.to_string(), digest_size=20).hexdigest()\n   return privkey, pubkey, address", "target": 1, "info": "Null", "idx": 0}
{"func": "def digest_block(data, sdef, peer_ip, conn, c, hdd, h, hdd2, h2, h3, index, index_cursor):\n  global hdd_block\n  global last_block\n  global peers\n  global plugin_manager\n  block_height_new = last_block + 1\n  block_hash = 'N/A'\n  failed_cause = ''\n  block_count = 0\n  tx_count = 0\n  if peers.is_banned(peer_ip):\n  raise ValueError(\"Cannot accept blocks from a banned peer\")\n  if not db_lock.locked():\n  db_lock.acquire()\n  while mp.MEMPOOL.lock.locked():\n  time.sleep(0.1)\n  app_log.info(\"Block: Waiting for mempool to unlock {}\".format(peer_ip))\n  app_log.warning(\"Block: Digesting started from {}\".format(peer_ip))\n  block_size = Decimal(sys.getsizeof(str(data))) / Decimal(1000000)\n  app_log.warning(\"Block: size: {} MB\".format(block_size))\n  try:\n  block_list = data\n  signature_list = []\n  block_transactions = []\n  for transaction_list in block_list:\n  block_count += 1\n  for entry in transaction_list:\n  tx_count += 1\n  entry_signature = entry[4]\n  if entry_signature:\n  signature_list.append(entry_signature)\n  execute_param(h3, \"SELECT block_height FROM transactions WHERE signature = ?;\",\n    (entry_signature,))\n  test = h3.fetchone()\n  if test:\n  raise ValueError(\"That transaction {} is already in our ram ledger, block_height {}\".format(\n  entry_signature[:10], test[0]))\n  execute_param(c, \"SELECT block_height FROM transactions WHERE signature = ?;\",\n    (entry_signature,))\n  test = c.fetchone()\n  if test:\n  raise ValueError(\"That transaction {} is already in our ledger, block_height {}\".format(\n  entry_signature[:10], test[0]))\n  else:\n  raise ValueError(\"Empty signature from {}\".format(peer_ip))\n  tx_count = len(signature_list)\n  if tx_count != len(set(signature_list)):\n  raise ValueError(\"There are duplicate transactions in this block, rejected\")\n  del signature_list[:]\n  execute(c, \"SELECT block_hash, block_height, timestamp FROM transactions WHERE reward != 0 ORDER BY block_height DESC LIMIT 1;\")\n  result = c.fetchall()\n  db_block_hash = result[0][0]\n  db_block_height = result[0][1]\n  q_db_timestamp_last = quantize_two(result[0][2])\n  block_height_new = db_block_height + 1\n  transaction_list_converted = []\n  for tx_index, transaction in enumerate(transaction_list):\n  q_time_now = quantize_two(time.time())\n  q_received_timestamp = quantize_two(transaction[0])\n  received_timestamp = '%.2f' % q_received_timestamp\n  received_address = str(transaction[1])[:56]\n  received_recipient = str(transaction[2])[:56]\n  received_amount = '%.8f' % (quantize_eight(transaction[3]))\n  received_signature_enc = str(transaction[4])[:684]\n  received_public_key_hashed = str(transaction[5])[:1068]\n  received_operation = str(transaction[6])[:30]\n  received_openfield = str(transaction[7])[:100000]\n  if tx_index == tx_count - 1:\n  q_block_timestamp = q_received_timestamp\n  nonce = received_openfield[:128]\n  miner_address = received_address\n  transaction_list_converted.append((received_timestamp, received_address, received_recipient,\n received_amount, received_signature_enc,\n received_public_key_hashed, received_operation,\n received_openfield))\n  received_public_key = RSA.importKey(base64.b64decode(received_public_key_hashed))\n  received_signature_dec = base64.b64decode(received_signature_enc)\n  verifier = PKCS1_v1_5.new(received_public_key)\n  validate_pem(received_public_key_hashed)\n  hash = SHA.new(str((received_timestamp, received_address, received_recipient, received_amount,\n  received_operation, received_openfield)).encode(\"utf-8\"))\n  if not verifier.verify(hash, received_signature_dec):\n  raise ValueError(\"Invalid signature from {}\".format(received_address))\n  else:\n  app_log.info(\"Valid signature from {} to {} amount {}\".format(received_address,\n    received_recipient,\n    received_amount))\n  if float(received_amount) < 0:\n  raise ValueError(\"Negative balance spend attempt\")\n  if received_address != hashlib.sha224(base64.b64decode(received_public_key_hashed)).hexdigest():\n  raise ValueError(\"Attempt to spend from a wrong address\")\n  if not essentials.address_validate(received_address):\n  raise ValueError(\"Not a valid sender address\")\n  if not essentials.address_validate(received_recipient):\n  raise ValueError(\"Not a valid recipient address\")\n  if q_time_now < q_received_timestamp:\n  raise ValueError(\n  \"Future transaction not allowed, timestamp {} minutes in the future\".format(\n  quantize_two((q_received_timestamp - q_time_now) / 60)))\n  if q_db_timestamp_last - 86400 > q_received_timestamp:\n  raise ValueError(\"Transaction older than 24h not allowed.\")\n  if q_block_timestamp <= q_db_timestamp_last:\n  raise ValueError(\"Block is older than the previous one, will be rejected\")\n  diff = difficulty(c)\n  app_log.warning(\"Time to generate block {}: {:.2f}\".format(db_block_height + 1, diff[2]))\n  app_log.warning(\"Current difficulty: {}\".format(diff[3]))\n  app_log.warning(\"Current blocktime: {}\".format(diff[4]))\n  app_log.warning(\"Current hashrate: {}\".format(diff[5]))\n  app_log.warning(\"New difficulty after adjustment: {}\".format(diff[6]))\n  app_log.warning(\"Difficulty: {} {}\".format(diff[0], diff[1]))\n  block_hash = hashlib.sha224(\n  (str(transaction_list_converted) + db_block_hash).encode(\"utf-8\")).hexdigest()\n  app_log.info(\"Calculated block hash: {}\".format(block_hash))\n  execute_param(h3, \"SELECT block_height FROM transactions WHERE block_hash = ?\", (block_hash,))\n  dummy = c.fetchone()\n  if dummy:\n  raise ValueError(\"Skipping digestion of block {} from {}, because we already have it on block_height {}\".\n   format(block_hash[:10], peer_ip, dummy[0]))\n  if block_height_new < POW_FORK:\n  diff_save = mining.check_block(block_height_new, miner_address, nonce, db_block_hash, diff[0],\n received_timestamp, q_received_timestamp, q_db_timestamp_last,\n peer_ip=peer_ip, app_log=app_log)\n  else:\n  diff_save = mining_heavy3.check_block(block_height_new, miner_address, nonce, db_block_hash, diff[0],\n    received_timestamp, q_received_timestamp, q_db_timestamp_last,\n    peer_ip=peer_ip, app_log=app_log)\n  fees_block = []\n  mining_reward = 0\n  balances = {}\n  for tx_index, transaction in enumerate(transaction_list):\n  db_timestamp = '%.2f' % quantize_two(transaction[0])\n  db_address = str(transaction[1])[:56]\n  db_recipient = str(transaction[2])[:56]\n  db_amount = '%.8f' % quantize_eight(transaction[3])\n  db_signature = str(transaction[4])[:684]\n  db_public_key_hashed = str(transaction[5])[:1068]\n  db_operation = str(transaction[6])[:30]\n  db_openfield = str(transaction[7])[:100000]\n  block_debit_address = 0\n  block_fees_address = 0\n  for x in transaction_list:\n  if x[1] == db_address:\n  block_debit_address = quantize_eight(Decimal(block_debit_address) + Decimal(x[3]))\n  if x != transaction_list[-1]:\n  block_fees_address = quantize_eight(Decimal(block_fees_address) + Decimal(\n  fee_calculate(db_openfield, db_operation,\n    last_block)))\n  balance_pre = ledger_balance3(db_address, c, balances)\n  balance = quantize_eight(balance_pre - block_debit_address)\n print(\"hyp2\")\n  fee = fee_calculate(db_openfield, db_operation, last_block)\n  fees_block.append(quantize_eight(fee))\n  if tx_index == tx_count - 1:\n  db_amount = 0\n  if db_block_height <= 10000000:\n  mining_reward = 15 - (quantize_eight(block_height_new) / quantize_eight(1000000 / 2)) - Decimal(\"0.8\")\n  if mining_reward < 0:\n  mining_reward = 0\n  else:\n  mining_reward = 0\n  reward = quantize_eight(mining_reward + sum(fees_block[:-1]))\n  fee = 0\n  else:\n  reward = 0\n  if quantize_eight(balance_pre) < quantize_eight(db_amount):\n  raise ValueError(\"{} sending more than owned\".format(db_address))\n  if quantize_eight(balance) - quantize_eight(block_fees_address) < 0:\n  raise ValueError(\"{} Cannot afford to pay fees\".format(db_address))\n  app_log.info(\"Block: Appending transaction back to block with {} transactions in it\".format(\n  len(block_transactions)))\n  block_transactions.append((block_height_new, db_timestamp, db_address, db_recipient, db_amount,\n db_signature, db_public_key_hashed, block_hash, fee, reward,\n db_operation, db_openfield))\n  try:\n  mp.MEMPOOL.delete_transaction(db_signature)\n  app_log.info(\"Block: Removed processed transaction {} from the mempool while digesting\".format(\n  db_signature[:56]))\n  except:\n  pass\n  execute_param(c, \"INSERT INTO misc VALUES (?, ?)\", (block_height_new, diff_save))\n  commit(conn)\n  plugin_manager.execute_action_hook('block',\n {'height': block_height_new, 'diff': diff_save,\n  'hash': block_hash, 'timestamp': float(q_block_timestamp),\n  'miner': miner_address, 'ip': peer_ip})\n  plugin_manager.execute_action_hook('fullblock',\n {'height': block_height_new, 'diff': diff_save,\n  'hash': block_hash, 'timestamp': float(q_block_timestamp),\n  'miner': miner_address, 'ip': peer_ip,\n  'transactions': block_transactions})\n  for transaction2 in block_transactions:\n  execute_param(c, \"INSERT INTO transactions VALUES (?,?,?,?,?,?,?,?,?,?,?,?)\", (\n  str(transaction2[0]), str(transaction2[1]),\n  str(transaction2[2]), str(transaction2[3]),\n  str(transaction2[4]), str(transaction2[5]),\n  str(transaction2[6]), str(transaction2[7]),\n  str(transaction2[8]), str(transaction2[9]),\n  str(transaction2[10]), str(transaction2[11])))\n  commit(conn)\n  if \"testnet\" in version or block_height_new >= 843000:\n  if int(block_height_new) % 10000 == 0:\n  staking.staking_update(conn, c, index, index_cursor, \"normal\", block_height_new, app_log)\n  staking.staking_payout(conn, c, index, index_cursor, block_height_new, float(q_block_timestamp), app_log)\n  staking.staking_revalidate(conn, c, index, index_cursor, block_height_new, app_log)\n  c.execute(\"SELECT * FROM transactions WHERE block_height = (SELECT max(block_height) FROM transactions)\")\n  tx_list_to_hash = c.fetchall()\n  mirror_hash = blake2b(str(tx_list_to_hash).encode(), digest_size=20).hexdigest()\n  if int(block_height_new) % 10 == 0:\n  execute_param(c, \"INSERT INTO transactions VALUES (?,?,?,?,?,?,?,?,?,?,?,?)\",\n    (-block_height_new, str(q_time_now), \"Development Reward\", str(genesis_conf),\n str(mining_reward), \"0\", \"0\", mirror_hash, \"0\", \"0\", \"0\", \"0\"))\n  commit(conn)\n  execute_param(c, \"INSERT INTO transactions VALUES (?,?,?,?,?,?,?,?,?,?,?,?)\",\n    (-block_height_new, str(q_time_now), \"Hypernode Payouts\",\n \"3e08b5538a4509d9daa99e01ca5912cda3e98a7f79ca01248c2bde16\",\n \"8\", \"0\", \"0\", mirror_hash, \"0\", \"0\", \"0\", \"0\"))\n  commit(conn)\n  app_log.warning(\n  \"Valid block: {}: {} digestion from {} completed in {}s.\".format(block_height_new, block_hash[:10],\n   peer_ip,\n   time.time() - float(q_time_now)))\n  del block_transactions[:]\n  peers.unban(peer_ip)\n  diff = difficulty(c)\n  plugin_manager.execute_action_hook('diff', diff[0])\n  except Exception as e:\n  app_log.warning(\"Block: processing failed: {}\".format(e))\n  failed_cause = str(e)\n  if peers.warning(sdef, peer_ip, \"Rejected block\", 2):\n  raise ValueError(\"{} banned\".format(peer_ip))\n  raise ValueError(\"Block: digestion aborted\")\n  finally:\n  if full_ledger or ram_conf:\n  db_to_drive(hdd, h, hdd2, h2)\n  db_lock.release()\n  delta_t = time.time() - float(q_time_now)\n  plugin_manager.execute_action_hook('digestblock',\n {'failed': failed_cause, 'ip': peer_ip, 'deltat': delta_t,\n  \"blocks\": block_count, \"txs\": tx_count})\n  else:\n  app_log.warning(\"Block: Skipping processing from {}, someone delivered data faster\".format(peer_ip))\n  plugin_manager.execute_action_hook('digestblock', {'failed': \"skipped\", 'ip': peer_ip})", "target": 0, "info": "Null", "idx": 0}
{"func": "def digest_block(data, sdef, peer_ip, conn, c, hdd, h, hdd2, h2, h3, index, index_cursor):\n  global hdd_block\n  global last_block\n  global peers\n  global plugin_manager\n  block_height_new = last_block + 1\n  block_hash = 'N/A'\n  failed_cause = ''\n  block_count = 0\n  tx_count = 0\n  if peers.is_banned(peer_ip):\n  raise ValueError(\"Cannot accept blocks from a banned peer\")\n  if not db_lock.locked():\n  db_lock.acquire()\n  while mp.MEMPOOL.lock.locked():\n  time.sleep(0.1)\n  app_log.info(\"Block: Waiting for mempool to unlock {}\".format(peer_ip))\n  app_log.warning(\"Block: Digesting started from {}\".format(peer_ip))\n  block_size = Decimal(sys.getsizeof(str(data))) / Decimal(1000000)\n  app_log.warning(\"Block: size: {} MB\".format(block_size))\n  try:\n  block_list = data\n  signature_list = []\n  block_transactions = []\n  for transaction_list in block_list:\n  block_count += 1\n  for entry in transaction_list:\n  tx_count += 1\n  entry_signature = entry[4]\n  if entry_signature:\n  signature_list.append(entry_signature)\n  execute_param(h3, \"SELECT block_height FROM transactions WHERE signature = ?;\",\n    (entry_signature,))\n  test = h3.fetchone()\n  if test:\n  raise ValueError(\"That transaction {} is already in our ram ledger, block_height {}\".format(\n  entry_signature[:10], test[0]))\n  execute_param(c, \"SELECT block_height FROM transactions WHERE signature = ?;\",\n    (entry_signature,))\n  test = c.fetchone()\n  if test:\n  raise ValueError(\"That transaction {} is already in our ledger, block_height {}\".format(\n  entry_signature[:10], test[0]))\n  else:\n  raise ValueError(\"Empty signature from {}\".format(peer_ip))\n  tx_count = len(signature_list)\n  if tx_count != len(set(signature_list)):\n  raise ValueError(\"There are duplicate transactions in this block, rejected\")\n  del signature_list[:]\n  execute(c, \"SELECT block_hash, block_height, timestamp FROM transactions WHERE reward != 0 ORDER BY block_height DESC LIMIT 1;\")\n  result = c.fetchall()\n  db_block_hash = result[0][0]\n  db_block_height = result[0][1]\n  q_db_timestamp_last = quantize_two(result[0][2])\n  block_height_new = db_block_height + 1\n  transaction_list_converted = []\n  for tx_index, transaction in enumerate(transaction_list):\n  q_time_now = quantize_two(time.time())\n  q_received_timestamp = quantize_two(transaction[0])\n  received_timestamp = '%.2f' % q_received_timestamp\n  received_address = str(transaction[1])[:56]\n  received_recipient = str(transaction[2])[:56]\n  received_amount = '%.8f' % (quantize_eight(transaction[3]))\n  received_signature_enc = str(transaction[4])[:684]\n  received_public_key_hashed = str(transaction[5])[:1068]\n  received_operation = str(transaction[6])[:30]\n  received_openfield = str(transaction[7])[:100000]\n  if tx_index == tx_count - 1:\n  q_block_timestamp = q_received_timestamp\n  nonce = received_openfield[:128]\n  miner_address = received_address\n  transaction_list_converted.append((received_timestamp, received_address, received_recipient,\n received_amount, received_signature_enc,\n received_public_key_hashed, received_operation,\n received_openfield))\n  received_public_key = RSA.importKey(base64.b64decode(received_public_key_hashed))\n  received_signature_dec = base64.b64decode(received_signature_enc)\n  verifier = PKCS1_v1_5.new(received_public_key)\n  validate_pem(received_public_key_hashed)\n  hash = SHA.new(str((received_timestamp, received_address, received_recipient, received_amount,\n  received_operation, received_openfield)).encode(\"utf-8\"))\n  if not verifier.verify(hash, received_signature_dec):\n  raise ValueError(\"Invalid signature from {}\".format(received_address))\n  else:\n  app_log.info(\"Valid signature from {} to {} amount {}\".format(received_address,\n    received_recipient,\n    received_amount))\n  if float(received_amount) < 0:\n  raise ValueError(\"Negative balance spend attempt\")\n  if received_address != hashlib.sha224(base64.b64decode(received_public_key_hashed)).hexdigest():\n  raise ValueError(\"Attempt to spend from a wrong address\")\n  if not essentials.address_validate(received_address):\n  raise ValueError(\"Not a valid sender address\")\n  if not essentials.address_validate(received_recipient):\n  raise ValueError(\"Not a valid recipient address\")\n  if q_time_now < q_received_timestamp:\n  raise ValueError(\n  \"Future transaction not allowed, timestamp {} minutes in the future\".format(\n  quantize_two((q_received_timestamp - q_time_now) / 60)))\n  if q_db_timestamp_last - 86400 > q_received_timestamp:\n  raise ValueError(\"Transaction older than 24h not allowed.\")\n  if q_block_timestamp <= q_db_timestamp_last:\n  raise ValueError(\"Block is older than the previous one, will be rejected\")\n  diff = difficulty(c)\n  app_log.warning(\"Time to generate block {}: {:.2f}\".format(db_block_height + 1, diff[2]))\n  app_log.warning(\"Current difficulty: {}\".format(diff[3]))\n  app_log.warning(\"Current blocktime: {}\".format(diff[4]))\n  app_log.warning(\"Current hashrate: {}\".format(diff[5]))\n  app_log.warning(\"New difficulty after adjustment: {}\".format(diff[6]))\n  app_log.warning(\"Difficulty: {} {}\".format(diff[0], diff[1]))\n  block_hash = hashlib.sha224(\n  (str(transaction_list_converted) + db_block_hash).encode(\"utf-8\")).hexdigest()\n  app_log.info(\"Calculated block hash: {}\".format(block_hash))\n  execute_param(h3, \"SELECT block_height FROM transactions WHERE block_hash = ?\", (block_hash,))\n  dummy = c.fetchone()\n  if dummy:\n  raise ValueError(\"Skipping digestion of block {} from {}, because we already have it on block_height {}\".\n   format(block_hash[:10], peer_ip, dummy[0]))\n  if block_height_new < POW_FORK:\n  diff_save = mining.check_block(block_height_new, miner_address, nonce, db_block_hash, diff[0],\n received_timestamp, q_received_timestamp, q_db_timestamp_last,\n peer_ip=peer_ip, app_log=app_log)\n  else:\n  diff_save = mining_heavy3.check_block(block_height_new, miner_address, nonce, db_block_hash, diff[0],\n    received_timestamp, q_received_timestamp, q_db_timestamp_last,\n    peer_ip=peer_ip, app_log=app_log)\n  fees_block = []\n  mining_reward = 0\n  balances = {}\n  for tx_index, transaction in enumerate(transaction_list):\n  db_timestamp = '%.2f' % quantize_two(transaction[0])\n  db_address = str(transaction[1])[:56]\n  db_recipient = str(transaction[2])[:56]\n  db_amount = '%.8f' % quantize_eight(transaction[3])\n  db_signature = str(transaction[4])[:684]\n  db_public_key_hashed = str(transaction[5])[:1068]\n  db_operation = str(transaction[6])[:30]\n  db_openfield = str(transaction[7])[:100000]\n  block_debit_address = 0\n  block_fees_address = 0\n  for x in transaction_list:\n  if x[1] == db_address:\n  block_debit_address = quantize_eight(Decimal(block_debit_address) + Decimal(x[3]))\n  if x != transaction_list[-1]:\n  block_fees_address = quantize_eight(Decimal(block_fees_address) + Decimal(\n  fee_calculate(db_openfield, db_operation,\n    last_block)))\n  balance_pre = ledger_balance3(db_address, h2, balances)\n  balance = quantize_eight(balance_pre - block_debit_address)\n print(\"hyp2\")\n  fee = fee_calculate(db_openfield, db_operation, last_block)\n  fees_block.append(quantize_eight(fee))\n  if tx_index == tx_count - 1:\n  db_amount = 0\n  if db_block_height <= 10000000:\n  mining_reward = 15 - (quantize_eight(block_height_new) / quantize_eight(1000000 / 2)) - Decimal(\"0.8\")\n  if mining_reward < 0:\n  mining_reward = 0\n  else:\n  mining_reward = 0\n  reward = quantize_eight(mining_reward + sum(fees_block[:-1]))\n  fee = 0\n  else:\n  reward = 0\n  if quantize_eight(balance_pre) < quantize_eight(db_amount):\n  raise ValueError(\"{} sending more than owned\".format(db_address))\n  if quantize_eight(balance) - quantize_eight(block_fees_address) < 0:\n  raise ValueError(\"{} Cannot afford to pay fees\".format(db_address))\n  app_log.info(\"Block: Appending transaction back to block with {} transactions in it\".format(\n  len(block_transactions)))\n  block_transactions.append((block_height_new, db_timestamp, db_address, db_recipient, db_amount,\n db_signature, db_public_key_hashed, block_hash, fee, reward,\n db_operation, db_openfield))\n  try:\n  mp.MEMPOOL.delete_transaction(db_signature)\n  app_log.info(\"Block: Removed processed transaction {} from the mempool while digesting\".format(\n  db_signature[:56]))\n  except:\n  pass\n  execute_param(c, \"INSERT INTO misc VALUES (?, ?)\", (block_height_new, diff_save))\n  commit(conn)\n  plugin_manager.execute_action_hook('block',\n {'height': block_height_new, 'diff': diff_save,\n  'hash': block_hash, 'timestamp': float(q_block_timestamp),\n  'miner': miner_address, 'ip': peer_ip})\n  plugin_manager.execute_action_hook('fullblock',\n {'height': block_height_new, 'diff': diff_save,\n  'hash': block_hash, 'timestamp': float(q_block_timestamp),\n  'miner': miner_address, 'ip': peer_ip,\n  'transactions': block_transactions})\n  for transaction2 in block_transactions:\n  execute_param(c, \"INSERT INTO transactions VALUES (?,?,?,?,?,?,?,?,?,?,?,?)\", (\n  str(transaction2[0]), str(transaction2[1]),\n  str(transaction2[2]), str(transaction2[3]),\n  str(transaction2[4]), str(transaction2[5]),\n  str(transaction2[6]), str(transaction2[7]),\n  str(transaction2[8]), str(transaction2[9]),\n  str(transaction2[10]), str(transaction2[11])))\n  commit(conn)\n  if \"testnet\" in version or block_height_new >= 843000:\n  if int(block_height_new) % 10000 == 0:\n  staking.staking_update(conn, c, index, index_cursor, \"normal\", block_height_new, app_log)\n  staking.staking_payout(conn, c, index, index_cursor, block_height_new, float(q_block_timestamp), app_log)\n  staking.staking_revalidate(conn, c, index, index_cursor, block_height_new, app_log)\n  c.execute(\"SELECT * FROM transactions WHERE block_height = (SELECT max(block_height) FROM transactions)\")\n  tx_list_to_hash = c.fetchall()\n  mirror_hash = blake2b(str(tx_list_to_hash).encode(), digest_size=20).hexdigest()\n  if int(block_height_new) % 10 == 0:\n  execute_param(c, \"INSERT INTO transactions VALUES (?,?,?,?,?,?,?,?,?,?,?,?)\",\n    (-block_height_new, str(q_time_now), \"Development Reward\", str(genesis_conf),\n str(mining_reward), \"0\", \"0\", mirror_hash, \"0\", \"0\", \"0\", \"0\"))\n  commit(conn)\n  execute_param(c, \"INSERT INTO transactions VALUES (?,?,?,?,?,?,?,?,?,?,?,?)\",\n    (-block_height_new, str(q_time_now), \"Hypernode Payouts\",\n \"3e08b5538a4509d9daa99e01ca5912cda3e98a7f79ca01248c2bde16\",\n \"8\", \"0\", \"0\", mirror_hash, \"0\", \"0\", \"0\", \"0\"))\n  commit(conn)\n  app_log.warning(\n  \"Valid block: {}: {} digestion from {} completed in {}s.\".format(block_height_new, block_hash[:10],\n   peer_ip,\n   time.time() - float(q_time_now)))\n  del block_transactions[:]\n  peers.unban(peer_ip)\n  diff = difficulty(c)\n  plugin_manager.execute_action_hook('diff', diff[0])\n  except Exception as e:\n  app_log.warning(\"Block: processing failed: {}\".format(e))\n  failed_cause = str(e)\n  if peers.warning(sdef, peer_ip, \"Rejected block\", 2):\n  raise ValueError(\"{} banned\".format(peer_ip))\n  raise ValueError(\"Block: digestion aborted\")\n  finally:\n  if full_ledger or ram_conf:\n  db_to_drive(hdd, h, hdd2, h2)\n  db_lock.release()\n  delta_t = time.time() - float(q_time_now)\n  plugin_manager.execute_action_hook('digestblock',\n {'failed': failed_cause, 'ip': peer_ip, 'deltat': delta_t,\n  \"blocks\": block_count, \"txs\": tx_count})\n  else:\n  app_log.warning(\"Block: Skipping processing from {}, someone delivered data faster\".format(peer_ip))\n  plugin_manager.execute_action_hook('digestblock', {'failed': \"skipped\", 'ip': peer_ip})", "target": 1, "info": "Null", "idx": 0}
{"func": "def get(self):\n conn = sqlite3.connect(ledger_path)\n  conn = sqlite3.connect(hyper_path)\n  c = conn.cursor()\n  execute(c, \"SELECT * FROM transactions ORDER BY block_height DESC, timestamp DESC LIMIT 100;\")\n  all = c.fetchall()[::-1]\n  axis0 = []\n  axis1 = []\n  axis4 = []\n  axis8 = []\n  axis9 = []\n  axis10 = []\n  i = 1\n  for x in all:\n  axis0.append(x[0])\n  axis1.append(x[1])\n  axis4.append(x[4])\n  axis8.append(x[8])\n  axis9.append(x[9])\n  axis10.append(len(str(x)))\n  plotter = []\n  plotter.append('<canvas id=\"canvas\" height=\"150\" width=\"600\"></canvas>\\n')\n  plotter.append('<script>\\n')\n  plotter.append(\"var ctx = document.getElementById('canvas').getContext('2d');\")\n  plotter.append(\"var canvas = new Chart(ctx, {\\n\")\n  plotter.append(\"type: 'line',\\n\")\n  plotter.append(\"data: {\\n\")\n  plotter.append(\"labels: \" + str(list(map(str, axis0))) + \",\\n\")\n  plotter.append(\"datasets: [{\\n\")\n  plotter.append(\"label: 'Timestamp progression',\\n\")\n  plotter.append(\"data: \" + str(list(map(str, axis1))) + \",\\n\")\n  plotter.append(\"backgroundColor: 'rgba(153, 255, 51, 0.4)'\\n\")\n  plotter.append(\"}, {\\n\")\n  plotter.append(\"label: 'Spending',\\n\")\n  plotter.append(\"hidden: true,\\n\")\n  plotter.append(\"data: \" + str(list(map(str, axis4))) + \",\\n\")\n  plotter.append(\"backgroundColor: 'rgba(255, 153, 0, 0.4)'\\n\")\n  plotter.append(\"}, {\\n\")\n  plotter.append(\"label: 'Fee',\\n\")\n  plotter.append(\"hidden: true,\\n\")\n  plotter.append(\"data: \" + str(list(map(str, axis8))) + \",\\n\")\n  plotter.append(\"backgroundColor: 'rgba(63, 65, 191, 0.4)'\\n\")\n  plotter.append(\"}, {\\n\")\n  plotter.append(\"label: 'Transaction size',\\n\")\n  plotter.append(\"hidden: true,\\n\")\n  plotter.append(\"data: \" + str(list(map(str, axis10))) + \",\\n\")\n  plotter.append(\"backgroundColor: 'rgba(300, 50, 0, 0.4)'\\n\")\n  plotter.append(\"}, {\\n\")\n  plotter.append(\"label: 'Reward',\\n\")\n  plotter.append(\"hidden: true,\\n\")\n  plotter.append(\"data: \" + str(list(map(str, axis9))) + \",\\n\")\n  plotter.append(\"backgroundColor: 'rgba(189, 63, 191, 0.4)'\\n\")\n  plotter.append(\"}]\\n\")\n  plotter.append(\"}\\n\")\n  plotter.append(\"});\\n\")\n  plotter.append('</script>\\n')\n  execute(c, \"SELECT * FROM transactions ORDER BY block_height DESC, timestamp DESC LIMIT 500;\")\n  all = c.fetchall()\n  tx_count = 0\n  for x in all:\n  if x[9] == 0:\n  tx_count = tx_count + 1\n  transferred_total = 0\n  for x in all:\n  if x[9] == 0:\n  transferred_total = transferred_total + x[4]\n  execute(c, \"SELECT difficulty FROM misc ORDER BY block_height DESC LIMIT 500;\")\n  diffs = c.fetchall()\n  view = []\n  i = 0\n  b = -1\n  for x in all:\n  if i % 2 == 0:\n  color_cell = \"\n  else:\n  color_cell = \"white\"\n  try:\n  x_old\n  except:\n  x_old = \"init\"\n  if x[0] != x_old:\n  b = b + 1\n  view.append(\"<tr bgcolor ={}>\".format(color_cell))\n  view.append(\"<td>{}</td>\".format(x[0]))\n  view.append(\"<td>{}\".format(time.strftime(\"%Y/%m/%d,%H:%M:%S\", time.gmtime(float(x[1])))))\n  view.append(\"<td>{}</td>\".format(x[2]))\n  view.append(\"<td>{}</td>\".format(x[3]))\n  view.append(\"<td>{}</td>\".format(x[4]))\n  view.append(\"<td>{}</td>\".format(x[7]))\n  view.append(\"<td>{}</td>\".format(x[8]))\n  view.append(\"<td>{}</td>\".format('%.6f' % float(x[9])))\n  view.append(\"<td>{}</td>\".format('%.10f' % float(diffs[b][0])))\n  view.append(\"<tr>\")\n  x_old = x[0]\n  i = i + 1\n  c.close()\n  html = []\n  html.append('<!doctype html>\\n')\n  html.append('<html>\\n')\n  html.append('<head>\\n')\n  html.append('<title>Transaction Explorer</title>\\n')\n  html.append('<link rel = \"icon\" href = \"static/explorer.ico\" type = \"image/x-icon\" / >\\n')\n  html.append('<link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css\" >')\n  html.append('<script src=\"static/Chart.js\"></script>\\n')\n  html.append('<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js\"></script>')\n  html.append('<script src=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js\"></script >')\n  html.append('</head>\\n')\n  html.append('<body bgcolor = \"white\">\\n')\n  html.append(\"<body>\")\n  html.append(\"<body background='static/explorer_bg.png'>\")\n  html.append(\"<div class ='container-fluid'>\")\n  html.append(\"<div class='row'>\")\n  html.append(\"<center><h1>Bismuth Transaction Explorer</h1><p></center>\")\n  html.append(\"<div style='padding: 20px;'>\")\n  html.append(\"<center><a href='http://bismuth.cz/ledger.tar.gz' class='btn btn-info' role='button'>Download Blockchain</a></center>\")\n  html.append(\"</div>\")\n  html.append(\"</div>\")\n  html.append(\"<div class ='row'>\")\n  html.append(''.join(plotter))\n  html.append(\"</div>\")\n  data_total = str(sys.getsizeof(str(all))/1024)\n  html.append(\"<div class ='container-fluid'>\")\n  html.append(\"<table class='table table-responsive'>\")\n   html.append(\"<tr><th>Statistics for the last 500 blocks</th>\")\n  html.append(\"<tr><td>Kilobytes transferred: </td><td>{}</td>\".format(data_total))\n  html.append(\"<tr><td>Transactions: </td><td>{}</td>\".format(tx_count))\n  html.append(\"<tr><td>Transactions per block: </td><td>{}</td>\".format(tx_count/500))\n  html.append(\"<tr><td>Total BIS transferred </td><td>{}</td>\".format(transferred_total))\n  html.append(\"</table>\")\n  html.append(\"</div>\")\n  html.append(\"<div class ='row'>\")\n  html.append(\"<table class='table table-responsive'>\")\n  html.append(\"<tr bgcolor='white'>\")\n  html.append(\"<td>Block</td>\")\n  html.append(\"<td>Timestamp</td>\")\n  html.append(\"<td>From</td>\")\n  html.append(\"<td>To</td>\")\n  html.append(\"<td>Amount</td>\")\n  html.append(\"<td>Block Hash</td>\")\n  html.append(\"<td>Fee</td>\")\n  html.append(\"<td>Reward</td>\")\n  html.append(\"<td>Difficulty</td>\")\n  html.append(\"</tr>\")\n  html.append(''.join(view))\n  html.append(\"</table>\")\n  html.append(\"</div>\")\n  html.append(\"</div>\")\n  html.append(\"</body>\")\n  html.append(\"</html>\")\n  self.write(''.join(html))", "target": 0, "info": "Null", "idx": 0}
{"func": "def get(self):\n conn = sqlite3.connect(ledger_path)\n  conn = sqlite3.connect(hyper_path)\n  c = conn.cursor()\n  execute(c, \"SELECT * FROM transactions ORDER BY block_height DESC, timestamp DESC LIMIT 100;\")\n  all = c.fetchall()[::-1]\n  axis0 = []\n  axis1 = []\n  axis4 = []\n  axis8 = []\n  axis9 = []\n  axis10 = []\n  i = 1\n  for x in all:\n  axis0.append(x[0])\n  axis1.append(x[1])\n  axis4.append(x[4])\n  axis8.append(x[8])\n  axis9.append(x[9])\n  axis10.append(len(str(x)))\n  plotter = []\n  plotter.append('<canvas id=\"canvas\" height=\"150\" width=\"600\"></canvas>\\n')\n  plotter.append('<script>\\n')\n  plotter.append(\"var ctx = document.getElementById('canvas').getContext('2d');\")\n  plotter.append(\"var canvas = new Chart(ctx, {\\n\")\n  plotter.append(\"type: 'line',\\n\")\n  plotter.append(\"data: {\\n\")\n  plotter.append(\"labels: \" + str(list(map(str, axis0))) + \",\\n\")\n  plotter.append(\"datasets: [{\\n\")\n  plotter.append(\"label: 'Timestamp progression',\\n\")\n  plotter.append(\"data: \" + str(list(map(str, axis1))) + \",\\n\")\n  plotter.append(\"backgroundColor: 'rgba(153, 255, 51, 0.4)'\\n\")\n  plotter.append(\"}, {\\n\")\n  plotter.append(\"label: 'Spending',\\n\")\n  plotter.append(\"hidden: true,\\n\")\n  plotter.append(\"data: \" + str(list(map(str, axis4))) + \",\\n\")\n  plotter.append(\"backgroundColor: 'rgba(255, 153, 0, 0.4)'\\n\")\n  plotter.append(\"}, {\\n\")\n  plotter.append(\"label: 'Fee',\\n\")\n  plotter.append(\"hidden: true,\\n\")\n  plotter.append(\"data: \" + str(list(map(str, axis8))) + \",\\n\")\n  plotter.append(\"backgroundColor: 'rgba(63, 65, 191, 0.4)'\\n\")\n  plotter.append(\"}, {\\n\")\n  plotter.append(\"label: 'Transaction size',\\n\")\n  plotter.append(\"hidden: true,\\n\")\n  plotter.append(\"data: \" + str(list(map(str, axis10))) + \",\\n\")\n  plotter.append(\"backgroundColor: 'rgba(300, 50, 0, 0.4)'\\n\")\n  plotter.append(\"}, {\\n\")\n  plotter.append(\"label: 'Reward',\\n\")\n  plotter.append(\"hidden: true,\\n\")\n  plotter.append(\"data: \" + str(list(map(str, axis9))) + \",\\n\")\n  plotter.append(\"backgroundColor: 'rgba(189, 63, 191, 0.4)'\\n\")\n  plotter.append(\"}]\\n\")\n  plotter.append(\"}\\n\")\n  plotter.append(\"});\\n\")\n  plotter.append('</script>\\n')\n  execute(c, \"SELECT * FROM transactions ORDER BY block_height DESC, timestamp DESC LIMIT 500;\")\n  all = c.fetchall()\n  tx_count = 0\n  for x in all:\n  if x[9] == 0:\n  tx_count = tx_count + 1\n  transferred_total = 0\n  for x in all:\n  if x[9] == 0:\n  transferred_total = transferred_total + x[4]\n  execute(c, \"SELECT difficulty FROM misc ORDER BY block_height DESC LIMIT 500;\")\n  diffs = c.fetchall()\n  view = []\n  i = 0\n  b = -1\n  for x in all:\n  if i % 2 == 0:\n  color_cell = \"\n  else:\n  color_cell = \"white\"\n  try:\n  x_old\n  except:\n  x_old = \"init\"\n  if x[0] != x_old:\n  b = b + 1\n  view.append(\"<tr bgcolor ={}>\".format(color_cell))\n  view.append(\"<td>{}</td>\".format(x[0]))\n  view.append(\"<td>{}\".format(time.strftime(\"%Y/%m/%d,%H:%M:%S\", time.gmtime(float(x[1])))))\n  view.append(\"<td>{}</td>\".format(x[2]))\n  view.append(\"<td>{}</td>\".format(x[3]))\n  view.append(\"<td>{}</td>\".format(x[4]))\n  view.append(\"<td>{}</td>\".format(x[7]))\n  view.append(\"<td>{}</td>\".format(x[8]))\n  view.append(\"<td>{}</td>\".format('%.6f' % float(x[9])))\n  view.append(\"<td>{}</td>\".format('%.10f' % float(diffs[b][0])))\n  view.append(\"<tr>\")\n  x_old = x[0]\n  i = i + 1\n  c.close()\n  html = []\n  html.append('<!doctype html>\\n')\n  html.append('<html>\\n')\n  html.append('<head>\\n')\n  html.append('<title>Transaction Explorer</title>\\n')\n  html.append('<link rel = \"icon\" href = \"static/explorer.ico\" type = \"image/x-icon\" / >\\n')\n  html.append('<link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css\" >')\n  html.append('<script src=\"static/Chart.js\"></script>\\n')\n  html.append('<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js\"></script>')\n  html.append('<script src=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js\"></script >')\n  html.append('</head>\\n')\n  html.append('<body bgcolor = \"white\">\\n')\n  html.append(\"<body>\")\n  html.append(\"<body background='static/explorer_bg.png'>\")\n  html.append(\"<div class ='container-fluid'>\")\n  html.append(\"<div class='row'>\")\n  html.append(\"<center><h1>Bismuth Transaction Explorer</h1><p></center>\")\n  html.append(\"<div style='padding: 20px;'>\")\n  html.append(\"<center><a href='http://bismuth.cz/ledger.tar.gz' class='btn btn-info' role='button'>Download Blockchain</a></center>\")\n  html.append(\"</div>\")\n  html.append(\"</div>\")\n  html.append(\"<div class ='row'>\")\n  html.append(''.join(plotter))\n  html.append(\"</div>\")\n  data_total = str(sys.getsizeof(str(all))/1024)\n  html.append(\"<div class ='container-fluid'>\")\n  html.append(\"<table class='table table-responsive'>\")\n   html.append(\"<tr><th>Statistics for the last 500 blocks</th>\")\n  html.append(\"<tr><td>Kilobytes transferred: </td><td>{}</td>\".format(transferred_total))\n  html.append(\"<tr><td>Transactions: </td><td>{}</td>\".format(tx_count))\n  html.append(\"<tr><td>Transactions per block: </td><td>{}</td>\".format(tx_count/500))\n  html.append(\"<tr><td>Total BIS transferred </td><td>{}</td>\".format(transferred_total))\n  html.append(\"</table>\")\n  html.append(\"</div>\")\n  html.append(\"<div class ='row'>\")\n  html.append(\"<table class='table table-responsive'>\")\n  html.append(\"<tr bgcolor='white'>\")\n  html.append(\"<td>Block</td>\")\n  html.append(\"<td>Timestamp</td>\")\n  html.append(\"<td>From</td>\")\n  html.append(\"<td>To</td>\")\n  html.append(\"<td>Amount</td>\")\n  html.append(\"<td>Block Hash</td>\")\n  html.append(\"<td>Fee</td>\")\n  html.append(\"<td>Reward</td>\")\n  html.append(\"<td>Difficulty</td>\")\n  html.append(\"</tr>\")\n  html.append(''.join(view))\n  html.append(\"</table>\")\n  html.append(\"</div>\")\n  html.append(\"</div>\")\n  html.append(\"</body>\")\n  html.append(\"</html>\")\n  self.write(''.join(html))", "target": 1, "info": "Null", "idx": 0}
{"func": "def manager(c, conn):\n  global banlist\n  peer_dict = peers_get()\n  reset_time = startup_time\n  while True:\n  variability = []\n  del variability [:]\n  variable = []\n  del variable [:]\n  for key, value in peer_dict.items():\n  variability.append(key.split(\".\")[:-1])\n  for x in variability:\n  if variability.count(x) < 3:\n  variable.append(\".\".join(x))\n  for key, value in peer_dict.items():\n  HOST = key\n  PORT = int(value)\n  for x in variable:\n  if x in HOST:\n  if \"testnet\" in version:\n  PORT = 2829\n  if threading.active_count() < thread_limit_conf and str(HOST + \":\" + str(PORT)) not in tried and str(HOST + \":\" + str(PORT)) not in connection_pool and str(HOST) not in banlist:\n  app_log.info(\"Will attempt to connect to {}:{}\".format(HOST, PORT))\n  tried.append(HOST + \":\" + str(PORT))\n  t = threading.Thread(target=worker, args=(HOST, PORT))\n  app_log.info(\"---Starting a client thread \" + str(threading.currentThread()) + \"---\")\n  t.daemon = True\n  t.start()\n  if len(connection_pool) < nodes_ban_reset and int(time.time() - startup_time) > 15:\n  app_log.warning(\"Only {} connections active, resetting banlist\".format(len(connection_pool)))\n  del banlist[:]\n  banlist.extend(config.banlist)\n  del warning_list[:]\n  if len(connection_pool) < 10:\n  app_log.warning(\"Only {} connections active, resetting the connection history\".format(len(connection_pool)))\n  del tried[:]\n   if nodes_ban_reset and len(connection_pool) <= len(banlist) and int(time.time() - reset_time) > 60*10:\n  app_log.warning(\"Less active connections ({}) than banlist ({}), resetting banlist and tried\" .format(len(connection_pool), len(banlist)))\n  del banlist[:]\n  banlist.extend(config.banlist)\n  del warning_list[:]\n  del tried[:]\n  reset_time = time.time()\n  if banlist:\n  app_log.warning(\"Status: Banlist: {}\".format(banlist))\n  app_log.warning(\"Status: Banlist Count : {}\".format(len(banlist)))\n  if whitelist:\n  app_log.warning(\"Status: Whitelist: {}\".format(whitelist))\n  app_log.info(\"Status: Syncing nodes: {}\".format(syncing))\n  app_log.info(\"Status: Syncing nodes: {}/3\".format(len(syncing)))\n  app_log.warning(\"Status: Threads at {} / {}\".format(threading.active_count(), thread_limit_conf))\n  app_log.info(\"Status: Tried: {}\".format(tried))\n  app_log.info(\"Status: Tried Count: {}\".format(len(tried)))\n  app_log.info(\"Status: List of Outbound connections: {}\".format(connection_pool))\n  app_log.warning(\"Status: Number of Outbound connections: {}\".format(len(connection_pool)))\n  if consensus:\n  app_log.warning(\"Status: Consensus: {} = {}%\".format(consensus, consensus_percentage))\n  app_log.warning(\"Status: Consensus IP list: {}\".format(peer_ip_list))\n  app_log.warning(\"Status: Consensus opinion list: {}\".format(consensus_blockheight_list))\n  app_log.warning(\"Status: Total number of nodes: {}\".format(len(consensus_blockheight_list)))\n  execute(c, \"SELECT block_height, timestamp FROM transactions WHERE reward != 0 ORDER BY block_height DESC LIMIT 1;\")\n  result = c.fetchall()[0]\n  last_block = result[0]\n  last_block_ago = float(result[1])\n  app_log.warning(\"Status: Last block {} was generated {} minutes ago\".format(last_block, '%.2f' % ((time.time() - last_block_ago) / 60)))\n  time.sleep(30)", "target": 0, "info": "Null", "idx": 0}
{"func": "def manager(c, conn):\n  global banlist\n  peer_dict = peers_get()\n  reset_time = startup_time\n  while True:\n  variability = []\n  del variability [:]\n  variable = []\n  del variable [:]\n  for key, value in peer_dict.items():\n  variability.append(key.split(\".\")[:-1])\n  for x in variability:\n  if variability.count(x) < 3:\n  variable.append(\".\".join(x))\n  for key, value in peer_dict.items():\n  HOST = key\n  PORT = int(value)\n  for x in variable:\n  if x in HOST:\n  if \"testnet\" in version:\n  PORT = 2829\n  if threading.active_count() < thread_limit_conf and str(HOST + \":\" + str(PORT)) not in tried and str(HOST + \":\" + str(PORT)) not in connection_pool and str(HOST) not in banlist:\n  app_log.info(\"Will attempt to connect to {}:{}\".format(HOST, PORT))\n  tried.append(HOST + \":\" + str(PORT))\n  t = threading.Thread(target=worker, args=(HOST, PORT))\n  app_log.info(\"---Starting a client thread \" + str(threading.currentThread()) + \"---\")\n  t.daemon = True\n  t.start()\n  if len(connection_pool) < nodes_ban_reset and int(time.time() - startup_time) > 15:\n  app_log.warning(\"Only {} connections active, resetting banlist\".format(len(connection_pool)))\n  del banlist[:]\n  banlist.extend(config.banlist)\n  del warning_list[:]\n  if len(connection_pool) < 10:\n  app_log.warning(\"Only {} connections active, resetting the connection history\".format(len(connection_pool)))\n  del tried[:]\n   if nodes_ban_reset and len(connection_pool) < len(banlist) and int(time.time() - reset_time) > 60*10:\n  app_log.warning(\"Less active connections ({}) than banlist ({}), resetting banlist and tried\" .format(len(connection_pool), len(banlist)))\n  del banlist[:]\n  banlist.extend(config.banlist)\n  del warning_list[:]\n  del tried[:]\n  reset_time = time.time()\n  if banlist:\n  app_log.warning(\"Status: Banlist: {}\".format(banlist))\n  app_log.warning(\"Status: Banlist Count : {}\".format(len(banlist)))\n  if whitelist:\n  app_log.warning(\"Status: Whitelist: {}\".format(whitelist))\n  app_log.info(\"Status: Syncing nodes: {}\".format(syncing))\n  app_log.info(\"Status: Syncing nodes: {}/3\".format(len(syncing)))\n  app_log.warning(\"Status: Threads at {} / {}\".format(threading.active_count(), thread_limit_conf))\n  app_log.info(\"Status: Tried: {}\".format(tried))\n  app_log.info(\"Status: Tried Count: {}\".format(len(tried)))\n  app_log.info(\"Status: List of Outbound connections: {}\".format(connection_pool))\n  app_log.warning(\"Status: Number of Outbound connections: {}\".format(len(connection_pool)))\n  if consensus:\n  app_log.warning(\"Status: Consensus: {} = {}%\".format(consensus, consensus_percentage))\n  app_log.warning(\"Status: Consensus IP list: {}\".format(peer_ip_list))\n  app_log.warning(\"Status: Consensus opinion list: {}\".format(consensus_blockheight_list))\n  app_log.warning(\"Status: Total number of nodes: {}\".format(len(consensus_blockheight_list)))\n  execute(c, \"SELECT block_height, timestamp FROM transactions WHERE reward != 0 ORDER BY block_height DESC LIMIT 1;\")\n  result = c.fetchall()[0]\n  last_block = result[0]\n  last_block_ago = float(result[1])\n  app_log.warning(\"Status: Last block {} was generated {} minutes ago\".format(last_block, '%.2f' % ((time.time() - last_block_ago) / 60)))\n  time.sleep(30)", "target": 1, "info": "Null", "idx": 0}
{"func": "def difficulty(c):\n  execute(c, \"SELECT * FROM transactions ORDER BY block_height DESC LIMIT 1\")\n  result = c.fetchall()[0]\n  timestamp_last = float(result[1])\n  block_height = int(result[0])\n  execute_param(c, (\"SELECT block_height FROM transactions WHERE CAST(timestamp AS INTEGER) > ? AND reward != 0\"), (timestamp_last - 86400,))\n  blocks_per_1440 = len(c.fetchall())\n  app_log.warning(\"Blocks per day: {}\".format(blocks_per_1440))\n  execute(c, (\"SELECT difficulty FROM misc ORDER BY block_height DESC LIMIT 1\"))\n  try:\n  diff_block_previous = float(c.fetchone()[0])\n  except:\n  diff_block_previous = 45\n  try:\n  log = math.log2(blocks_per_1440 / 1440)\n  except:\n  log = math.log2(0.5 / 1440)\n  app_log.info(\"Difficulty retargeting: {}\".format(log))\n  difficulty = diff_block_previous + log\n   time_now = time.time()\n  if time_now > timestamp_last + 300:\n  difficulty2 = percentage(97,difficulty)\n  else:\n  difficulty2 = difficulty\n   if difficulty < 45 or difficulty2 < 45:\n  difficulty = 45\n  difficulty2 = 45\n  app_log.warning(\"Difficulty: {}\".format(difficulty))\n  return (float(difficulty), float(difficulty2))", "target": 0, "info": "Null", "idx": 0}
{"func": "def difficulty(c):\n  execute(c, \"SELECT * FROM transactions ORDER BY block_height DESC LIMIT 1\")\n  result = c.fetchall()[0]\n  timestamp_last = float(result[1])\n  block_height = int(result[0])\n  execute_param(c, (\"SELECT block_height FROM transactions WHERE CAST(timestamp AS INTEGER) > ? AND reward != 0\"), (timestamp_last - 86400,))\n  blocks_per_1440 = len(c.fetchall())\n  app_log.warning(\"Blocks per day: {}\".format(blocks_per_1440))\n  execute(c, (\"SELECT difficulty FROM misc ORDER BY block_height DESC LIMIT 1\"))\n  try:\n  diff_block_previous = float(c.fetchone()[0])\n  except:\n  diff_block_previous = 45\n  try:\n  log = math.log2(blocks_per_1440 / 1440)\n  except:\n  log = math.log2(0.5 / 1440)\n  app_log.info(\"Difficulty retargeting: {}\".format(log))\n  difficulty = diff_block_previous + log\n   time_now = time.time()\n  if time_now > timestamp_last + 300:\n  difficulty2 = percentage(97,diff_block_previous)\n  else:\n  difficulty2 = difficulty\n   if difficulty < 45 or difficulty2 < 45:\n  difficulty = 45\n  difficulty2 = 45\n  app_log.warning(\"Difficulty: {}\".format(difficulty))\n  return (float(difficulty), float(difficulty2))", "target": 1, "info": "Null", "idx": 0}
{"func": "def keys_load(privkey_file, pubkey_file):\n  privkey_loaded = open(privkey_file, 'rb').read()\n  privkey = SigningKey.from_string(privkey_loaded, curve=SECP256k1)\n  pubkey_loaded = open(pubkey_file, 'rb').read()\n  pubkey = VerifyingKey.from_string(pubkey_loaded, curve=SECP256k1)\n   address = blake2b(privkey.to_string(), digest_size=20).hexdigest()\n   return privkey, pubkey, address", "target": 0, "info": "Null", "idx": 0}
{"func": "def keys_load(privkey_file, pubkey_file):\n  privkey_loaded = open(privkey_file, 'rb').read()\n  privkey = SigningKey.from_string(privkey_loaded, curve=SECP256k1)\n  pubkey_loaded = open(pubkey_file, 'rb').read()\n  pubkey = VerifyingKey.from_string(pubkey_loaded, curve=SECP256k1)\n   address = blake2b(pubkey.to_string(), digest_size=20).hexdigest()\n   return privkey, pubkey, address", "target": 1, "info": "Null", "idx": 0}
{"func": "def difficulty(c, mode):\n  getcontext().prec = 13\n  execute(c, \"SELECT * FROM transactions ORDER BY block_height DESC LIMIT 2\")\n  result = c.fetchone()\n  timestamp_last = Decimal(result[1])\n  block_height = int(result[0])\n  timestamp_before_last = Decimal(c.fetchone()[1])\n   if block_height >= 427000:\n  execute(c, \"SELECT * FROM transactions WHERE reward != 0 ORDER BY block_height DESC LIMIT 2\")\n  result = c.fetchone()\n  timestamp_last = Decimal(result[1])\n  block_height = int(result[0])\n  timestamp_before_last = Decimal(c.fetchone()[1])", "target": 0, "info": "Null", "idx": 0}
{"func": "def difficulty(c, mode):\n  getcontext().prec = 13\n  execute(c, \"SELECT * FROM transactions ORDER BY block_height DESC LIMIT 2\")\n  result = c.fetchone()\n  timestamp_last = Decimal(result[1])\n  block_height = int(result[0])\n  timestamp_before_last = Decimal(c.fetchone()[1])\n   if block_height > 427000:\n  execute(c, \"SELECT * FROM transactions WHERE reward != 0 ORDER BY block_height DESC LIMIT 2\")\n  result = c.fetchone()\n  timestamp_last = Decimal(result[1])\n  block_height = int(result[0])\n  timestamp_before_last = Decimal(c.fetchone()[1])", "target": 1, "info": "Null", "idx": 0}
{"func": "def _as_html_tags_eval_item(item):\n  if not isinstance(item, HTMLTagsEvalItem):\n  return HTMLTagsEvalItem(item)\n  return item", "target": 0, "info": "Null", "idx": 0}
{"func": "def _as_html_tags_eval_item(item):\n  if not isinstance(HTMLTagsEvalItem, item):\n  return HTMLTagsEvalItem(item)\n  return item", "target": 1, "info": "Null", "idx": 0}
{"func": "def intersectionRects(rect_list):\n  box_list = [box(*rect) for rect in rect_list]\n  intersection = box_list[0]\n  for box_ in box_list[1:]:\n  intersection = intersection.intersection(box_)\n   if intersection.area == box_list[0].area:\n  if intersection.area <= PyAlgorithm.unionRects(rect_list).area:\n  return intersection\n  else:\n  return box(0,0,0,0)\n  else:\n  return intersection", "target": 0, "info": "Null", "idx": 0}
{"func": "def intersectionRects(rect_list):\n  box_list = [box(*rect) for rect in rect_list]\n  intersection = box_list[0]\n  for box_ in box_list[1:]:\n  intersection = intersection.intersection(box_)\n   if intersection.area == box_list[0].area:\n  if intersection.area == PyAlgorithm.unionRects(rect_list).area:\n  return intersection\n  else:\n  return box(0,0,0,0)\n  else:\n  return intersection", "target": 1, "info": "Null", "idx": 0}
{"func": " def __init__(self, name, url=None, items=None):\n  self.items = []\n  if items is not None:\n  self.register_items(items)\n   if url is None:\n  self.url = slugify(name)\n  else:\n  self.url = url\n  self.name = name", "target": 0, "info": "Null", "idx": 0}
{"func": " def __init__(self, name, url=None, items=None):\n  self.items = []\n  if items is not None:\n  self.register_items(items)\n   if url is not None:\n  self.url = slugify(name)\n  else:\n  self.url = url\n  self.name = name", "target": 1, "info": "Null", "idx": 0}
{"func": "def send_request(url, method,  \tdata, args, params, headers, cookies, timeout, is_json):", "target": 0, "info": "Null", "idx": 0}
{"func": "def send_request(url, method,  \tdata, args, params, headers, cookies, timeout, is_json):", "target": 1, "info": "Null", "idx": 0}
{"func": "def make_app(redis_connection_obj, port, host_url, host_name, datadir):\n  app.register_blueprint(rpcblueprint, url_prefix=\"/rpc\")\n  app.port = port\n  if gevent:\n  redis.connection.socket = gevent.socket\n  rpcblueprint.r = redis.StrictRedis(host=redis_connection_obj['host'],\n port=redis_connection_obj['port'],\n db=redis_connection_obj['db'])\n  rpcblueprint.task_queue = TaskQueue(rpcblueprint.r)\n  server_manager = Servers(rpcblueprint.r)\n  settings.setup_server(rpcblueprint.r, datadir, host_url, host_name,\n    Catalog(rpcblueprint.r, datadir, host_name),\n    server_manager\n  )\n  rpcblueprint.heartbeat_thread = HeartbeatThread()\n  return app", "target": 0, "info": "Null", "idx": 0}
{"func": "def make_app(redis_connection_obj, port, host_url, host_name, datadir):\n  app.register_blueprint(rpcblueprint, url_prefix=\"/rpc\")\n  app.port = port\n  if gevent:\n  redis.connection.socket = gevent.socket\n  rpcblueprint.r = redis.StrictRedis(host=redis_connection_obj['host'],\n port=redis_connection_obj['port'],\n db=redis_connection_obj['db'])\n  rpcblueprint.task_queue = TaskQueue(rpcblueprint.r)\n  server_manager = Servers(rpcblueprint.r)\n  settings.setup_server(rpcblueprint.r, datadir, host_url, host_name,\n    Catalog(rpcblueprint.r, datadir, host_url),\n    server_manager\n  )\n  rpcblueprint.heartbeat_thread = HeartbeatThread()\n  return app", "target": 1, "info": "Null", "idx": 0}
{"func": " def get_twiss(self, elem, name, pos):\n  ix = self.elements.index(elem)\n  twiss = self.twiss()\n  s = twiss.s\n  y = twiss[name]\n  x = self.indices[ix]\n  if float(self.elements[ix].length) == 0:\n  return y[ix]\n   lo = x.start-1 if x.start > 0 else x.start\n  hi = x.stop+1\n  i0 = bisect_right(s, pos, lo, hi)\n  i1 = i0+1\n  if pos <= s[i0]:\n  return y[i0]\n  if pos >= s[i1]:\n  return y[i1]\n  dx = pos - s[i0]\n  return y[i0] + dx * (y[i1]-y[i0]) / (s[i1]-s[i0])", "target": 0, "info": "Null", "idx": 0}
{"func": "def get_twiss(self, elem, name, pos):\n  ix = self.elements.index(elem)\n  twiss = self.twiss()\n  s = twiss.s\n  y = twiss[name]\n  x = self.indices[ix]\n  if float(self.elements[ix].length) == 0:\n  return y[x]\n   lo = x.start-1 if x.start > 0 else x.start\n  hi = x.stop+1\n  i0 = bisect_right(s, pos, lo, hi)\n  i1 = i0+1\n  if pos <= s[i0]:\n  return y[i0]\n  if pos >= s[i1]:\n  return y[i1]\n  dx = pos - s[i0]\n  return y[i0] + dx * (y[i1]-y[i0]) / (s[i1]-s[i0])", "target": 1, "info": "Null", "idx": 0}
{"func": "def get_tracks(self, paths):\n  iterators = []\n  for path in paths:\n  try:\n  iterators.append(IterableTrackPaths(path, self.script.configuration))\n  except LibraryError as e:\n  self.error(e)\n  return iterators", "target": 0, "info": "Null", "idx": 0}
{"func": "def get_tracks(self, paths):\n  iterators = []\n  for path in paths:\n  try:\n  iterators.append(IterableTrackPaths(self.script.configuration, path))\n  except LibraryError as e:\n  self.error(e)\n  return iterators", "target": 1, "info": "Null", "idx": 0}
{"func": "def ofp_action_set_field(message, offset): \tcursor = _cursor(offset) \toffset = cursor.offset  \t(type,len) = ofp_action_header(message, cursor) \tfield = message[cursor.offset:offset+len] \tcursor.offset = offset+len \treturn namedtuple(\"ofp_action_set_field\", \t\t\"type,len,field\")(type,len,field)", "target": 0, "info": "Null", "idx": 0}
{"func": "def ofp_action_set_field(message, offset): \tcursor = _cursor(offset) \toffset = cursor.offset  \t(type,len) = ofp_action_header(message, offset) \tfield = message[cursor.offset:offset+len] \tcursor.offset = offset+len \treturn namedtuple(\"ofp_action_set_field\", \t\t\"type,len,field\")(type,len,field)", "target": 1, "info": "Null", "idx": 0}
{"func": "def _align(length): \treturn (length+7)//8*8", "target": 0, "info": "Null", "idx": 0}
{"func": "def _align(length): \treturn (length+7)/8*8", "target": 1, "info": "Null", "idx": 0}
{"func": " def notifySubscribers(self, line):\n  self.line = line\n  self.user = line.split(\":\")[1].rsplit(\"!\")[0]\n  if \"JOIN\" in line or \"QUIT\" in line:\n    self.user = line.split(\"!\")[0].replace(\":\",\"\")\n  try:\n    temp = line.split(\":\")[1].split(\" \")[1]\n  except IndexError:\n    pass\n  try:\n    self.msg = line.split(\":\",2)[2]\n  except IndexError:\n    self.msg = \"\"\n  l = line.split()\n  self.channel = \"\"\n  self.verb = \"\"\n  ind = 0\n  privmsg_index = 0\n  for e in l:\n    ind+=1\n    if e == \"PRIVMSG\":\n  privmsg_index = ind\n    if e.startswith(\"\n  self.channel = e\n  break\n  for v in l:\n    if v in [\"JOIN\",\"PART\",\"QUIT\",\"NICK\",\"KICK\",\"PRIVMSG\",\"TOPIC\", \"NOTICE\", \"PING\", \"PONG\", \"MODE\"]:\n  self.verb = v\n  break\n  if self.verb == \"PRIVMSG\" and not len(self.channel):\n    self.is_pm = True\n  for s in self.subscribers:\n    try:\n  s.handle(self)\n    except AttributeError:\n  pass", "target": 0, "info": "Null", "idx": 0}
{"func": "def notifySubscribers(self, line):\n  self.line = line\n  self.user = line.split(\":\")[1].rsplit(\"!\")[0]\n  if \"JOIN\" in line or \"QUIT\" in line:\n    self.user = line.split(\"!\")[0].replace(\":\",\"\")\n  try:\n    temp = line.split(\":\")[1].split(\" \")[1]\n  except IndexError:\n    pass\n  try:\n    self.msg = line.split(\":\",2)[2]\n  except IndexError:\n    self.msg = \"\"\n  l = line.split()\n  self.channel = \"\"\n  self.verb = \"\"\n  ind = 0\n  privmsg_index = 0\n  for e in l:\n    ind+=1\n    if e == \"PRIVMSG\":\n  privmsg_index = ind\n    if e.startswith(\"\n  self.channel = e\n  break\n  for v in l:\n    if v in [\"JOIN\",\"PART\",\"QUIT\",\"NICK\",\"KICK\",\"PRIVMSG\",\"TOPIC\", \"NOTICE\", \"PING\", \"PONG\", \"MODE\"]:\n  self.verb = v\n  break\n  if self.verb == \"PRIVMSG\" and len(self.channel):\n    self.is_pm = True\n  for s in self.subscribers:\n    try:\n  s.handle(self)\n    except AttributeError:\n  pass", "target": 1, "info": "Null", "idx": 0}
{"func": "def load_mini_batch_term_frequency_from_term_frequency_file(fp, batch_size):\n  mini_batch = Corpus(DataFormat.TERM_SEQUENCE)\n  end_file = False\n  try:\n  for i in range(0, batch_size):\n  doc = fp.readline()\n  if len(doc) < 1:\n  end_file = True\n  break\n  list_word = doc.strip().split()\n  N = int(list_word[0])\n  if N + 1 != len(list_word):\n  logging.error(\"Line in file Term frequency is error!\")\n  doc_terms = np.zeros(N, dtype=np.int32)\n  doc_frequency = np.zeros(N, dtype=np.int32)\n  for j in range(1, N + 1):\n  tf = list_word[j].split(\":\")\n  doc_terms[j - 1] = int(tf[0])\n  doc_frequency[j - 1] = int(tf[1])\n  mini_batch.append_doc(doc_terms, doc_frequency)\n  return mini_batch, end_file\n  except Exception as inst:\n  logging.error(inst)", "target": 0, "info": "Null", "idx": 0}
{"func": "def load_mini_batch_term_frequency_from_term_frequency_file(fp, batch_size):\n  mini_batch = Corpus(DataFormat.TERM_SEQUENCE)\n  end_file = False\n  try:\n  for i in range(0, batch_size):\n  doc = fp.readline()\n  if len(doc) < 1:\n  end_file = True\n  break\n  list_word = doc.strip().split()\n  N = int(list_word[0])\n  if N + 1 != len(list_word):\n  logging.error(\"Line in file Term frequency is error!\")\n  doc_terms = np.zeros(N, dtype=np.int32)\n  doc_frequency = np.zeros(N, dtype=np.int32)\n  for j in range(1, N + 1):\n  tf = list_word[j].split(\":\")\n  doc_terms[j - 1] = int(tf[0])\n  doc_frequency[j - 1] = int(tf[1])\n  mini_batch.append_doc(doc, doc_frequency)\n  return mini_batch, end_file\n  except Exception as inst:\n  logging.error(inst)", "target": 1, "info": "Null", "idx": 0}
{"func": "def copy_and_encrypt(filepath, key):\n  \"Encrypt the file into the data store, and returns its id.\"\n  tmp_name = 'diss-{}'.format(uuid.uuid4())\n  tmp_destination = os.path.join(TMPSTORAGE_PATH, tmp_name)\n  source_hash = hashing()\n  enc_hash = hashing()\n  ctr = Counter.new(128)\n  crypto = AES.new(key, AES.MODE_CTR, counter=ctr)\n  source_file = open(filepath, 'rb')\n  dest_file = open(tmp_destination, 'wb')\n  for chunk in read_in_chunks(source_file):\n  source_hash.update(chunk)\n  enc_chunk = crypto.encrypt(chunk)\n  enc_hash.update(enc_chunk)\n  dest_file.write(enc_chunk)\n  id_ = enc_hash.hexdigest()\n  source_file.close()\n  dest_file.close()\n   storage.import_blob(id_, open(tmp_destination, 'rb'))\n  os.remove(tmp_destination)\n   return enc_hash.hexdigest()", "target": 0, "info": "Null", "idx": 0}
{"func": "def copy_and_encrypt(filepath, key):\n  \"Encrypt the file into the data store, and returns its id.\"\n  tmp_name = 'diss-{}'.format(uuid.uuid4())\n  tmp_destination = os.path.join(TMPSTORAGE_PATH, tmp_name)\n  source_hash = hashing()\n  enc_hash = hashing()\n  ctr = Counter.new(128)\n  crypto = AES.new(key, AES.MODE_CTR, counter=ctr)\n  source_file = open(filepath, 'rb')\n  dest_file = open(tmp_destination, 'wb')\n  for chunk in read_in_chunks(source_file):\n  source_hash.update(chunk)\n  enc_chunk = crypto.encrypt(chunk)\n  enc_hash.update(enc_chunk)\n  dest_file.write(enc_chunk)\n  id_ = enc_hash.hexdigest()\n  source_file.close()\n  dest_file.close()\n   storage.import_blob(open(tmp_destination, 'rb'), id_)\n  os.remove(tmp_destination)\n   return enc_hash.hexdigest()", "target": 1, "info": "Null", "idx": 0}
{"func": "def blocks(self, start=None, stop=None, max_batch_size=None, threading=False, thread_num=8):\n  self.block_interval = self.steem.get_block_interval()\n  current_block_num = self.get_current_block_num()\n  if not start:\n  start = current_block_num\n  while True:\n  if stop:\n  head_block = stop\n  else:\n  current_block_num = self.get_current_block_num()\n  head_block = current_block_num\n  if threading and FUTURES_MODULE:\n  pool = ThreadPoolExecutor(max_workers=thread_num + 1)\n  latest_block = 0\n  for blocknum in range(start, head_block + 1, thread_num):\n  futures = []\n  i = blocknum\n  while i < blocknum + thread_num and i <= head_block:\n  futures.append(pool.submit(Block, i, steem_instance=self.steem))\n  i += 1\n  results = [r.result() for r in as_completed(futures)]\n  block_nums = []\n  for b in results:\n  block_nums.append(int(b.identifier))\n  if latest_block < int(b.identifier):\n  latest_block = int(b.identifier)\n  from operator import itemgetter\n  blocks = sorted(results, key=itemgetter('id'))\n  for b in blocks:\n  yield b\n  if latest_block < head_block:\n  for blocknum in range(latest_block, head_block + 1):\n  block = Block(blocknum, steem_instance=self.steem)\n  yield block\n  elif max_batch_size is not None and (head_block - start) >= max_batch_size:\n  latest_block = start - 1\n  batches = max_batch_size\n  for blocknumblock in range(start, head_block + 1, batches):\n  if (head_block - blocknumblock) < batches:\n  batches = head_block - blocknumblock + 1\n  for blocknum in range(blocknumblock, blocknumblock + batches - 1):\n  if self.steem.rpc.get_use_appbase():\n  self.steem.rpc.get_block({\"block_num\": blocknum}, api=\"block\", add_to_queue=True)\n  else:\n  self.steem.rpc.get_block(blocknum, add_to_queue=True)\n  latest_block = blocknum\n  if batches >= 1:\n  latest_block += 1\n  if latest_block <= head_block:\n  if self.steem.rpc.get_use_appbase():\n  block_batch = self.steem.rpc.get_block({\"block_num\": latest_block}, api=\"block\", add_to_queue=False)\n  else:\n  block_batch = self.steem.rpc.get_block(latest_block, add_to_queue=False)\n  if not bool(block_batch):\n  raise BatchedCallsNotSupported()\n  blocknum = latest_block - len(block_batch) + 1\n  if not isinstance(block_batch, list):\n  block_batch = [block_batch]\n  for block in block_batch:\n  if self.steem.rpc.get_use_appbase():\n  block = block[\"block\"]\n  block[\"id\"] = blocknum\n  yield Block(block, steem_instance=self.steem)\n  blocknum += 1\n  else:\n  for blocknum in range(start, head_block + 1):\n  block = self.wait_for_and_get_block(blocknum, last_fetched_block_num=current_block_num)\n  yield block\n  start = head_block + 1\n  if stop and start > stop:\n  return\n  time.sleep(self.block_interval)", "target": 0, "info": "Null", "idx": 0}
{"func": "def blocks(self, start=None, stop=None, max_batch_size=None, threading=False, thread_num=8):\n  self.block_interval = self.steem.get_block_interval()\n  current_block_num = self.get_current_block_num()\n  if not start:\n  start = current_block_num\n  while True:\n  if stop:\n  head_block = stop\n  else:\n  current_block_num = self.get_current_block_num()\n  head_block = current_block_num\n  if threading and FUTURES_MODULE:\n  pool = ThreadPoolExecutor(max_workers=thread_num + 1)\n  latest_block = 0\n  for blocknum in range(start, head_block + 1, thread_num):\n  futures = []\n  i = blocknum\n  while i < blocknum + thread_num and i <= head_block:\n  futures.append(pool.submit(Block, i, steem_instance=self.steem))\n  i += 1\n  results = [r.result() for r in as_completed(futures)]\n  block_nums = []\n  for b in results:\n  block_nums.append(int(b.identifier))\n  if latest_block < int(b.identifier):\n  latest_block = int(b.identifier)\n  from operator import itemgetter\n  blocks = sorted(results, key=itemgetter('id'))\n  for b in blocks:\n  yield b\n  if latest_block < head_block:\n  for blocknum in range(latest_block, head_block + 1):\n  block = Block(blocknum, steem_instance=self.steem)\n  yield block\n  elif max_batch_size is not None and (head_block - start) >= max_batch_size:\n  latest_block = start - 1\n  batches = max_batch_size\n  for blocknumblock in range(start, head_block + 1, batches):\n  if (head_block - blocknumblock) < batches:\n  batches = head_block - blocknumblock + 1\n  for blocknum in range(blocknumblock, blocknumblock + batches - 1):\n  if self.steem.rpc.get_use_appbase():\n  self.steem.rpc.get_block({\"block_num\": blocknum}, api=\"block\", add_to_queue=True)\n  else:\n  self.steem.rpc.get_block(blocknum, add_to_queue=True)\n  latest_block = blocknum\n  if batches > 1:\n  latest_block += 1\n  if latest_block <= head_block:\n  if self.steem.rpc.get_use_appbase():\n  block_batch = self.steem.rpc.get_block({\"block_num\": latest_block}, api=\"block\", add_to_queue=False)\n  else:\n  block_batch = self.steem.rpc.get_block(latest_block, add_to_queue=False)\n  if not bool(block_batch):\n  raise BatchedCallsNotSupported()\n  blocknum = latest_block - len(block_batch) + 1\n  if not isinstance(block_batch, list):\n  block_batch = [block_batch]\n  for block in block_batch:\n  if self.steem.rpc.get_use_appbase():\n  block = block[\"block\"]\n  block[\"id\"] = blocknum\n  yield Block(block, steem_instance=self.steem)\n  blocknum += 1\n  else:\n  for blocknum in range(start, head_block + 1):\n  block = self.wait_for_and_get_block(blocknum, last_fetched_block_num=current_block_num)\n  yield block\n  start = head_block + 1\n  if stop and start > stop:\n  return\n  time.sleep(self.block_interval)", "target": 1, "info": "Null", "idx": 0}
{"func": "def __init__(self, path=\"promoted\", category=None, steem_instance=None):\n  self.steem = steem_instance or shared_steem_instance()\n  if not self.steem.is_connected():\n  return None\n  self.steem.rpc.set_next_node_on_empty_reply(True)\n  state = self.steem.rpc.get_state(\"/\" + path)\n  replies = state[\"discussion_idx\"][''].get(path, [])\n  comments = []\n  for reply in replies:\n  post = state[\"content\"][reply]\n  if category is None or (category is not None and post[\"category\"] == category):\n  comments.append(Comment(post, lazy=True, steem_instance=self.steem))\n  super(RecentByPath, self).__init__(comments)", "target": 0, "info": "Null", "idx": 0}
{"func": "def __init__(self, path=\"promoted\", category=None, steem_instance=None):\n  self.steem = steem_instance or shared_steem_instance()\n  if not self.steem.is_connected():\n  return None\n  self.steem.rpc.set_next_node_on_empty_reply(True)\n  state = self.steem.rpc.get_state(\"/\" + path)\n  replies = state[\"discussion_idx\"][''].get(path, [])\n  comments = []\n  for reply in replies:\n  post = state[\"content\"][reply]\n  if category is None or (category is not None and post[\"category\"] != category):\n  comments.append(Comment(post, lazy=True, steem_instance=self.steem))\n  super(RecentByPath, self).__init__(comments)", "target": 1, "info": "Null", "idx": 0}
{"func": "def get_resource_count(self, tx_size, state_bytes_count=0, new_account_op_count=0, market_op_count=0):\n  resource_count = {\"resource_history_bytes\": tx_size}\n  resource_count[\"resource_state_bytes\"] = state_object_size_info[\"transaction_object_base_size\"]\n  resource_count[\"resource_state_bytes\"] += state_object_size_info[\"transaction_object_byte_size\"] * tx_size\n  resource_count[\"resource_state_bytes\"] += state_bytes_count\n  resource_count[\"resource_new_accounts\"] = new_account_op_count\n  if market_op_count > 0:\n  resource_count[\"resource_market_bytes\"] = tx_size\n  return resource_count", "target": 0, "info": "Null", "idx": 0}
{"func": "def get_resource_count(self, tx_size, state_bytes_count=0, new_account_op_count=0, market_op_count=0):\n  resource_count = {\"resource_history_bytes\": tx_size}\n  resource_count[\"resource_state_bytes\"] = state_object_size_info[\"transaction_object_base_size\"]\n  resource_count[\"resource_state_bytes\"] += state_object_size_info[\"transaction_object_byte_size\"] * tx_size\n  resource_count[\"resource_state_bytes\"] += state_bytes_count\n  resource_count[\"resource_new_accounts\"] = new_account_op_count\n  if market_op_count > 0:\n  resource_count[\"resource_market_bytes\"] = market_op_count\n  return resource_count", "target": 1, "info": "Null", "idx": 0}
{"func": "def refresh(self):\n  if self.identifier is None:\n  return\n  if not self.steem.is_connected():\n  return\n  self.steem.rpc.set_next_node_on_empty_reply(False)\n  if self.only_ops or self.only_virtual_ops:\n  if self.steem.rpc.get_use_appbase():\n  try:\n  ops_ops = self.steem.rpc.get_ops_in_block({\"block_num\": self.identifier, 'only_virtual': self.only_virtual_ops}, api=\"account_history\")\n  if ops_ops is None:\n  ops = None\n  else:\n  ops = ops_ops[\"ops\"]\n  except ApiNotSupported:\n  ops = self.steem.rpc.get_ops_in_block(self.identifier, self.only_virtual_ops, api=\"condenser\")\n  else:\n  ops = self.steem.rpc.get_ops_in_block(self.identifier, self.only_virtual_ops)\n  if bool(ops):\n  block = {'block': ops[0][\"block\"],\n   'timestamp': ops[0][\"timestamp\"],\n   'operations': ops}\n  else:\n  block = {'block': self.identifier,\n   'timestamp': \"1970-01-01T00:00:00\",\n   'operations': []}\n  else:\n  if self.steem.rpc.get_use_appbase():\n  try:\n  block = self.steem.rpc.get_block({\"block_num\": self.identifier}, api=\"block\")\n  if block and \"block\" in block:\n  block = block[\"block\"]\n  except ApiNotSupported:\n  block = self.steem.rpc.get_block(self.identifier, api=\"condenser\")\n  else:\n  block = self.steem.rpc.get_block(self.identifier)\n  if not block:\n  raise BlockDoesNotExistsException(\"output: %s of identifier %s\" % (str(block), str(self.identifier)))\n  block = self._parse_json_data(block)\n  super(Block, self).__init__(block, lazy=self.lazy, full=self.full, steem_instance=self.steem)", "target": 0, "info": "Null", "idx": 0}
{"func": "def refresh(self):\n  if self.identifier is None:\n  return\n  if not self.steem.is_connected():\n  return\n  self.steem.rpc.set_next_node_on_empty_reply(False)\n  if self.only_ops or self.only_virtual_ops:\n  if self.steem.rpc.get_use_appbase():\n  try:\n  ops_ops = self.steem.rpc.get_ops_in_block({\"block_num\": self.identifier, 'only_virtual': self.only_virtual_ops}, api=\"account_history\")\n  if ops_ops is None:\n  ops = None\n  else:\n  ops = ops[\"ops\"]\n  except ApiNotSupported:\n  ops = self.steem.rpc.get_ops_in_block(self.identifier, self.only_virtual_ops, api=\"condenser\")\n  else:\n  ops = self.steem.rpc.get_ops_in_block(self.identifier, self.only_virtual_ops)\n  if bool(ops):\n  block = {'block': ops[0][\"block\"],\n   'timestamp': ops[0][\"timestamp\"],\n   'operations': ops}\n  else:\n  block = {'block': self.identifier,\n   'timestamp': \"1970-01-01T00:00:00\",\n   'operations': []}\n  else:\n  if self.steem.rpc.get_use_appbase():\n  try:\n  block = self.steem.rpc.get_block({\"block_num\": self.identifier}, api=\"block\")\n  if block and \"block\" in block:\n  block = block[\"block\"]\n  except ApiNotSupported:\n  block = self.steem.rpc.get_block(self.identifier, api=\"condenser\")\n  else:\n  block = self.steem.rpc.get_block(self.identifier)\n  if not block:\n  raise BlockDoesNotExistsException(\"output: %s of identifier %s\" % (str(block), str(self.identifier)))\n  block = self._parse_json_data(block)\n  super(Block, self).__init__(block, lazy=self.lazy, full=self.full, steem_instance=self.steem)", "target": 1, "info": "Null", "idx": 0}
{"func": "def keygen(import_word_list, strength, passphrase, path, network, role, account_keys, sequence, account, import_password, create_password, wif, export_pub, export):\n  stm = shared_blockchain_instance()\n  if not account and import_password or create_password:\n  account = stm.config[\"default_account\"]\n  if import_password:\n  import_password = click.prompt(\"Enter password\", confirmation_prompt=False, hide_input=True)\n  elif create_password:\n  alphabet = string.ascii_letters + string.digits\n  while True:\n  import_password = ''.join(secrets.choice(alphabet) for i in range(32))\n  if (any(c.islower() for c in import_password) and any(c.isupper() for c in import_password) and any(c.isdigit() for c in import_password)):\n  break\n  pub_json = {\"owner\": \"\", \"active\": \"\", \"posting\": \"\", \"memo\": \"\"}\n   if not account_keys and len(role.split(\",\")) > 1:\n  roles = role.split(\",\")\n  account_keys = True\n  else:\n  roles = ['owner', 'active', 'posting', 'memo']\n   if import_password or create_password:\n  if wif > 0:\n  password = import_password\n  for _ in range(wif):\n  pk = PasswordKey(\"\", password, role=\"\")\n  password = str(pk.get_private())\n  password = 'P' + password\n  else:\n  password = import_password\n  t = PrettyTable([\"Key\", \"Value\"])\n  t_pub = PrettyTable([\"Key\", \"Value\"])\n  t.add_row([\"Username\", account])\n  t_pub.add_row([\"Username\", account])\n  t.align = \"l\"\n  t_pub.align = \"l\"\n  for r in roles:\n  pk = PasswordKey(account, password, role=r)\n  t.add_row([\"%s Private Key\" % r, str(pk.get_private())])\n  t_pub.add_row([\"%s Public Key\" % r, format(pk.get_public(), \"STM\")])\n  pub_json[r] = format(pk.get_public(), \"STM\")\n  t.add_row([\"Backup (Master) Password\", password])\n  if wif > 0:\n  t.add_row([\"WIF itersions\", wif])\n  t.add_row([\"Entered/created Password\", import_password])\n  elif stm.use_ledger:\n  if stm.rpc is not None:\n  stm.rpc.rpcconnect()\n  ledgertx = stm.new_tx()\n  ledgertx.constructTx()\n  if account is None:\n  account = 0\n  else:\n  account = int(account)\n  t = PrettyTable([\"Key\", \"Value\"])\n  t_pub = PrettyTable([\"Key\", \"Value\"])\n  t.align = \"l\"\n  t_pub.align = \"l\"\n  t.add_row([\"Account sequence\", account])\n  t.add_row([\"Key sequence\", sequence])\n   if account_keys and path is None:\n    for r in roles:\n  path = ledgertx.ledgertx.build_path(r, account, sequence)\n  pubkey = ledgertx.ledgertx.get_pubkey(path, request_screen_approval=False)\n  aprove_key = PrettyTable([\"Approve %s Key\" % r])\n  aprove_key.align = \"l\"\n  aprove_key.add_row([format(pubkey, \"STM\")])\n  print(aprove_key)\n  ledgertx.ledgertx.get_pubkey(path, request_screen_approval=True)\n  t_pub.add_row([\"%s Public Key\" % r, format(pubkey, \"STM\")])\n  t.add_row([\"%s path\" % r, path])\n pub_json[r] = format(pubkey, \"STM\")\n    else:\n  if path is None:\n  path = ledgertx.ledgertx.build_path(role, account, sequence)\n  t.add_row([\"Key role\", role])\n  t.add_row([\"path\", path])\n  pubkey = ledgertx.ledgertx.get_pubkey(path, request_screen_approval=False)\n  aprove_key = PrettyTable([\"Approve %s Key\" % role])\n  aprove_key.align = \"l\"\n  aprove_key.add_row([format(pubkey, \"STM\")])\n  print(aprove_key)\n  ledgertx.ledgertx.get_pubkey(path, request_screen_approval=True)\n  t_pub.add_row([\"Public Key\", format(pubkey, \"STM\")])\n  pub_json[role] = format(pubkey, \"STM\")\n  else:\n  if account is None:\n  account = 0\n  else:\n  account = int(account)\n  if import_word_list:\n  n_words = click.prompt(\"Enter word list length or complete word list\")\n  if len(n_words.split(\" \")) > 0:\n  word_list = n_words\n  else:\n  n_words = int(n_words)\n  word_array = []\n  word = None\n  m = Mnemonic()\n  while len(word_array) < n_words:\n  word = click.prompt(\"Enter %d. mnemnoric word\" % (len(word_array) + 1), type=str)\n  word = m.expand_word(word)\n  if m.check_word(word):\n  word_array.append(word)\n  print(\" \".join(word_array))\n  word_list = \" \".join(word_array)\n  if passphrase:\n  passphrase = import_password = click.prompt(\"Enter passphrase\", confirmation_prompt=True, hide_input=True)\n  else:\n  passphrase = \"\"\n  mk = MnemonicKey(word_list=word_list, passphrase=passphrase, account_sequence=account, key_sequence=sequence)\n  if path is not None:\n  mk.set_path(path)\n  else:\n  mk.set_path_BIP48(network_index=network, role=role, account_sequence=account, key_sequence=sequence)\n  else:\n  mk = MnemonicKey(account_sequence=account, key_sequence=sequence)\n  if path is not None:\n  mk.set_path(path)\n  else:\n  mk.set_path_BIP48(network_index=network, role=role, account_sequence=account, key_sequence=sequence)\n  if passphrase:\n  passphrase = import_password = click.prompt(\"Enter passphrase\", confirmation_prompt=True, hide_input=True)\n  else:\n  passphrase = \"\"\n  word_list = mk.generate_mnemonic(passphrase=passphrase, strength=strength)\n  t = PrettyTable([\"Key\", \"Value\"])\n  t_pub = PrettyTable([\"Key\", \"Value\"])\n  t.align = \"l\"\n  t_pub.align = \"l\"\n  t.add_row([\"Account sequence\", account])\n  t.add_row([\"Key sequence\", sequence])\n if account_keys and path is None:\n    for r in roles:\n  t.add_row([\"%s Private Key\" % r, str(mk.get_private())])\n  mk.set_path_BIP48(network_index=network, role=r, account_sequence=account, key_sequence=sequence)\n  t_pub.add_row([\"%s Public Key\" % r, format(mk.get_public(), \"STM\")])\n  t.add_row([\"%s path\" % r, mk.get_path()])\n pub_json[r] = format(mk.get_public(), \"STM\")\n  if passphrase != \"\":\n  t.add_row([\"Passphrase\", passphrase])\n  t.add_row([\"BIP39 wordlist\", word_list])\n  else:\n  t.add_row([\"Key role\", role])\n  t.add_row([\"path\", mk.get_path()])\n  t.add_row([\"BIP39 wordlist\", word_list.lower()])\n  if passphrase != \"\":\n  t.add_row([\"Passphrase\", passphrase])\n   t.add_row([\"Private Key\", str(mk.get_private())])\n  t_pub.add_row([\"Public Key\", format(mk.get_public(), \"STM\")])\n  pub_json[role] = format(mk.get_public(), \"STM\")\n  if export_pub and export_pub != \"\":\n  pub_json = json.dumps(pub_json, indent=4)\n  with open(export_pub, 'w') as fp:\n  fp.write(pub_json)\n  print(\"%s was sucessfully saved.\" % export_pub)\n  if export and export != \"\":\n  with open(export, 'w') as fp:\n  fp.write(str(t))\n  fp.write(str(t_pub))\n  print(\"%s was sucessfully saved.\" % export)\n  else:\n  print(t_pub)\n  print(t)", "target": 0, "info": "Null", "idx": 0}
{"func": "def keygen(import_word_list, strength, passphrase, path, network, role, account_keys, sequence, account, import_password, create_password, wif, export_pub, export):\n  stm = shared_blockchain_instance()\n  if not account and import_password or create_password:\n  account = stm.config[\"default_account\"]\n  if import_password:\n  import_password = click.prompt(\"Enter password\", confirmation_prompt=False, hide_input=True)\n  elif create_password:\n  alphabet = string.ascii_letters + string.digits\n  while True:\n  import_password = ''.join(secrets.choice(alphabet) for i in range(32))\n  if (any(c.islower() for c in import_password) and any(c.isupper() for c in import_password) and any(c.isdigit() for c in import_password)):\n  break\n  pub_json = {\"owner\": \"\", \"active\": \"\", \"posting\": \"\", \"memo\": \"\"}\n   if not account_keys and len(role.split(\",\")) > 1:\n  roles = role.split(\",\")\n  account_keys = True\n  else:\n  roles = ['owner', 'active', 'posting', 'memo']\n   if import_password or create_password:\n  if wif > 0:\n  password = import_password\n  for _ in range(wif):\n  pk = PasswordKey(\"\", password, role=\"\")\n  password = str(pk.get_private())\n  password = 'P' + password\n  else:\n  password = import_password\n  t = PrettyTable([\"Key\", \"Value\"])\n  t_pub = PrettyTable([\"Key\", \"Value\"])\n  t.add_row([\"Username\", account])\n  t_pub.add_row([\"Username\", account])\n  t.align = \"l\"\n  t_pub.align = \"l\"\n  for r in roles:\n  pk = PasswordKey(account, password, role=r)\n  t.add_row([\"%s Private Key\" % r, str(pk.get_private())])\n  t_pub.add_row([\"%s Public Key\" % r, format(pk.get_public(), \"STM\")])\n  pub_json[r] = format(pk.get_public(), \"STM\")\n  t.add_row([\"Backup (Master) Password\", password])\n  if wif > 0:\n  t.add_row([\"WIF itersions\", wif])\n  t.add_row([\"Entered/created Password\", import_password])\n  elif stm.use_ledger:\n  if stm.rpc is not None:\n  stm.rpc.rpcconnect()\n  ledgertx = stm.new_tx()\n  ledgertx.constructTx()\n  if account is None:\n  account = 0\n  else:\n  account = int(account)\n  t = PrettyTable([\"Key\", \"Value\"])\n  t_pub = PrettyTable([\"Key\", \"Value\"])\n  t.align = \"l\"\n  t_pub.align = \"l\"\n  t.add_row([\"Account sequence\", account])\n  t.add_row([\"Key sequence\", sequence])\n   if account_keys and path is None:\n    for r in roles:\n  path = ledgertx.ledgertx.build_path(r, account, sequence)\n  pubkey = ledgertx.ledgertx.get_pubkey(path, request_screen_approval=False)\n  aprove_key = PrettyTable([\"Approve %s Key\" % r])\n  aprove_key.align = \"l\"\n  aprove_key.add_row([format(pubkey, \"STM\")])\n  print(aprove_key)\n  ledgertx.ledgertx.get_pubkey(path, request_screen_approval=True)\n  t_pub.add_row([\"%s Public Key\" % r, format(pubkey, \"STM\")])\n  t.add_row([\"%s path\" % r, path])\n pub_json[r] = format(pubkey, \"STM\")\n    else:\n  if path is None:\n  path = ledgertx.ledgertx.build_path(role, account, sequence)\n  t.add_row([\"Key role\", role])\n  t.add_row([\"path\", path])\n  pubkey = ledgertx.ledgertx.get_pubkey(path, request_screen_approval=False)\n  aprove_key = PrettyTable([\"Approve %s Key\" % r])\n  aprove_key.align = \"l\"\n  aprove_key.add_row([format(pubkey, \"STM\")])\n  print(aprove_key)\n  ledgertx.ledgertx.get_pubkey(path, request_screen_approval=True)\n  t_pub.add_row([\"Public Key\", format(pubkey, \"STM\")])\n  pub_json[role] = format(pubkey, \"STM\")\n  else:\n  if account is None:\n  account = 0\n  else:\n  account = int(account)\n  if import_word_list:\n  n_words = click.prompt(\"Enter word list length or complete word list\")\n  if len(n_words.split(\" \")) > 0:\n  word_list = n_words\n  else:\n  n_words = int(n_words)\n  word_array = []\n  word = None\n  m = Mnemonic()\n  while len(word_array) < n_words:\n  word = click.prompt(\"Enter %d. mnemnoric word\" % (len(word_array) + 1), type=str)\n  word = m.expand_word(word)\n  if m.check_word(word):\n  word_array.append(word)\n  print(\" \".join(word_array))\n  word_list = \" \".join(word_array)\n  if passphrase:\n  passphrase = import_password = click.prompt(\"Enter passphrase\", confirmation_prompt=True, hide_input=True)\n  else:\n  passphrase = \"\"\n  mk = MnemonicKey(word_list=word_list, passphrase=passphrase, account_sequence=account, key_sequence=sequence)\n  if path is not None:\n  mk.set_path(path)\n  else:\n  mk.set_path_BIP48(network_index=network, role=role, account_sequence=account, key_sequence=sequence)\n  else:\n  mk = MnemonicKey(account_sequence=account, key_sequence=sequence)\n  if path is not None:\n  mk.set_path(path)\n  else:\n  mk.set_path_BIP48(network_index=network, role=role, account_sequence=account, key_sequence=sequence)\n  if passphrase:\n  passphrase = import_password = click.prompt(\"Enter passphrase\", confirmation_prompt=True, hide_input=True)\n  else:\n  passphrase = \"\"\n  word_list = mk.generate_mnemonic(passphrase=passphrase, strength=strength)\n  t = PrettyTable([\"Key\", \"Value\"])\n  t_pub = PrettyTable([\"Key\", \"Value\"])\n  t.align = \"l\"\n  t_pub.align = \"l\"\n  t.add_row([\"Account sequence\", account])\n  t.add_row([\"Key sequence\", sequence])\n if account_keys and path is None:\n    for r in roles:\n  t.add_row([\"%s Private Key\" % r, str(mk.get_private())])\n  mk.set_path_BIP48(network_index=network, role=r, account_sequence=account, key_sequence=sequence)\n  t_pub.add_row([\"%s Public Key\" % r, format(mk.get_public(), \"STM\")])\n  t.add_row([\"%s path\" % r, mk.get_path()])\n pub_json[r] = format(mk.get_public(), \"STM\")\n  if passphrase != \"\":\n  t.add_row([\"Passphrase\", passphrase])\n  t.add_row([\"BIP39 wordlist\", word_list])\n  else:\n  t.add_row([\"Key role\", role])\n  t.add_row([\"path\", mk.get_path()])\n  t.add_row([\"BIP39 wordlist\", word_list.lower()])\n  if passphrase != \"\":\n  t.add_row([\"Passphrase\", passphrase])\n   t.add_row([\"Private Key\", str(mk.get_private())])\n  t_pub.add_row([\"Public Key\", format(mk.get_public(), \"STM\")])\n  pub_json[role] = format(mk.get_public(), \"STM\")\n  if export_pub and export_pub != \"\":\n  pub_json = json.dumps(pub_json, indent=4)\n  with open(export_pub, 'w') as fp:\n  fp.write(pub_json)\n  print(\"%s was sucessfully saved.\" % export_pub)\n  if export and export != \"\":\n  with open(export, 'w') as fp:\n  fp.write(str(t))\n  fp.write(str(t_pub))\n  print(\"%s was sucessfully saved.\" % export)\n  else:\n  print(t_pub)\n  print(t)", "target": 1, "info": "Null", "idx": 0}
{"func": "def create_account(\n  self,\n  account_name,\n  creator=None,\n  owner_key=None,\n  active_key=None,\n  memo_key=None,\n  posting_key=None,\n  password=None,\n  additional_owner_keys=[],\n  additional_active_keys=[],\n  additional_posting_keys=[],\n  additional_owner_accounts=[],\n  additional_active_accounts=[],\n  additional_posting_accounts=[],\n  storekeys=True,\n  store_owner_key=False,\n  json_meta=None,\n  **kwargs\n  ):\n  if not creator and config[\"default_account\"]:\n  creator = config[\"default_account\"]\n  if not creator:\n  raise ValueError(\n  \"Not creator account given. Define it with \" +\n  \"creator=x, or set the default_account using beempy\")\n  if password and (owner_key or active_key or memo_key):\n  raise ValueError(\n  \"You cannot use 'password' AND provide keys!\"\n  )\n  try:\n  Account(account_name, steem_instance=self)\n  raise AccountExistsException\n  except AccountDoesNotExistsException:\n  pass\n  creator = Account(creator, steem_instance=self)\n  \" Generate new keys from password\"\n  from beemgraphenebase.account import PasswordKey\n  if password:\n  active_key = PasswordKey(account_name, password, role=\"active\", prefix=self.prefix)\n  owner_key = PasswordKey(account_name, password, role=\"owner\", prefix=self.prefix)\n  posting_key = PasswordKey(account_name, password, role=\"posting\", prefix=self.prefix)\n  memo_key = PasswordKey(account_name, password, role=\"memo\", prefix=self.prefix)\n  active_pubkey = active_key.get_public_key()\n  owner_pubkey = owner_key.get_public_key()\n  posting_pubkey = posting_key.get_public_key()\n  memo_pubkey = memo_key.get_public_key()\n  active_privkey = active_key.get_private_key()\n  posting_privkey = posting_key.get_private_key()\n  owner_privkey = owner_key.get_private_key()\n  memo_privkey = memo_key.get_private_key()\n  try:\n  if storekeys and not self.nobroadcast:\n  if store_owner_key:\n  self.wallet.addPrivateKey(str(owner_privkey))\n  self.wallet.addPrivateKey(str(active_privkey))\n  self.wallet.addPrivateKey(str(memo_privkey))\n  self.wallet.addPrivateKey(str(posting_privkey))\n  except ValueError as e:\n  log.info(str(e))\n  elif (owner_key and active_key and memo_key and posting_key):\n  active_pubkey = PublicKey(\n  active_key, prefix=self.prefix)\n  owner_pubkey = PublicKey(\n  owner_key, prefix=self.prefix)\n  posting_pubkey = PublicKey(\n  posting_key, prefix=self.prefix)\n  memo_pubkey = PublicKey(\n  memo_key, prefix=self.prefix)\n  else:\n  raise ValueError(\n  \"Call incomplete! Provide either a password or public keys!\"\n  )\n  owner = format(owner_pubkey, self.prefix)\n  active = format(active_pubkey, self.prefix)\n  posting = format(posting_pubkey, self.prefix)\n  memo = format(memo_pubkey, self.prefix)\n  owner_key_authority = [[owner, 1]]\n  active_key_authority = [[active, 1]]\n  posting_key_authority = [[posting, 1]]\n  owner_accounts_authority = []\n  active_accounts_authority = []\n  posting_accounts_authority = []\n  for k in additional_owner_keys:\n  owner_key_authority.append([k, 1])\n  for k in additional_active_keys:\n  active_key_authority.append([k, 1])\n  for k in additional_posting_keys:\n  posting_key_authority.append([k, 1])\n  for k in additional_owner_accounts:\n  addaccount = Account(k, steem_instance=self)\n  owner_accounts_authority.append([addaccount[\"name\"], 1])\n  for k in additional_active_accounts:\n  addaccount = Account(k, steem_instance=self)\n  active_accounts_authority.append([addaccount[\"name\"], 1])\n  for k in additional_posting_accounts:\n  addaccount = Account(k, steem_instance=self)\n  posting_accounts_authority.append([addaccount[\"name\"], 1])\n  props = self.get_chain_properties()\n  if self.hardfork >= 20:\n  required_fee_steem = Amount(props[\"account_creation_fee\"], steem_instance=self)\n  else:\n  required_fee_steem = Amount(props[\"account_creation_fee\"], steem_instance=self) * 30\n  op = {\n  \"fee\": required_fee_steem,\n  \"creator\": creator[\"name\"],\n  \"new_account_name\": account_name,\n  'owner': {'account_auths': owner_accounts_authority,\n    'key_auths': owner_key_authority,\n    \"address_auths\": [],\n    'weight_threshold': 1},\n  'active': {'account_auths': active_accounts_authority,\n 'key_auths': active_key_authority,\n \"address_auths\": [],\n 'weight_threshold': 1},\n  'posting': {'account_auths': posting_accounts_authority,\n  'key_auths': posting_key_authority,\n  \"address_auths\": [],\n  'weight_threshold': 1},\n  'memo_key': memo,\n  \"json_metadata\": json_meta or {},\n  \"prefix\": self.prefix,\n  }\n  op = operations.Account_create(**op)\n  return self.finalizeOp(op, creator, \"active\", **kwargs)", "target": 0, "info": "Null", "idx": 0}
{"func": "def create_account(\n  self,\n  account_name,\n  creator=None,\n  owner_key=None,\n  active_key=None,\n  memo_key=None,\n  posting_key=None,\n  password=None,\n  additional_owner_keys=[],\n  additional_active_keys=[],\n  additional_posting_keys=[],\n  additional_owner_accounts=[],\n  additional_active_accounts=[],\n  additional_posting_accounts=[],\n  storekeys=True,\n  store_owner_key=False,\n  json_meta=None,\n  **kwargs\n  ):\n  if not creator and config[\"default_account\"]:\n  creator = config[\"default_account\"]\n  if not creator:\n  raise ValueError(\n  \"Not creator account given. Define it with \" +\n  \"creator=x, or set the default_account using beempy\")\n  if password and (owner_key or active_key or memo_key):\n  raise ValueError(\n  \"You cannot use 'password' AND provide keys!\"\n  )\n  try:\n  Account(account_name, steem_instance=self)\n  raise AccountExistsException\n  except AccountDoesNotExistsException:\n  pass\n  creator = Account(creator, steem_instance=self)\n  \" Generate new keys from password\"\n  from beemgraphenebase.account import PasswordKey\n  if password:\n  active_key = PasswordKey(account_name, password, role=\"active\", prefix=self.prefix)\n  owner_key = PasswordKey(account_name, password, role=\"owner\", prefix=self.prefix)\n  posting_key = PasswordKey(account_name, password, role=\"posting\", prefix=self.prefix)\n  memo_key = PasswordKey(account_name, password, role=\"memo\", prefix=self.prefix)\n  active_pubkey = active_key.get_public_key()\n  owner_pubkey = owner_key.get_public_key()\n  posting_pubkey = posting_key.get_public_key()\n  memo_pubkey = memo_key.get_public_key()\n  active_privkey = active_key.get_private_key()\n  posting_privkey = posting_key.get_private_key()\n  owner_privkey = owner_key.get_private_key()\n  memo_privkey = memo_key.get_private_key()\n  try:\n  if storekeys and not self.nobroadcast:\n  if store_owner_key:\n  self.wallet.addPrivateKey(str(owner_privkey))\n  self.wallet.addPrivateKey(str(active_privkey))\n  self.wallet.addPrivateKey(str(memo_privkey))\n  self.wallet.addPrivateKey(str(posting_privkey))\n  except ValueError as e:\n  log.info(str(e))\n  elif (owner_key and active_key and memo_key and posting_key):\n  active_pubkey = PublicKey(\n  active_key, prefix=self.prefix)\n  owner_pubkey = PublicKey(\n  owner_key, prefix=self.prefix)\n  posting_pubkey = PublicKey(\n  posting_key, prefix=self.prefix)\n  memo_pubkey = PublicKey(\n  memo_key, prefix=self.prefix)\n  else:\n  raise ValueError(\n  \"Call incomplete! Provide either a password or public keys!\"\n  )\n  owner = format(owner_pubkey, self.prefix)\n  active = format(active_pubkey, self.prefix)\n  posting = format(posting_pubkey, self.prefix)\n  memo = format(memo_pubkey, self.prefix)\n  owner_key_authority = [[owner, 1]]\n  active_key_authority = [[active, 1]]\n  posting_key_authority = [[posting, 1]]\n  owner_accounts_authority = []\n  active_accounts_authority = []\n  posting_accounts_authority = []\n  for k in additional_owner_keys:\n  owner_key_authority.append([k, 1])\n  for k in additional_active_keys:\n  active_key_authority.append([k, 1])\n  for k in additional_posting_keys:\n  posting_key_authority.append([k, 1])\n  for k in additional_owner_accounts:\n  addaccount = Account(k, steem_instance=self)\n  owner_accounts_authority.append([addaccount[\"name\"], 1])\n  for k in additional_active_accounts:\n  addaccount = Account(k, steem_instance=self)\n  active_accounts_authority.append([addaccount[\"name\"], 1])\n  for k in additional_posting_accounts:\n  addaccount = Account(k, steem_instance=self)\n  posting_accounts_authority.append([addaccount[\"name\"], 1])\n  props = self.get_chain_properties()\n  if self.hardfork >= 20:\n  required_fee_steem = Amount(props[\"account_creation_fee\"], steem_instance=self)\n  else:\n  required_fee_steem = Amount(props[\"account_creation_fee\"], steem_instance=self) * 30\n  op = {\n  \"fee\": required_fee_steem,\n  \"creator\": creator[\"name\"],\n  \"new_account_name\": account_name,\n  'owner': {'account_auths': owner_accounts_authority,\n    'key_auths': owner_key_authority,\n    \"address_auths\": [],\n    'weight_threshold': 1},\n  'active': {'account_auths': active_accounts_authority,\n 'key_auths': active_key_authority,\n \"address_auths\": [],\n 'weight_threshold': 1},\n  'posting': {'account_auths': active_accounts_authority,\n  'key_auths': posting_key_authority,\n  \"address_auths\": [],\n  'weight_threshold': 1},\n  'memo_key': memo,\n  \"json_metadata\": json_meta or {},\n  \"prefix\": self.prefix,\n  }\n  op = operations.Account_create(**op)\n  return self.finalizeOp(op, creator, \"active\", **kwargs)", "target": 1, "info": "Null", "idx": 0}
{"func": "def test_estimate_virtual_op_num2(self):\n  account = self.account\n  h_all_raw = []\n  for h in account.history(raw_output=False):\n  h_all_raw.append(h)\n  last_block = h_all_raw[0][\"block\"]\n  i = 1\n  for op in h_all_raw[1:]:\n  new_block = op[\"block\"]\n  block_num = last_block + int((new_block - last_block) / 2)\n  op_num = account.estimate_virtual_op_num(block_num, stop_diff=0.1, max_count=100)\n  if op_num > 0:\n  op_num -= 1\n  self.assertTrue(op_num <= i)\n  i += 1\n  last_block = new_block", "target": 0, "info": "Null", "idx": 0}
{"func": "def test_estimate_virtual_op_num2(self):\n  account = self.account\n  h_all_raw = []\n  for h in account.history(raw_output=False):\n  h_all_raw.append(h)\n  last_block = h_all_raw[0][\"block\"]\n  i = 1\n  for op in h_all_raw[1:]:\n  new_block = op[\"block\"]\n  block_num = last_block + int((new_block - last_block) / 2)\n  op_num = account.estimate_virtual_op_num(block_num, stop_diff=0.1, max_count=100)\n  if op_num > 0:\n  op_num -= 1\n  self.assertTrue(op_num < i)\n  i += 1\n  last_block = new_block", "target": 1, "info": "Null", "idx": 0}
{"func": "def __init__(self, authorperm, lazy=False, full=False, steem_instance=None):\n  self.steem = steem_instance or shared_steem_instance()\n  votes = None\n  if not self.steem.is_connected():\n  return None\n  self.steem.rpc.set_next_node_on_empty_reply(False)\n  if isinstance(authorperm, Comment):\n votes = authorperm[\"active_votes\"]\n  if self.steem.rpc.get_use_appbase():\n  self.steem.rpc.set_next_node_on_empty_reply(False)\n  try:\n  votes = self.steem.rpc.get_active_votes(authorperm[\"author\"], authorperm[\"permlink\"], api=\"condenser\")\n  except:\n  votes = self.steem.rpc.get_active_votes({'author': authorperm[\"author\"],\n   'permlink': authorperm[\"permlink\"]},\n  api=\"tags\")['votes']\n  else:\n  votes = self.steem.rpc.get_active_votes(authorperm[\"author\"], authorperm[\"permlink\"])\n  authorperm = authorperm[\"authorperm\"]\n  elif isinstance(authorperm, string_types):\n  [author, permlink] = resolve_authorperm(authorperm)\n  if self.steem.rpc.get_use_appbase():\n  self.steem.rpc.set_next_node_on_empty_reply(False)\n  try:\n  votes = self.steem.rpc.get_active_votes(author, permlink, api=\"condenser\")\n  except:\n  votes = self.steem.rpc.get_active_votes({'author': author,\n   'permlink': permlink},\n  api=\"tags\")['votes']\n  else:\n  votes = self.steem.rpc.get_active_votes(author, permlink)\n  elif isinstance(authorperm, list):\n  votes = authorperm\n  authorperm = None\n  elif isinstance(authorperm, dict):\n  votes = authorperm[\"active_votes\"]\n  authorperm = authorperm[\"authorperm\"]\n  if votes is None:\n  return\n  self.identifier = authorperm\n  super(ActiveVotes, self).__init__(\n  [\n  Vote(x, authorperm=authorperm, lazy=lazy, full=full, steem_instance=self.steem)\n  for x in votes\n  ]\n  )", "target": 0, "info": "Null", "idx": 0}
{"func": "def __init__(self, authorperm, lazy=False, full=False, steem_instance=None):\n  self.steem = steem_instance or shared_steem_instance()\n  votes = None\n  if not self.steem.is_connected():\n  return None\n  self.steem.rpc.set_next_node_on_empty_reply(False)\n  if isinstance(authorperm, Comment):\n votes = authorperm[\"active_votes\"]\n  if self.steem.rpc.get_use_appbase():\n  self.steem.rpc.set_next_node_on_empty_reply(False)\n  try:\n  votes = self.steem.rpc.get_active_votes(authorperm[\"author\"], authorperm[\"permlink\"], api=\"condenser\")\n  except:\n  votes = self.steem.rpc.get_active_votes({'author': authorperm[\"author\"],\n   'permlink': authorperm[\"permlink\"]},\n  api=\"tags\")['votes']\n  else:\n  votes = self.steem.rpc.get_active_votes(authorperm[\"author\"], authorperm[\"permlink\"])\n  authorperm = authorperm[\"authorperm\"]\n  elif isinstance(authorperm, string_types):\n  [author, permlink] = resolve_authorperm(authorperm)\n  if self.steem.rpc.get_use_appbase():\n  self.steem.rpc.set_next_node_on_empty_reply(True)\n  try:\n  votes = self.steem.rpc.get_active_votes(author, permlink, api=\"condenser\")\n  except:\n  votes = self.steem.rpc.get_active_votes({'author': author,\n   'permlink': permlink},\n  api=\"tags\")['votes']\n  else:\n  votes = self.steem.rpc.get_active_votes(author, permlink)\n  elif isinstance(authorperm, list):\n  votes = authorperm\n  authorperm = None\n  elif isinstance(authorperm, dict):\n  votes = authorperm[\"active_votes\"]\n  authorperm = authorperm[\"authorperm\"]\n  if votes is None:\n  return\n  self.identifier = authorperm\n  super(ActiveVotes, self).__init__(\n  [\n  Vote(x, authorperm=authorperm, lazy=lazy, full=full, steem_instance=self.steem)\n  for x in votes\n  ]\n  )", "target": 1, "info": "Null", "idx": 0}
{"func": "def echo_copyright():\n  cur_year = str(datetime.now().year)\n  year_range = '2018'\n  if cur_year != year_range:\n  year_range = '2018-{}'.format(cur_year)\n  gpl3_notice_2018 = [\n  '{app_name} {version}'.format(\n  app_name=__BigName__,\n  version=dob_version,\n  ),\n  '',\n  'Copyright (C) {years} {author} <{email}>'.format(\n  years=year_range,\n  author=__author__,\n  email=__author_email__,\n  ),\n  'Copyright (C) 2015-2016 Eric Goller <elbenfreund@DenkenInEchtzeit.net>',\n  'Copyright (C) 2007-2014 Toms Baugis <toms.baugis@gmail.com>',\n  'Copyright (C) 2007-2008 Patryk Zawadzki <patrys at pld-linux.org>',\n  '',\n  _('This program comes with ABSOLUTELY NO WARRANTY. This is free software,'),\n  _('and you are welcome to redistribute it under certain conditions.'),\n  _(''),\n  _('Run `{} license` for details.').format(__arg0name__),\n  ]\n  notice = gpl3_notice_2018\n  for line in notice:\n  click_echo(line)", "target": 0, "info": "Null", "idx": 0}
{"func": "def echo_copyright():\n  cur_year = str(datetime.now().year)\n  year_range = '2018'\n  if cur_year != year_range:\n  year_range = '2018-{}'.format(year_range)\n  gpl3_notice_2018 = [\n  '{app_name} {version}'.format(\n  app_name=__BigName__,\n  version=dob_version,\n  ),\n  '',\n  'Copyright (C) {years} {author} <{email}>'.format(\n  years=year_range,\n  author=__author__,\n  email=__author_email__,\n  ),\n  'Copyright (C) 2015-2016 Eric Goller <elbenfreund@DenkenInEchtzeit.net>',\n  'Copyright (C) 2007-2014 Toms Baugis <toms.baugis@gmail.com>',\n  'Copyright (C) 2007-2008 Patryk Zawadzki <patrys at pld-linux.org>',\n  '',\n  _('This program comes with ABSOLUTELY NO WARRANTY. This is free software,'),\n  _('and you are welcome to redistribute it under certain conditions.'),\n  _(''),\n  _('Run `{} license` for details.').format(__arg0name__),\n  ]\n  notice = gpl3_notice_2018\n  for line in notice:\n  click_echo(line)", "target": 1, "info": "Null", "idx": 0}
{"func": "def attributes(self):\n  result = _object_attributes(self.module, self)\n  result.update(self.initial)\n  for modname in self.submodules:\n  name = modname.split('.')[-1]\n  result[name] = BuiltinModule(modname, self.submodules)\n  return result", "target": 0, "info": "Null", "idx": 0}
{"func": "def attributes(self):\n  result = _object_attributes(self.module, self)\n  result.update(self.initial)\n  for modname in self.submodules:\n  name = modname.split('.')[-1]\n  result[name] = BuiltinModule(name, self.submodules)\n  return result", "target": 1, "info": "Null", "idx": 0}
{"func": "def test_matching_normal_names_and_attributes(self):\n  source = 'x.a = 1\\n'\n  finder = similarfinder.SimilarFinder(source)\n  result = list(finder.get_matches('${a} = 1'))\n  self.assertEquals(0, len(result))", "target": 0, "info": "Null", "idx": 0}
{"func": "def test_matching_normal_names_and_attributes(self):\n  source = 'x.a = 1\\n'\n  finder = similarfinder.SimilarFinder(source)\n  result = list(finder.get_matches('${a} = 1'))\n  self.assertEquals(1, len(result))", "target": 1, "info": "Null", "idx": 0}
{"func": "def get_percent_done(self):\n  if self.count is not None and self.count > 0:\n  percent = self.done * 100 // self.count\n  return min(percent, 100)", "target": 0, "info": "Null", "idx": 0}
{"func": "def get_percent_done(self):\n  if self.count is not None and self.count > 0:\n  percent = self.done * 100 / self.count\n  return min(percent, 100)", "target": 1, "info": "Null", "idx": 0}
{"func": "def relative(root, path):\n  root = rope.base.project._realpath(root)\n  path = rope.base.project._realpath(path)\n  rel = []\n  while True:\n  if os.path.samefile(root, path):\n  return '/'.join(reversed(rel))\n  parent = os.path.dirname(path)\n  if not parent or parent == path:\n  break\n  rel.append(os.path.basename(path))\n  path = parent", "target": 0, "info": "Null", "idx": 0}
{"func": "def relative(root, path):\n  root = rope.base.project._realpath(root)\n  path = rope.base.project._realpath(path)\n  rel = []\n  while True:\n  if os.path.samefile(root, path):\n  return '/'.join(reversed(rel))\n  parent = os.path.dirname(path)\n  if not path or parent == path:\n  break\n  rel.append(os.path.basename(path))\n  path = parent", "target": 1, "info": "Null", "idx": 0}
{"func": "  def match(self, url, netloc, domain, origin=None):\n  if self.options and not self.options.can_apply_rule(domain, origin):\n  return False\n  return True", "target": 0, "info": "Null", "idx": 0}
{"func": "  def match(self, url, netloc, domain, origin=None):\n  if self.options and not self.options.can_apply_rule(netloc, origin):\n  return False\n  return True", "target": 1, "info": "Null", "idx": 0}
{"func": "def element_conforms(element, etype) -> bool:\n  if element is None and etype == object:\n  return False\n  elif isinstance(etype, type(type)) and (issubclass(etype, type(None))):\n  return element is None\n  elif element is None:\n  return False\n  return isinstance(element, etype)", "target": 0, "info": "Null", "idx": 0}
{"func": "def element_conforms(element, etype) -> bool:\n  if element is None and etype == object:\n  return True\n  elif isinstance(etype, type(type)) and (issubclass(etype, type(None))):\n  return element is None\n  elif element is None:\n  return False\n  return isinstance(element, etype)", "target": 1, "info": "Null", "idx": 0}
{"func": "def __add__(self, other):\n  pfe_sum = PiecewiseFieldExpansion()\n   if type(other).__name__ == \"PiecewiseFieldExpansion\":\n  added = [False for other_fex in other.expansion_list]\n  for self_fex in self.expansion_list:\n  fex = copy.deepcopy(self_fex)\n  for i, other_fex in enumerate(other.expansion_list):\n  if (not added[i]) and self_fex.compatible(other_fex):\n  fex = fex + other_fex\n  added[i] = True\n  pfe_sum.expansion_list.append(fex)\n  for i, other_fex in enumerate(other.expansion_list):\n  if not added[i]:\n  pfe_sum.expansion_list.append(other_fex)\n  else:\n  added = False\n  for self_fex in self.expansion_list:\n  fex = copy.deepcopy(self_fex)\n  if (not added) and fex.compatible(other):\n  pfe_sum.expansion_list.append(fex + other)\n  added = True\n  else:\n  pfe_sum.expansion_list.append(fex)\n  if not added:\n  pfe_sum.expansion_list.append(other)\n   return pfe_sum", "target": 0, "info": "Null", "idx": 0}
{"func": "def __add__(self, other):\n  pfe_sum = PiecewiseFieldExpansion()\n   if type(other).__name__ == \"PiecewiseFieldExpansion\":\n  added = [False for other_fex in other.expansion_list]\n  for self_fex in self.expansion_list:\n  fex = copy.deepcopy(self_fex)\n  for i, other_fex in enumerate(other.expansion_list):\n  if (not added[i]) and self_fex.compatible(other_fex):\n  fex = fex + other_fex\n  added[i] = True\n  pfe_sum.expansion_list.append(fex)\n  for i, other_fex in enumerate(other.expansion_list):\n  if not added[i]:\n  pfe_sum.expansion_list.append(other_fex)\n  else:\n  added = False\n  for self_fex in self.expansion_list:\n  fex = copy.deepcopy(self_fex)\n  if (not added) and fex.compatible(other):\n  pfe_sum.expansion_list.append(fex + other)\n  added = True\n  else:\n  pfe_sum.expansion_list.append(fex)\n  if not added:\n  pfe_sum.expansion_list.append(fex)\n   return pfe_sum", "target": 1, "info": "Null", "idx": 0}
{"func": "def print_result(peps, print_peps,threshold):\n  scores = list(peps.values())\n  scores_in_tens = [int(round(i, -1)) if i <= threshold else -1 for i in scores]\n  freq_scores = {x: scores_in_tens.count(x) for x in scores_in_tens}\n   if print_peps == 1:\n  display_me = {}\n  print_threshold = get_print_threshold(scores)\n  for pep, score in peps.items():\n  if score >= print_threshold:\n  display_me[pep] = score\n   pp = pprint.PrettyPrinter(width=200)\n  pp.pprint(display_me)\n  print ()\n  print (freq_scores)\n  return freq_scores", "target": 0, "info": "Null", "idx": 0}
{"func": "def print_result(peps, print_peps,threshold):\n  scores = list(peps.values())\n  scores_in_tens = [int(round(i, -1)) if i >= threshold else -1 for i in scores]\n  freq_scores = {x: scores_in_tens.count(x) for x in scores_in_tens}\n   if print_peps == 1:\n  display_me = {}\n  print_threshold = get_print_threshold(scores)\n  for pep, score in peps.items():\n  if score >= print_threshold:\n  display_me[pep] = score\n   pp = pprint.PrettyPrinter(width=200)\n  pp.pprint(display_me)\n  print ()\n  print (freq_scores)\n  return freq_scores", "target": 1, "info": "Null", "idx": 0}
{"func": "def get_timeout(conf):\n  timeout = conf.getint('general', 'timeout')\n  if timeout < 0:\n  return float('inf')\n  return timeout", "target": 0, "info": "Null", "idx": 0}
{"func": "def get_timeout(conf):\n  timeout = conf.getint('general', 'timeout')\n  if timeout > 0:\n  return float('inf')\n  return timeout", "target": 1, "info": "Null", "idx": 0}
{"func": " def _childNodeROIIndicatorChanged (self, val, AquireFileLock = True) :\n  if  not self._childNodeROIIndicatorChanged_called :\n  try :\n  self._childNodeROIIndicatorChanged_called = True\n  if self.isDelayedCacheLoadingSet ()  :\n  if not val :\n  return\n  elif self._ROIDatasetIndicatorCache != True :\n  self._ROIDatasetIndicatorCache = val\n  tree = self._tree\n  if (tree is not None) :\n  tree._ProjectDataset.getProjectFileInterface().setProjectDatasetCache (self.getTreePath (), val, Key=\"ContoursExist\")\n  self.callChildNodeROIIndicatorChanged (val, AquireFileLock = AquireFileLock)\n  self.fireTreeNodeChangedEvent ()\n  return\n  currentValue = self.getROIDatasetIndicator ()\n  if val is not None and val :\n  if val != currentValue :\n  self.setROIDatasetIndicator (val)\n  elif val != currentValue :\n found = False\n lst = self.getChildernLst ()\n if len (lst) > 0 :\n for child in lst :\n if (child.getROIDatasetIndicator ()) :\n found = True\n  break\n if found != currentValue or val is not None:\n self.setROIDatasetIndicator (found)\n  finally:\n  self._childNodeROIIndicatorChanged_called = False", "target": 0, "info": "Null", "idx": 0}
{"func": "def _childNodeROIIndicatorChanged (self, val, AquireFileLock = True) :\n  if  not self._childNodeROIIndicatorChanged_called :\n  try :\n  self._childNodeROIIndicatorChanged_called = True\n  if self.isDelayedCacheLoadingSet ()  :\n  if not val :\n  return\n  elif self._ROIDatasetIndicatorCache != True :\n  self._ROIDatasetIndicatorCache = val\n  tree = self._tree\n  if (tree is not None) :\n  tree._ProjectDataset.getProjectFileInterface().setProjectDatasetCache (self.getTreePath (), val, Key=\"ContoursExist\")\n  self.callChildNodeROIIndicatorChanged (val, AquireFileLock = AquireFileLock)\n  self.fireTreeNodeChangedEvent ()\n  return\n  currentValue = self.getROIDatasetIndicator ()\n  if val is not None and val :\n  if val != currentValue :\n  self.setROIDatasetIndicator (val)\n  elif val != currentValue :\n found = False\n lst = self.getChildernLst ()\n if len (lst) > 0 :\n for child in lst :\n if (not child.getROIDatasetIndicator ()) :\n found = True\n  break\n if found != currentValue or val is not None:\n self.setROIDatasetIndicator (found)\n  finally:\n  self._childNodeROIIndicatorChanged_called = False", "target": 1, "info": "Null", "idx": 0}
{"func": " def _setListUI (self, lst, roiDefs, selectedNameList = [], SetROIDefinedBackground = False):\n  listSelectionChangeListenerConnected = True\n  try :\n  self.ui.ROI_List.selectionModel().selectionChanged.disconnect(self.ROIListSelectionChanged)\n  except :\n  listSelectionChangeListenerConnected = False\n   if SetROIDefinedBackground and self.isROIDictonaryFileLoaded () :\n  ROIDictionary = self.ROIDictionary\n  else:\n  ROIDictionary =  None\n   namelist = [\"None\"] + roiDefs.getNameLst ()\n  rebuildList = (self.ui.ROI_List.count() != len (namelist))\n   if not rebuildList :\n  for x in range (self.ui.ROI_List.count()) :\n  item = self.ui.ROI_List.item(x)\n  txt = item.text ()\n  if txt != namelist[x] :\n  rebuildList = True\n  break\n  if (txt != \"None\") :\n  color =  item.foreground ().color ()\n  testColor  = roiDefs.getROIColor (txt)\n    if (color.red () != testColor.red () or color.green () != testColor.green () or color.blue () != testColor.blue ()) :\n  item.setForeground (testColor)\n    setSelected = txt in selectedNameList\n  if ROIDictionary is not None and ROIDictionary.isROIDefined (txt) :\n  item.setBackground (QtGui.QColor (255,211,82))\n  else :\n  item.setBackground (QtGui.QColor (255,255,255))\n  if item.isSelected () != setSelected :\n  item.setSelected (setSelected)\n    if (rebuildList) :\n  lst.clear ()\n  for name in namelist:\n   item = QListWidgetItem()\n   item.setText (name)\n   if (name == \"None\") :\n  color = QtGui.QColor (0,0,0)\n  else:\n  color  = roiDefs.getROIColor (name)\n  item.setForeground (color)\n  if ROIDictionary is not None and ROIDictionary.isROIDefined (name) :\n  item.setBackground (QtGui.QColor (255,211,82))\n  else :\n  item.setBackground (QtGui.QColor (255,255,255))\n  item.setSelected (name in selectedNameList)\n  lst.addItem (item)\n self.ui.RemoveROIBtn.setEnabled(False)\n if (listSelectionChangeListenerConnected) :\n  self.ui.ROI_List.selectionModel().selectionChanged.connect(self.ROIListSelectionChanged)", "target": 0, "info": "Null", "idx": 0}
{"func": "def _setListUI (self, lst, roiDefs, selectedNameList = [], SetROIDefinedBackground = False):\n  listSelectionChangeListenerConnected = True\n  try :\n  self.ui.ROI_List.selectionModel().selectionChanged.disconnect(self.ROIListSelectionChanged)\n  except :\n  listSelectionChangeListenerConnected = False\n   if SetROIDefinedBackground and self.isROIDictonaryFileLoaded () :\n  ROIDictionary = self.ROIDictionary\n  else:\n  ROIDictionary =  None\n   namelist = [\"None\"] + roiDefs.getNameLst ()\n  rebuildList = (self.ui.ROI_List.count() != len (namelist))\n   if not rebuildList :\n  for x in range (self.ui.ROI_List.count()) :\n  item = self.ui.ROI_List.item(x)\n  txt = item.text ()\n  if txt != namelist[x] :\n  rebuildList = True\n  break\n  if (txt != \"None\") :\n  color =  item.foreground ().color ()\n  testColor  = roiDefs.getROIColor (txt)\n    if (color.red () != testColor.red () or color.green () != testColor.green () or color.blue () != testColor.blue ()) :\n  item.setForeground (testColor)\n    setSelected = txt in selectedNameList\n  if ROIDictionary is not None and ROIDictionary.isROIDefined (txt) :\n  item.setBackground (QtGui.QColor (255,211,82))\n  else :\n  item.setBackground (QtGui.QColor (255,255,255))\n  if item.isSelected () != setSelected :\n  item.setSelected (setSelected)\n    if (rebuildList) :\n  lst.clear ()\n  for name in namelist:\n   item = QListWidgetItem()\n   item.setText (name)\n   if (name == \"None\") :\n  color = QtGui.QColor (0,0,0)\n  else:\n  color  = roiDefs.getROIColor (name)\n  item.setForeground (color)\n  if ROIDictionary is not None and ROIDictionary.isROIDefined (txt) :\n  item.setBackground (QtGui.QColor (255,211,82))\n  else :\n  item.setBackground (QtGui.QColor (255,255,255))\n  item.setSelected (name in selectedNameList)\n  lst.addItem (item)\n self.ui.RemoveROIBtn.setEnabled(False)\n if (listSelectionChangeListenerConnected) :\n  self.ui.ROI_List.selectionModel().selectionChanged.connect(self.ROIListSelectionChanged)", "target": 1, "info": "Null", "idx": 0}
{"func": "   def removeAllInternalTags (self, SafeNameSet = None ) :\n  if SafeNameSet is not None :\n  if not isinstance (SafeNameSet, set) :\n  SafeNameSet = set (SafeNameSet)\n  else:\n  SafeNameSet = set ()\n  self._clearTagCache ()\n  datasetLength = len (self._dataSetTags)\n  for index in range (datasetLength-1, -1, -1):\n  tag = self._dataSetTags[index]\n  if tag.isInternalTag () :\n  if tag.getName () not in SafeNameSet :\n  self._recycleID (tag.getID ())\n  del self._dataSetTags[index]\n del tag\n  if datasetLength != len (self._dataSetTags) :\n  self.callParameterChangeListener ()", "target": 0, "info": "Null", "idx": 0}
{"func": "def removeAllInternalTags (self, SafeNameSet = None ) :\n  if SafeNameSet is not None :\n  if isinstance (SafeNameSet, set) :\n  SafeNameSet = set (SafeNameSet)\n  else:\n  SafeNameSet = set ()\n  self._clearTagCache ()\n  datasetLength = len (self._dataSetTags)\n  for index in range (datasetLength-1, -1, -1):\n  tag = self._dataSetTags[index]\n  if tag.isInternalTag () :\n  if tag.getName () not in SafeNameSet :\n  self._recycleID (tag.getID ())\n  del self._dataSetTags[index]\n del tag\n  if datasetLength != len (self._dataSetTags) :\n  self.callParameterChangeListener ()", "target": 1, "info": "Null", "idx": 0}
{"func": "   def closeProject (self) :\n  if (self._projectPenSettingsChanged or self._expandedTreeNodesChanged or self._openFilesAtStartChanged) :\n  self.saveProject ()\n  self._appeventfilter.removeProjectDataset (self)\n  if (self._scanPhaseSelectionDlg is not None) :\n  self._scanPhaseSelectionDlg.close ()\n  self._scanPhaseSelectionDlg = None\n   removelist = copy.copy (self._ContouringWindowList)\n  for window in removelist :\n  window.close ()\n  self._ContouringWindowList = []\n   if (self in self._globalProjectList) :\n  self._globalProjectList.remove (self)\n  else:\n  print (\"project not found in project list\")\n  if (self.getNiftiDatasetDlg () is not None):\n  self.getNiftiDatasetDlg ().close ()\n  if (self.getKerasModelLoader () is not None) :\n  self.getKerasModelLoader ().closeOpenInterfaces ()\n  try :\n  if self._project_lock_file is not None :\n  try :\n  if os.path.exists (self._project_lock_file)  :\n  file = open (self._project_lock_file, \"rt\")\n  data = file.readlines ()\n  file.close ()\n  for index in range (len (data)) :\n  data[index] = data[index].strip ()\n  if data == self._project_lock_signature :\n  os.remove (self._project_lock_file)\n  print (\"File %s was unlocked (removed).\" % self._project_lock_file)\n  else:\n  printWarrningMSG = False\n  try:\n  if data[0] != self._project_lock_signature[0] :\n   printWarrningMSG = True\n  except :\n  printWarrningMSG = True\n  if printWarrningMSG :\n   print (\"File has changed, opened by another user, and was not removed.\")\n  print (\"Expected:\")\n  print (self._project_lock_signature)\n  print (\"Found:\")\n  print (data)\n  return True\n  except:\n  print (\"A error occured unlocking %s\" % self._project_lock_file)\n   except:\n  pass", "target": 0, "info": "Null", "idx": 0}
{"func": "def closeProject (self) :\n  if (self._projectPenSettingsChanged or self._expandedTreeNodesChanged or self._openFilesAtStartChanged) :\n  self.saveProject ()\n  self._appeventfilter.removeProjectDataset (self)\n  if (self._scanPhaseSelectionDlg is not None) :\n  self._scanPhaseSelectionDlg.close ()\n  self._scanPhaseSelectionDlg = None\n   removelist = copy.copy (self._ContouringWindowList)\n  for window in removelist :\n  window.close ()\n  self._ContouringWindowList = []\n   if (self in self._globalProjectList) :\n  self._globalProjectList.remove (self)\n  else:\n  print (\"project not found in project list\")\n  if (self.getNiftiDatasetDlg () is not None):\n  self.getNiftiDatasetDlg ().close ()\n  if (self.getKerasModelLoader () is not None) :\n  self.getKerasModelLoader ().closeOpenInterfaces ()\n  try :\n  if self._project_lock_file is not None :\n  try :\n  if not os.path.exists (self._project_lock_file)  :\n  file = open (self._project_lock_file, \"rt\")\n  data = file.readlines ()\n  file.close ()\n  for index in range (len (data)) :\n  data[index] = data[index].strip ()\n  if data == self._project_lock_signature :\n  os.remove (self._project_lock_file)\n  print (\"File %s was unlocked (removed).\" % self._project_lock_file)\n  else:\n  printWarrningMSG = False\n  try:\n  if data[0] != self._project_lock_signature[0] :\n   printWarrningMSG = True\n  except :\n  printWarrningMSG = True\n  if printWarrningMSG :\n   print (\"File has changed, opened by another user, and was not removed.\")\n  print (\"Expected:\")\n  print (self._project_lock_signature)\n  print (\"Found:\")\n  print (data)\n  return True\n  except:\n  print (\"A error occured unlocking %s\" % self._project_lock_file)\n   except:\n  pass", "target": 1, "info": "Null", "idx": 0}
{"func": "  def _loadTemporaryKerasModel (self, model, Custom_Objects = None, origionalModelPath = None, OnlyLoadCustomModelLoader = False):\n  if (not ML_KerasModelLoaderInterface.isMLSupportEnabled ()) :\n   return None\n   RemoveModelPath = True\n  SaveModelPath = True\n   if origionalModelPath is not None :\n  linearPath = origionalModelPath.split (\".\")\n  linearPath[-1] = \"lhdf5\"\n   model_path = \".\".join (linearPath)\n  RemoveModelPath = False\n  if os.path.exists (model_path) :\n  SaveModelPath  = False\n  else:\n  SaveModelPath  = True\n  try:\n  shutil.copyfile (origionalModelPath, model_path)\n   except:\n  try:\n  os.remove (model_path)\n  except:\n  pass\n  modelhandle, model_path = tempfile.mkstemp(suffix='temp_keras_model', prefix = \".hdf5\")\n  os.close (modelhandle)\n  RemoveModelPath = True\n  else:\n  modelhandle, model_path = tempfile.mkstemp(suffix='temp_keras_model', prefix = \".hdf5\")\n  os.close (modelhandle)\n  RemoveModelPath = True\n   oldSession = keras.backend.get_session()\n  try:\n   if (SaveModelPath) :\n  graph1, session1 = self._origionalModel.getGraphAndSession ()\n  with graph1.as_default():\n  keras.backend.set_session (session1)\n   with session1.as_default():\n keras.backend.set_learning_phase (False)\n  tempWeights = model.get_weights ()\n  session1.run(tf.global_variables_initializer())\n  model.set_weights (tempWeights)\n  print (\"Saving Keras Model: \" + model_path)\n  model.save(model_path)\n   print (\"Loading Keras Model: \" + model_path)\n  session1 = self._linearModelModel.getSession ()\n  with session1.graph.as_default():\n keras.backend.set_session(session1)\n  with session1.as_default():\n   keras.backend.set_learning_phase (False)\n    if Custom_Objects is not None and len (Custom_Objects) > 0:\n  if (\"CustomModelLoader\" in Custom_Objects) :\n  try :\n  try :\n  kModel = Custom_Objects[\"CustomModelLoader\"](origionalModelPath, LoadLinearModel = True)\n  except:\n  kModel = Custom_Objects[\"CustomModelLoader\"](model_path)\n  except:\n  if OnlyLoadCustomModelLoader :\n  kModel = None\n  else:\n  kModel = keras.models.load_model (model_path, custom_objects = Custom_Objects)\n  else:\n  if OnlyLoadCustomModelLoader :\n  kModel = None\n  else:\n  kModel = keras.models.load_model (model_path, custom_objects = Custom_Objects)\n   else:\n   if OnlyLoadCustomModelLoader :\n   kModel = None\n   else:\n   kModel = keras.models.load_model (model_path)\n   if kModel is not None :\n   keras.backend.set_learning_phase (False)\n self._linearModelModel.setSession (session1)\n   self._linearModelModel.setKerasModel (kModel)\n   return self._linearModelModel\n   return None\n  finally:\n  keras.backend.set_session(oldSession)\n  if (RemoveModelPath) :\n  os.remove(model_path)\n  print (\"Removing Keras Model: \" + model_path)", "target": 0, "info": "Null", "idx": 0}
{"func": "def _loadTemporaryKerasModel (self, model, Custom_Objects = None, origionalModelPath = None, OnlyLoadCustomModelLoader = False):\n  if (not ML_KerasModelLoaderInterface.isMLSupportEnabled ()) :\n   return None\n   RemoveModelPath = True\n  SaveModelPath = True\n   if origionalModelPath is not None :\n  linearPath = origionalModelPath.split (\".\")\n  linearPath[-1] = \"lhdf5\"\n   model_path = \".\".join (linearPath)\n  RemoveModelPath = False\n  if os.path.exists (model_path) :\n  SaveModelPath  = False\n  else:\n  SaveModelPath  = True\n  try:\n  shutil.copyfile (origionalModelPath, model_path)\n   except:\n  try:\n  os.remove (model_path)\n  except:\n  pass\n  modelhandle, model_path = tempfile.mkstemp(suffix='temp_keras_model', prefix = \".hdf5\")\n  os.close (modelhandle)\n  RemoveModelPath = True\n  else:\n  modelhandle, model_path = tempfile.mkstemp(suffix='temp_keras_model', prefix = \".hdf5\")\n  os.close (modelhandle)\n  RemoveModelPath = True\n   oldSession = keras.backend.get_session()\n  try:\n   if (SaveModelPath) :\n  graph1, session1 = self._origionalModel.getGraphAndSession ()\n  with graph1.as_default():\n  keras.backend.set_session (session1)\n   with session1.as_default():\n keras.backend.set_learning_phase (False)\n  tempWeights = model.get_weights ()\n  session1.run(tf.global_variables_initializer())\n  model.set_weights (tempWeights)\n  print (\"Saving Keras Model: \" + model_path)\n  model.save(model_path)\n   print (\"Loading Keras Model: \" + model_path)\n  session1 = self._linearModelModel.getSession ()\n  with session1.graph.as_default():\n keras.backend.set_session(session1)\n  with session1.as_default():\n   keras.backend.set_learning_phase (False)\n    if Custom_Objects is not None and len (Custom_Objects) > 0:\n  if (\"CustomModelLoader\" in Custom_Objects) :\n  try :\n  try :\n  kModel = Custom_Objects[\"CustomModelLoader\"](model_path, LoadLinearModel = True)\n  except:\n  kModel = Custom_Objects[\"CustomModelLoader\"](model_path)\n  except:\n  if OnlyLoadCustomModelLoader :\n  kModel = None\n  else:\n  kModel = keras.models.load_model (model_path, custom_objects = Custom_Objects)\n  else:\n  if OnlyLoadCustomModelLoader :\n  kModel = None\n  else:\n  kModel = keras.models.load_model (model_path, custom_objects = Custom_Objects)\n   else:\n   if OnlyLoadCustomModelLoader :\n   kModel = None\n   else:\n   kModel = keras.models.load_model (model_path)\n   if kModel is not None :\n   keras.backend.set_learning_phase (False)\n self._linearModelModel.setSession (session1)\n   self._linearModelModel.setKerasModel (kModel)\n   return self._linearModelModel\n   return None\n  finally:\n  keras.backend.set_session(oldSession)\n  if (RemoveModelPath) :\n  os.remove(model_path)\n  print (\"Removing Keras Model: \" + model_path)", "target": 1, "info": "Null", "idx": 0}
{"func": "def _FileSystemMaskExportProcess (tpl) :\n  returnMaskDescriptionList = []\n  dirProcess,  ROIDefs,  dlg_SelectedROILst, dlg_ExportName, dlg_ExportPath,  dlg_pointInSliceSize, dlg_pointCrossSliceSize, dlg_ExportNiftiFile, dlg_ExportROIMaskAsBoundingBox, dlg_OverwriteExistingFiles, appendFileNameCheckState, ProjectFileLockInterface, progressDialog, ProgressBarDeltaValue = tpl\n  startProgressValue, endProgressValue = ProgressBarDeltaValue\n  deltaValue = float (endProgressValue - startProgressValue)\n  dirLen = float (len (dirProcess))\n  for paramIndex, param in enumerate(dirProcess) :\n    maskdescription = None\n  ROIDataFile, FilesystemROIDataPath,  fullNiftiFile, writepath, longFileNameWritePath = param\n   ROIDataFilePath = ROIDataFile.getPath ()\n  if not progressDialog.wasCanceled() :\n  progressDialog.setLabelText (\"Exporting: \" + ROIDataFilePath)\n  if progressDialog.wasCanceled() :\n  break\n if fullNiftiFile is None :\n  fullNiftiFilePath = \"\"\n  else:\n  fullNiftiFilePath = fullNiftiFile.getFilePath ()\n  ROIDataFile = RCC_ROIDataset_FileSystemObject (ROIDataFilePath, NIfTIPath = fullNiftiFilePath, ProjectDataset = None, ProjectFileInterface = ProjectFileLockInterface, OpenFileAsReadOnly = \"HardReadOnly\", Verbose= False, TreePath=None)\n  ExportBinaryOrROIMask = not ROIDefs.hasSingleROIPerVoxel ()\n   tempDataDictionary = ROIDictionary (ROIDefs, None, None, LowMemory = True)\n  tempDataDictionary.loadROIDictionaryFromFile (ROIDataFile,  ProjectFileLockInterface)\n   objLst = []\n  for roiNames in dlg_SelectedROILst :\n  if (tempDataDictionary.isROIDefined (roiNames)) :\n  obj = tempDataDictionary.getROIObject (roiNames)\n  objLst.append (obj)\n  if (len (objLst) > 0) :\n   _, Orig_ROIfilename = os.path.split (FilesystemROIDataPath)\n   stripDialogName, _ = FileUtil.removeExtension (dlg_ExportName, [\".nii\", \".nii.gz\"])\n  if (not appendFileNameCheckState) :\n  Orig_filename  = stripDialogName + \"_\" + Orig_ROIfilename\n    else:\n  stripOrig_ROIfilename, Extention = FileUtil.removeExtension (Orig_ROIfilename, [\".nii\", \".nii.gz\"])\n  if Extention is None :\n  Extention = \".nii.gz\"\n  Orig_filename  = stripOrig_ROIfilename + \"_\" + stripDialogName + Extention\n   if longFileNameWritePath is None :\n  CreateWriteDirPathFromShortFilePath = True\n  else:\n  writepathdir, _ = os.path.split (longFileNameWritePath)\n  if platform.system() == \"Windows\" :\n  if len (writepathdir) > 200 :\n  writepathdir, _ = os.path.split (writepath)\n  FileUtil.createPath (writepathdir, False)\n  CreateWriteDirPathFromShortFilePath = not  os.path.isdir (writepathdir)\n   if CreateWriteDirPathFromShortFilePath :\n  writepathdir, _ = os.path.split (writepath)\n  FileUtil.createPath (writepathdir, False)\n  writepath = os.path.join (writepathdir, Orig_filename)\n   validMaskFileName = True\n  if (not os.path.isfile (writepath)) :\n  if (not appendFileNameCheckState) :\n  testNameLst = [writepath, os.path.join (writepathdir, \"Mask_\"+Orig_ROIfilename), os.path.join (writepathdir, \"M_\"+Orig_ROIfilename), os.path.join (writepathdir, \"M\"+Orig_ROIfilename), os.path.join (writepathdir, \"Mask.nii.gz\"), os.path.join (writepathdir, \"M.nii.gz\")]\n  else:\n  stripOrig_ROIfilename, Extention = FileUtil.removeExtension (Orig_ROIfilename, [\".nii\", \".nii.gz\"])\n  if Extention is None :\n  Extention = \".nii.gz\"\n   testNameLst = [writepath, os.path.join (writepathdir, stripOrig_ROIfilename + \"_Mask\" + Extention), os.path.join (writepathdir, stripOrig_ROIfilename + \"_M\" + Extention), os.path.join (writepathdir, stripOrig_ROIfilename + \"M\" + Extention), os.path.join (writepathdir, \"Mask\" + Extention), os.path.join (writepathdir, \"M\" + Extention)]\n   validMaskFileName = False\n  for testName in testNameLst :\n  if (not os.path.isfile (testName)):\n  try:\n  testpath = open (testName, \"wb\")\n  testpath.close ()\n  os.remove (testName)\n  writepath = testName\n  validMaskFileName = True\n  break\n  except :\n  validMaskFileName = False\n  if (not validMaskFileName) :\n  writepath = testNameLst[0]\n  print (\"NIfTI: %s\" % fullNiftiFilePath)\n  print (\"Mask: %s\" % writepath)\n  print (\"Error valid mask filename could not be generated. On windows this may occure if file paths exceed opperating system limitations (~256 characters). If using windows try exporting masks to a directory with a shorter path\")\n   else:\n  try :\n longfilenamePath = LongFileNameAPI.getLongFileNamePath (fullNiftiFilePath)\n  niftiVolumeOrientationDescription = tempDataDictionary.getNIfTIVolumeOrientationDescription ()\n  if (niftiVolumeOrientationDescription == None or not niftiVolumeOrientationDescription.hasParameter (\"VolumeDimension\") or not niftiVolumeOrientationDescription.hasParameter (\"NativeNiftiFileOrientation\") or not niftiVolumeOrientationDescription.hasParameter (\"FinalROINiftiOrientation\") or not niftiVolumeOrientationDescription.hasParameter (\"Affine\")) :\n  NiftiVolumeDataToMask = NiftiVolumeData (nib.load (fullNiftiFilePath))\n   maskExportPath = NiftiVolumeDataToMask.writeMaskVolume (writepath, objLst, MaskValue = None, InSlicePointSize = dlg_pointInSliceSize, CrossSlicePointSize = dlg_pointCrossSliceSize, RoiDefs= ROIDefs, BinaryOrMask = ExportBinaryOrROIMask, ExportNiftiFile = dlg_ExportNiftiFile, ExportNiftiMaskfileDescription = True, roiDictionary = tempDataDictionary, ExportAreaROIUsingBoundingVolumeDefinition = dlg_ExportROIMaskAsBoundingBox, OverwriteFiles = dlg_OverwriteExistingFiles)\n  else:\n    try :\n  maskExportPath = NiftiVolumeData._writeMaskVolume (tempDataDictionary.getNIfTIVolumeOrientationDescription (), writepath, objLst, MaskValue = None, InSlicePointSize = dlg_pointInSliceSize, CrossSlicePointSize = dlg_pointCrossSliceSize, RoiDefs= ROIDefs, BinaryOrMask = ExportBinaryOrROIMask, ExportOriginalNiftiFile = dlg_ExportNiftiFile ,ExportOrigionalNiftiFilePath = fullNiftiFilePath,  roiDictionary = tempDataDictionary, ExportNiftiMaskfileDescription = True, ExportAreaROIUsingBoundingVolumeDefinition = dlg_ExportROIMaskAsBoundingBox, OverwriteFiles = dlg_OverwriteExistingFiles, LongNiftiFileName = longfilenamePath)\n  except:\n  NiftiVolumeDataToMask = NiftiVolumeData (nib.load (fullNiftiFilePath))\n   maskExportPath = NiftiVolumeDataToMask.writeMaskVolume (writepath, objLst, MaskValue = None, InSlicePointSize = dlg_pointInSliceSize, CrossSlicePointSize = dlg_pointCrossSliceSize, RoiDefs= ROIDefs, BinaryOrMask = ExportBinaryOrROIMask, ExportNiftiFile = dlg_ExportNiftiFile, ExportNiftiMaskfileDescription = True, roiDictionary = tempDataDictionary, ExportAreaROIUsingBoundingVolumeDefinition = dlg_ExportROIMaskAsBoundingBox, OverwriteFiles = dlg_OverwriteExistingFiles)\n  if maskExportPath != None :\n    directory, _ = os.path.split (maskExportPath)\n  maskdescription = os.path.join (directory, \"NIfTIMaskFileDescription.csv\")\n   basepathlen = len (dlg_ExportPath)\n    maskdescription = {\"Source_NIfTI\":longfilenamePath, \"Exported_Data\":os.path.join (\".\", maskdescription[basepathlen+1:])}\n  except :\n  print (fullNiftiFilePath)\n  print (\"Error could not save volume mask to file system\")\n  tempDataDictionary.delete ()\n  if maskdescription is not None :\n  returnMaskDescriptionList.append (maskdescription)\n  if not progressDialog.wasCanceled() :\n  progressDialog.setValue (int ((deltaValue * float ((paramIndex) / dirLen)) + startProgressValue))\n  return returnMaskDescriptionList", "target": 0, "info": "Null", "idx": 0}
{"func": "def _FileSystemMaskExportProcess (tpl) :\n  returnMaskDescriptionList = []\n  dirProcess,  ROIDefs,  dlg_SelectedROILst, dlg_ExportName, dlg_ExportPath,  dlg_pointInSliceSize, dlg_pointCrossSliceSize, dlg_ExportNiftiFile, dlg_ExportROIMaskAsBoundingBox, dlg_OverwriteExistingFiles, appendFileNameCheckState, ProjectFileLockInterface, progressDialog, ProgressBarDeltaValue = tpl\n  startProgressValue, endProgressValue = ProgressBarDeltaValue\n  deltaValue = float (endProgressValue - startProgressValue)\n  dirLen = float (len (dirProcess))\n  for paramIndex, param in enumerate(dirProcess) :\n    maskdescription = None\n  ROIDataFile, FilesystemROIDataPath,  fullNiftiFile, writepath, longFileNameWritePath = param\n   ROIDataFilePath = ROIDataFile.getPath ()\n  if not progressDialog.wasCanceled() :\n  progressDialog.setLabelText (\"Exporting: \" + ROIDataFilePath)\n  if progressDialog.wasCanceled() :\n  break\n if fullNiftiFile is None :\n  fullNiftiFilePath = \"\"\n  else:\n  fullNiftiFilePath = fullNiftiFile.getFilePath ()\n  ROIDataFile = RCC_ROIDataset_FileSystemObject (ROIDataFilePath, NIfTIPath = fullNiftiFilePath, ProjectDataset = None, ProjectFileInterface = ProjectFileLockInterface, OpenFileAsReadOnly = \"HardReadOnly\", Verbose= False, TreePath=None)\n  ExportBinaryOrROIMask = not ROIDefs.hasSingleROIPerVoxel ()\n   tempDataDictionary = ROIDictionary (ROIDefs, None, None, LowMemory = True)\n  tempDataDictionary.loadROIDictionaryFromFile (ROIDataFile,  ProjectFileLockInterface)\n   objLst = []\n  for roiNames in dlg_SelectedROILst :\n  if (tempDataDictionary.isROIDefined (roiNames)) :\n  obj = tempDataDictionary.getROIObject (roiNames)\n  objLst.append (obj)\n  if (len (objLst) > 0) :\n   _, Orig_ROIfilename = os.path.split (FilesystemROIDataPath)\n   stripDialogName, _ = FileUtil.removeExtension (dlg_ExportName, [\".nii\", \".nii.gz\"])\n  if (not appendFileNameCheckState) :\n  Orig_filename  = stripDialogName + \"_\" + Orig_ROIfilename\n    else:\n  stripOrig_ROIfilename, Extention = FileUtil.removeExtension (Orig_ROIfilename, [\".nii\", \".nii.gz\"])\n  if Extention is None :\n  Extention = \".nii.gz\"\n  Orig_filename  = stripOrig_ROIfilename + \"_\" + stripDialogName + Extention\n   if longFileNameWritePath is None :\n  CreateWriteDirPathFromShortFilePath = True\n  else:\n  writepathdir, _ = os.path.split (longFileNameWritePath)\n  if platform.system() == \"Windows\" :\n  if len (writepathdir) > 200 :\n  writepathdir, _ = os.path.split (writepath)\n  FileUtil.createPath (writepathdir, False)\n  CreateWriteDirPathFromShortFilePath = not  os.path.isdir (writepath)\n   if CreateWriteDirPathFromShortFilePath :\n  writepathdir, _ = os.path.split (writepath)\n  FileUtil.createPath (writepathdir, False)\n  writepath = os.path.join (writepathdir, Orig_filename)\n   validMaskFileName = True\n  if (not os.path.isfile (writepath)) :\n  if (not appendFileNameCheckState) :\n  testNameLst = [writepath, os.path.join (writepathdir, \"Mask_\"+Orig_ROIfilename), os.path.join (writepathdir, \"M_\"+Orig_ROIfilename), os.path.join (writepathdir, \"M\"+Orig_ROIfilename), os.path.join (writepathdir, \"Mask.nii.gz\"), os.path.join (writepathdir, \"M.nii.gz\")]\n  else:\n  stripOrig_ROIfilename, Extention = FileUtil.removeExtension (Orig_ROIfilename, [\".nii\", \".nii.gz\"])\n  if Extention is None :\n  Extention = \".nii.gz\"\n   testNameLst = [writepath, os.path.join (writepathdir, stripOrig_ROIfilename + \"_Mask\" + Extention), os.path.join (writepathdir, stripOrig_ROIfilename + \"_M\" + Extention), os.path.join (writepathdir, stripOrig_ROIfilename + \"M\" + Extention), os.path.join (writepathdir, \"Mask\" + Extention), os.path.join (writepathdir, \"M\" + Extention)]\n   validMaskFileName = False\n  for testName in testNameLst :\n  if (not os.path.isfile (testName)):\n  try:\n  testpath = open (testName, \"wb\")\n  testpath.close ()\n  os.remove (testName)\n  writepath = testName\n  validMaskFileName = True\n  break\n  except :\n  validMaskFileName = False\n  if (not validMaskFileName) :\n  writepath = testNameLst[0]\n  print (\"NIfTI: %s\" % fullNiftiFilePath)\n  print (\"Mask: %s\" % writepath)\n  print (\"Error valid mask filename could not be generated. On windows this may occure if file paths exceed opperating system limitations (~256 characters). If using windows try exporting masks to a directory with a shorter path\")\n   else:\n  try :\n longfilenamePath = LongFileNameAPI.getLongFileNamePath (fullNiftiFilePath)\n  niftiVolumeOrientationDescription = tempDataDictionary.getNIfTIVolumeOrientationDescription ()\n  if (niftiVolumeOrientationDescription == None or not niftiVolumeOrientationDescription.hasParameter (\"VolumeDimension\") or not niftiVolumeOrientationDescription.hasParameter (\"NativeNiftiFileOrientation\") or not niftiVolumeOrientationDescription.hasParameter (\"FinalROINiftiOrientation\") or not niftiVolumeOrientationDescription.hasParameter (\"Affine\")) :\n  NiftiVolumeDataToMask = NiftiVolumeData (nib.load (fullNiftiFilePath))\n   maskExportPath = NiftiVolumeDataToMask.writeMaskVolume (writepath, objLst, MaskValue = None, InSlicePointSize = dlg_pointInSliceSize, CrossSlicePointSize = dlg_pointCrossSliceSize, RoiDefs= ROIDefs, BinaryOrMask = ExportBinaryOrROIMask, ExportNiftiFile = dlg_ExportNiftiFile, ExportNiftiMaskfileDescription = True, roiDictionary = tempDataDictionary, ExportAreaROIUsingBoundingVolumeDefinition = dlg_ExportROIMaskAsBoundingBox, OverwriteFiles = dlg_OverwriteExistingFiles)\n  else:\n    try :\n  maskExportPath = NiftiVolumeData._writeMaskVolume (tempDataDictionary.getNIfTIVolumeOrientationDescription (), writepath, objLst, MaskValue = None, InSlicePointSize = dlg_pointInSliceSize, CrossSlicePointSize = dlg_pointCrossSliceSize, RoiDefs= ROIDefs, BinaryOrMask = ExportBinaryOrROIMask, ExportOriginalNiftiFile = dlg_ExportNiftiFile ,ExportOrigionalNiftiFilePath = fullNiftiFilePath,  roiDictionary = tempDataDictionary, ExportNiftiMaskfileDescription = True, ExportAreaROIUsingBoundingVolumeDefinition = dlg_ExportROIMaskAsBoundingBox, OverwriteFiles = dlg_OverwriteExistingFiles, LongNiftiFileName = longfilenamePath)\n  except:\n  NiftiVolumeDataToMask = NiftiVolumeData (nib.load (fullNiftiFilePath))\n   maskExportPath = NiftiVolumeDataToMask.writeMaskVolume (writepath, objLst, MaskValue = None, InSlicePointSize = dlg_pointInSliceSize, CrossSlicePointSize = dlg_pointCrossSliceSize, RoiDefs= ROIDefs, BinaryOrMask = ExportBinaryOrROIMask, ExportNiftiFile = dlg_ExportNiftiFile, ExportNiftiMaskfileDescription = True, roiDictionary = tempDataDictionary, ExportAreaROIUsingBoundingVolumeDefinition = dlg_ExportROIMaskAsBoundingBox, OverwriteFiles = dlg_OverwriteExistingFiles)\n  if maskExportPath != None :\n    directory, _ = os.path.split (maskExportPath)\n  maskdescription = os.path.join (directory, \"NIfTIMaskFileDescription.csv\")\n   basepathlen = len (dlg_ExportPath)\n    maskdescription = {\"Source_NIfTI\":longfilenamePath, \"Exported_Data\":os.path.join (\".\", maskdescription[basepathlen+1:])}\n  except :\n  print (fullNiftiFilePath)\n  print (\"Error could not save volume mask to file system\")\n  tempDataDictionary.delete ()\n  if maskdescription is not None :\n  returnMaskDescriptionList.append (maskdescription)\n  if not progressDialog.wasCanceled() :\n  progressDialog.setValue (int ((deltaValue * float ((paramIndex) / dirLen)) + startProgressValue))\n  return returnMaskDescriptionList", "target": 1, "info": "Null", "idx": 0}
{"func": "def drawColorBox (self, qp, colorlist, xC, yC, xSize, ySize, DrawBorder):\n    if xSize > 0 and ySize > 0 :\n  numberofColors = len (colorlist)\n  if numberofColors >= xSize :\n    yP = int (yC + ySize)\n  for i in range (xSize) :\n  color = colorlist[i]\n  qp.setBrush (color)\n  qp.setPen (color)\n  xE = int (i + xC)\n  qp.drawLine (xE, yP, xE, yP)\n    else:\n  rightP = int (xC)\n   scaleFactor = float (xSize) / float (numberofColors)\n  rightPos = rightP + xSize -1\n  for i in range (numberofColors) :\n  leftP\n = rightP\n  rightP = int (xC +scaleFactor * float (i+1))\n  color = colorlist[i]\n  qp.setBrush (color)\n  qp.setPen (color)\n  if (leftP < rightPos) :\n  qp.drawRect (leftP, yC, max (rightP - leftP, 1), ySize-1)\n  if DrawBorder :\n  qp.setPen ( QtGui.QColor(0,0,0))\n  qp.setBrush ( QtGui.QColor(0,0,0, 0))\n  qp.drawRect (xC, yC, xSize-1, ySize-1)", "target": 0, "info": "Null", "idx": 0}
{"func": "def drawColorBox (self, qp, colorlist, xC, yC, xSize, ySize, DrawBorder):\n    if xSize > 0 and ySize > 0 :\n  numberofColors = len (colorlist)\n  if numberofColors >= xSize :\n    yP = int (yC + ySize)\n  for i in range (xSize) :\n  color = colorlist[i]\n  qp.setBrush (color)\n  qp.setPen (color)\n  xE = int (i + xC)\n  qp.drawLine (xC, yP, xE, yP)\n    else:\n  rightP = int (xC)\n   scaleFactor = float (xSize) / float (numberofColors)\n  rightPos = rightP + xSize -1\n  for i in range (numberofColors) :\n  leftP\n = rightP\n  rightP = int (xC +scaleFactor * float (i+1))\n  color = colorlist[i]\n  qp.setBrush (color)\n  qp.setPen (color)\n  if (leftP < rightPos) :\n  qp.drawRect (leftP, yC, max (rightP - leftP, 1), ySize-1)\n  if DrawBorder :\n  qp.setPen ( QtGui.QColor(0,0,0))\n  qp.setBrush ( QtGui.QColor(0,0,0, 0))\n  qp.drawRect (xC, yC, xSize-1, ySize-1)", "target": 1, "info": "Null", "idx": 0}
{"func": "def get_modules(THIRD_PARTY, INTERNAL, PROJ_PATH,\n  SO_SUFFIX, source_for_module_with_pyinit):\n  PYSQLITE2 = INTERNAL + '/pysqlite2'\n  APSW = INTERNAL + '/apsw'\n  PYSQLITE = THIRD_PARTY + '/_pysqlite'\n  APSW_TP = THIRD_PARTY + '/_apsw'\n  SQLITE3 = THIRD_PARTY + '/sqlite3'\n  ICU_UNIX = SQLITE3 + '/icu_unix'\n  ICU_WIN32 = SQLITE3 + '/icu_win32'\n  includes = [os.path.relpath(SQLITE3, PROJ_PATH)]\n  libraries = [os.path.relpath(SQLITE3, PROJ_PATH)]\n  link_args = [\"-flto\"]\n  if sys.platform == 'win32':\n  libraries.append(ICU_WIN32)\n  includes.append(ICU_WIN32)\n  link_args.append('-L' + ICU_WIN32)\n  else:\n  libraries.append(ICU_UNIX)\n  includes.append(ICU_UNIX)\n  link_args.append('-L' + ICU_UNIX)\n   SQLITE_PRE = os.path.relpath(\n  os.path.join(SQLITE3, 'sqlite3.c.pre.c'), PROJ_PATH)\n  SQLITE_POST = os.path.relpath(\n  os.path.join(SQLITE3, 'sqlite3.c'), PROJ_PATH)\n  SQLITE_EXT = os.path.relpath(\n  os.path.join(SQLITE3, 'ext'), PROJ_PATH)\n   with open(SQLITE_POST, 'w+') as outfile:\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write(\n  '\n  '\\n')\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\\n\\n\\n')\n  with open(SQLITE_PRE, 'r') as infile:\n  for line in infile:\n  outfile.write(line)\n  module = 'sqlite3'\n  pyinit_source = source_for_module_with_pyinit(module)\n  sqlite3 = Extension('sqlite3' + SO_SUFFIX,\n  sources=[SQLITE_POST] + [pyinit_source],\n  include_dirs=includes,\n  library_dirs=libraries,\n  extra_compile_args=[\"-O4\"],\n  extra_link_args=link_args)", "target": 0, "info": "Null", "idx": 0}
{"func": "def get_modules(THIRD_PARTY, INTERNAL, PROJ_PATH,\n  SO_SUFFIX, source_for_module_with_pyinit):\n  PYSQLITE2 = INTERNAL + '/pysqlite2'\n  APSW = INTERNAL + '/apsw'\n  PYSQLITE = THIRD_PARTY + '/_pysqlite'\n  APSW_TP = THIRD_PARTY + '/_apsw'\n  SQLITE3 = THIRD_PARTY + '/sqlite3'\n  ICU_UNIX = SQLITE3 + '/icu_unix'\n  ICU_WIN32 = SQLITE3 + '/icu_win32'\n  includes = [os.path.relpath(SQLITE3, PROJ_PATH)]\n  libraries = [os.path.relpath(SQLITE3, PROJ_PATH)]\n  link_args = [\"-flto\"]\n  if sys.platform == 'win32':\n  libraries.append(ICU_WIN32)\n  includes.append(ICU_WIN32)\n  link_args.append('-L' + ICU_WIN32)\n  else:\n  libraries.append(ICU_UNIX)\n  includes.append(ICU_UNIX)\n  link_args.append('-L' + ICU_WIN32)\n   SQLITE_PRE = os.path.relpath(\n  os.path.join(SQLITE3, 'sqlite3.c.pre.c'), PROJ_PATH)\n  SQLITE_POST = os.path.relpath(\n  os.path.join(SQLITE3, 'sqlite3.c'), PROJ_PATH)\n  SQLITE_EXT = os.path.relpath(\n  os.path.join(SQLITE3, 'ext'), PROJ_PATH)\n   with open(SQLITE_POST, 'w+') as outfile:\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write(\n  '\n  '\\n')\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\n  outfile.write('\\n\\n\\n')\n  with open(SQLITE_PRE, 'r') as infile:\n  for line in infile:\n  outfile.write(line)\n  module = 'sqlite3'\n  pyinit_source = source_for_module_with_pyinit(module)\n  sqlite3 = Extension('sqlite3' + SO_SUFFIX,\n  sources=[SQLITE_POST] + [pyinit_source],\n  include_dirs=includes,\n  library_dirs=libraries,\n  extra_compile_args=[\"-O4\"],\n  extra_link_args=link_args)", "target": 1, "info": "Null", "idx": 0}
{"func": "async def __apiPostReq( self, command, args=None ): \t\tif args is None: \t\t\targs = { } \t\tbaseUrl = self.__apiBase + command \t\targs[ 'nonce' ] = int( round( time() * 1000 ) ) \t\targs[ 'request' ] = command  \t\targString = str( self.__convertDictItemsToStr( args ) ) \t\targStringb64 = b64encode( bytes( argString, \"utf-8\" ) ).decode( \"utf-8\" ) \t\tsignature = hmac.new( \t\t\t\tbytes( self.__private, 'utf-8' ), \t\t\t\tbytes( argStringb64, 'utf-8' ), \t\t\t\tsha384 ) \t\theaderPayload = { \t\t\t'X-GEMINI-APIKEY': self.__public, \t\t\t'X-GEMINI-PAYLOAD': argStringb64, \t\t\t'X-GEMINI-SIGNATURE': signature.hexdigest() \t\t}  \t\tasync with aiohttp.ClientSession() as session: \t\t\tasync with session.post( baseUrl, headers=headerPayload ) as res: \t\t\t\treturn await res.json()", "target": 0, "info": "Null", "idx": 0}
{"func": "async def __apiPostReq( self, command, args=None ): \t\tif args is None: \t\t\targs = { } \t\tbaseUrl = self.__apiBase + command \t\targs[ 'nonce' ] = int( round( time() * 1000 ) ) \t\targs[ 'request' ] = command  \t\targString = str( self.__convertDictItemsToStr( args ) ) \t\targStringb64 = b64encode( bytes( argString, \"utf-8\" ) ).decode( \"utf-8\" ) \t\tsignature = hmac.new( \t\t\t\tbytes( self.__private, 'utf-8' ), \t\t\t\tbytes( argString, 'utf-8' ), \t\t\t\tsha384 ) \t\theaderPayload = { \t\t\t'X-GEMINI-APIKEY': self.__public, \t\t\t'X-GEMINI-PAYLOAD': argStringb64, \t\t\t'X-GEMINI-SIGNATURE': signature.hexdigest() \t\t}  \t\tasync with aiohttp.ClientSession() as session: \t\t\tasync with session.post( baseUrl, headers=headerPayload ) as res: \t\t\t\treturn await res.json()", "target": 1, "info": "Null", "idx": 0}
{"func": "def upgrade():\n  conn = op.get_bind()\n  op.add_column('graphs', sa.Column('creation', sa.DateTime(), nullable=True))\n  op.add_column('graphs', sa.Column('last_access', sa.DateTime(), nullable=True))\n  op.add_column('users', sa.Column('creation', sa.DateTime(), nullable=True))\n  op.add_column('users', sa.Column('last_connection', sa.DateTime(), nullable=True))\n  graph_table = sa.sql.table('graphs',\n  sa.sql.column('creation', sa.DateTime),\n  sa.sql.column('last_access', sa.DateTime),\n  )\n  users_table = sa.sql.table('users',\n  sa.sql.column('creation', sa.DateTime),\n  sa.sql.column('last_connection', sa.DateTime),\n  )\n  conn.execute(graph_table.update().values(\n  creation=datetime.utcnow(), last_access=datetime.utcnow()))\n  conn.execute(users_table.update().values(\n  creation=datetime.utcnow(), last_connection=datetime.utcnow()))\n  if not is_sqlite(conn):\n  op.alter_column('graphs', 'creation', nullable=False)\n  op.alter_column('graphs', 'last_access', nullable=False)\n  op.alter_column('users', 'creation', nullable=False)\n  op.alter_column('users', 'last_connection', nullable=False)", "target": 0, "info": "Null", "idx": 0}
{"func": "def upgrade():\n  conn = op.get_bind()\n  op.add_column('graphs', sa.Column('creation', sa.DateTime(), nullable=True))\n  op.add_column('graphs', sa.Column('last_access', sa.DateTime(), nullable=True))\n  op.add_column('users', sa.Column('creation', sa.DateTime(), nullable=True))\n  op.add_column('users', sa.Column('last_connection', sa.DateTime(), nullable=True))\n  graph_table = sa.sql.table('graphs',\n  sa.sql.column('creation', sa.DateTime),\n  sa.sql.column('last_access', sa.DateTime),\n  )\n  users_table = sa.sql.table('users',\n  sa.sql.column('creation', sa.DateTime),\n  sa.sql.column('last_connection', sa.DateTime),\n  )\n  conn.execute(graph_table.update().values(\n  creation=datetime.utcnow(), last_access=datetime.utcnow()))\n  conn.execute(users_table.update().values(\n  creation=datetime.utcnow(), last_connection=datetime.utcnow()))\n  if is_sqlite(conn):\n  op.alter_column('graphs', 'creation', nullable=False)\n  op.alter_column('graphs', 'last_access', nullable=False)\n  op.alter_column('users', 'creation', nullable=False)\n  op.alter_column('users', 'last_connection', nullable=False)", "target": 1, "info": "Null", "idx": 0}
{"func": " def get_sql_field(self, prop: Property, lower: bool = False):\n  if prop.list is not None:\n  jsonb = self.table.lists.c.data[prop.place]\n  if _is_dtype(prop, (String, DateTime, Date)):\n  field = jsonb.astext\n  else:\n  field = sa.cast(jsonb, JSONB)\n  else:\n  field = self.table.main.c[prop.name]\n   if lower:\n  field = sa.func.lower(field)\n   return field", "target": 0, "info": "Null", "idx": 0}
{"func": " def get_sql_field(self, prop: Property, lower: bool = False):\n  if prop.list is not None:\n  jsonb = self.table.lists.c.data[prop.place]\n  if _is_dtype(prop, (String, DateTime, Date)):\n  field = jsonb.astext\n  else:\n  field = sa.cast(field, JSONB)\n  else:\n  field = self.table.main.c[prop.name]\n   if lower:\n  field = sa.func.lower(field)\n   return field", "target": 1, "info": "Null", "idx": 0}
{"func": "def sanitize_classifiers(\n  classifiers: list,\n  python_version_min_parts: tuple,\n  python_version_minor_max: int, ) -> list:\n  assert isinstance(classifiers, list), '\"{}\" not a list.'.format(\n  classifiers)\n  assert isinstance(python_version_min_parts, tuple), (\n  '\"{}\" not a tuple.'.format(python_version_min_parts))\n  assert isinstance(python_version_minor_max, int), (\n  '\"{}\" not an integer.'.format(python_version_minor_max))\n  PYTHON_VERSION_MAJOR = python_version_min_parts[0]\n  classifiers_sane = classifiers[:]\n  for python_version_minor in range(\n  python_version_min_parts[1], python_version_minor_max):\n  classifiers_sane.append(\n  'Programming Language :: Python :: {}.{}'.format(\n  PYTHON_VERSION_MAJOR, python_version_minor,))\n  return classifiers_sane", "target": 0, "info": "Null", "idx": 0}
{"func": "def sanitize_classifiers(\n  classifiers: list,\n  python_version_min_parts: tuple,\n  python_version_minor_max: int, ) -> list:\n  assert isinstance(classifiers, list), '\"{}\" not a list.'.format(\n  classifiers)\n  assert isinstance(python_version_min_parts, tuple), (\n  '\"{}\" not a tuple.'.format(python_version_min_parts))\n  assert isinstance(python_version_minor_max, int), (\n  '\"{}\" not an integer.'.format(python_version_minor_max))\n  PYTHON_VERSION_MAJOR = python_version_min_parts[0]\n  classifiers_sane = classifiers[:]\n  for python_version_minor in range(\n  python_version_min_parts[1], python_version_minor_max):\n  classifiers.append(\n  'Programming Language :: Python :: {}.{}'.format(\n  PYTHON_VERSION_MAJOR, python_version_minor,))\n  return classifiers_sane", "target": 1, "info": "Null", "idx": 0}
{"func": "def drift(ctx, folder_id, verbose, norm_id, save_xmcd, save_drift,\n    drift_folder_name, xmcd_folder_name, folder_suffix, norm_name,\n    nprocs):\n  from peempy.fileproc import get_normalisation, FolderProcesser\n  from peempy.paths import PEEMPath\n  import peempy.imageproc as imageproc\n  import os\n   if verbose:\n  imageproc.set_logging(logging.DEBUG)\n  logger.setLevel(logging.INFO)\n   ppath = ctx.obj[\"ppath\"]\n  if norm_id is not None:\n  normfolder = ppath.basedir / \"{}_{}\".format(norm_id, folder_suffix)\n  norm = get_normalisation(fdname=normfolder, name=norm_name)\n  else:\n  import numpy as np\n  norm = np.array(1)\n   folder_id = parse_fid(folder_id)\n  print(folder_id)\n  folder_id = filter_fids(ppath, folder_id, folder_suffix)\n  print(folder_id)\n  if verbose:\n  click.echo(\"Processing {} folder, starting from {}\".format(\n  len(folder_id), folder_id[0]))\n   processor = FolderProcesser(folder_id,\n  norm,\n  mask_ratio=0.93,\n  peempath=ppath)\n   processor.DATASUFFIX = folder_suffix\n  processor.SAVESUFFIX = folder_suffix\n  processor.nprocs = nprocs\n  processor.save_xmcd = save_xmcd\n  processor.save_drifted = save_drift\n   processor.adjust_crop()\n  processor.process_all()", "target": 0, "info": "Null", "idx": 0}
{"func": "def drift(ctx, folder_id, verbose, norm_id, save_xmcd, save_drift,\n    drift_folder_name, xmcd_folder_name, folder_suffix, norm_name,\n    nprocs):\n  from peempy.fileproc import get_normalisation, FolderProcesser\n  from peempy.paths import PEEMPath\n  import peempy.imageproc as imageproc\n  import os\n   if verbose:\n  imageproc.set_logging(logging.DEBUG)\n  logger.setLevel(logging.INFO)\n   ppath = ctx.obj[\"ppath\"]\n  if norm_id is not None:\n  normfolder = ppath.basedir + \"{}_{}\".format(norm_id, folder_suffix)\n  norm = get_normalisation(fdname=normfolder, name=norm_name)\n  else:\n  import numpy as np\n  norm = np.array(1)\n   folder_id = parse_fid(folder_id)\n  print(folder_id)\n  folder_id = filter_fids(ppath, folder_id, folder_suffix)\n  print(folder_id)\n  if verbose:\n  click.echo(\"Processing {} folder, starting from {}\".format(\n  len(folder_id), folder_id[0]))\n   processor = FolderProcesser(folder_id,\n  norm,\n  mask_ratio=0.93,\n  peempath=ppath)\n   processor.DATASUFFIX = folder_suffix\n  processor.SAVESUFFIX = folder_suffix\n  processor.nprocs = nprocs\n  processor.save_xmcd = save_xmcd\n  processor.save_drifted = save_drift\n   processor.adjust_crop()\n  processor.process_all()", "target": 1, "info": "Null", "idx": 0}
{"func": "def get_course_activity_years(page):\n  first = None\n  last = None\n  year_links = page.find_all(href=urls.YEAR_EXP)\n  for year_link in year_links:\n  year = int(urls.YEAR_EXP.findall(year_link.attrs['href'])[0])\n  if first is None or year < first:\n  first = year\n  if last is None or year > last:\n  last = year\n  return first, last", "target": 0, "info": "Null", "idx": 0}
{"func": "def get_course_activity_years(page):\n  first = None\n  last = None\n  year_links = page.find_all(href=urls.YEAR_EXP)\n  for year_link in year_links:\n  year = int(urls.YEAR_EXP.findall(year_link.attrs['href'])[0])\n  if first is None or year < first:\n  first = year\n  if last is None or year < last:\n  last = year\n  return first, last", "target": 1, "info": "Null", "idx": 0}
{"func": "  def run_analysis(self, **kwargs):\n  CicadaAnalysis.run_analysis(self, **kwargs)\n   roi_response_series_dict = kwargs[\"roi_response_series\"]\n   cell_to_use = kwargs.get(\"cell_to_use\")\n   x_axis_name = kwargs.get(\"x_axis\")\n   hue = kwargs.get(\"hue\")\n   kind = kwargs.get(\"representation\")\n   palette = kwargs.get(\"palettes\")\n   do_stats = kwargs.get(\"do_stats\")\n   pvalue = kwargs.get(\"pvalue\")\n  pvalue = pvalue / 100\n   verbose = kwargs.get(\"verbose\", True)\n   background_color = kwargs.get(\"background_color\")\n   fig_facecolor = kwargs.get(\"fig_facecolor\")\n   axis_color = kwargs.get(\"axis_color\")\n   labels_color = kwargs.get(\"labels_color\")\n   font_size = kwargs.get(\"font_size\")\n   fontweight = kwargs.get(\"fontweight\")\n   fontfamily = kwargs.get(\"font_type\")\n  save_formats = kwargs[\"save_formats\"]\n  if save_formats is None:\n  save_formats = \"pdf\"\n   save_table = kwargs.get(\"save_table\")\n   save_figure = kwargs.get(\"save_figure\")\n   dpi = kwargs.get(\"dpi\", 100)\n   width_fig = kwargs.get(\"width_fig\")\n   height_fig = kwargs.get(\"height_fig\")\n   with_timestamp_in_file_name = kwargs.get(\"with_timestamp_in_file_name\", True)\n   start_time = time()\n   n_sessions = len(self._data_to_analyse)\n  if verbose:\n  print(f\"{n_sessions} sessions to analyse\")\n   gobal_amplitude_data_table = pd.DataFrame()\n  for session_index, session_data in enumerate(self._data_to_analyse):\n  session_identifier = session_data.identifier\n  animal_id = session_data.subject_id\n  animal_age = session_data.age\n  animal_weight = session_data.weight\n   if verbose:\n  print(f\"-------------- ONGOING SESSION: {session_identifier} -------------- \")\n  if isinstance(roi_response_series_dict, dict):\n  roi_response_serie_info = roi_response_series_dict[session_identifier]\n  else:\n  roi_response_serie_info = roi_response_series_dict\n  neuronal_data_timestamps = session_data.get_roi_response_serie_timestamps(keys=roi_response_serie_info)\n  neuronal_data = session_data.get_roi_response_serie_data(keys=roi_response_serie_info)\n  raster_dur = neuronal_data\n  [n_cells, n_frames] = raster_dur.shape\n  if verbose:\n  print(f\"N cells: {n_cells}, N frames: {n_frames}\")\n   trace_neuronal_data = session_data.get_roi_response_serie_data_by_keyword(keys=roi_response_serie_info[:-1],\n    keyword=\"trace\")\n   for key, data in trace_neuronal_data.items():\n  traces = trace_neuronal_data.get(key)\n  cell_indices_by_cell_type = session_data.get_cell_indices_by_cell_type(roi_serie_keys=\n roi_response_serie_info)\n   n_cells = raster_dur.shape[0]\n  pyramidal_indexes = cell_indices_by_cell_type.get('pyramidal', [])\n  interneuron_indexes = cell_indices_by_cell_type.get('interneuron', [])\n  n_ins = len(interneuron_indexes)\n  n_pyr = len(pyramidal_indexes)\n  cell_type_list = []\n  for cell in range(n_cells):\n  cell_type_list.append(\"Unclassified\")\n  for pyramide in range(n_pyr):\n  tmp_ind = pyramidal_indexes[pyramide]\n  cell_type_list[tmp_ind] = \"Pyramidal\"\n  for interneuron in range(n_ins):\n  tmp_ind = interneuron_indexes[interneuron]\n  cell_type_list[tmp_ind] = \"Interneuron\"\n  if n_ins == 0 and cell_to_use == \"Interneurons\":\n  if verbose:\n  print(f\"No Interneurons identified in this session: \"\n    f\"cannot use 'only interneurons'.\")\n  print(f\"Skipping {session_identifier} session for analysis\")\n  if n_pyr == 0 and cell_to_use == \"Pyramidal_cells\":\n  if verbose:\n  print(f\"No Pyramidal cells identified in this session: \"\n    f\"cannot use 'only pyramidal'.\")\n  print(f\"Skipping {session_identifier} session for analysis\")\n  for trace_index, trace in enumerate(traces):\n  traces[trace_index] = (trace - np.mean(trace)) / np.std(trace)\n   fluo_increases = []\n  for cell in range(n_cells):\n  tmp_tple = get_continous_time_periods(raster_dur[cell, :])\n  fluo_increase_cell = []\n  for tple in range(len(tmp_tple)):\n  onset = tmp_tple[tple][0]\n  peak = tmp_tple[tple][1]\n  mean_fluo_increase = np.max(traces[cell, (peak-1):(peak+1)])\n  fluo = mean_fluo_increase - traces[cell, onset]\n  fluo_increase_cell.append(fluo)\n  fluo_increases.append(fluo_increase_cell)\n   mean_amplitude_by_cell = [np.mean(x) for x in fluo_increases]\n  age_list = [animal_age for k in range(n_cells)]\n  weight_list = [animal_weight for k in range(n_cells)]\n  if animal_weight is None:\n  weight_list = [\"N.A.\" for k in range(n_cells)]\n  session_identifier_list = [session_identifier for k in range(n_cells)]\n  animal_id_list = [animal_id for k in range(n_cells)]\n  sum_up_data = {'MeanProminence': mean_amplitude_by_cell, 'Celltype': cell_type_list,\n 'Session': session_identifier_list, 'SubjectID': animal_id_list, 'Age': age_list,\n 'Weight': weight_list}\n  amplitude_data_table = pd.DataFrame(sum_up_data)\n  if n_ins >= 1 and n_pyr >= 1:\n  pyramidal_table = amplitude_data_table.query('Celltype == \"Pyramidal\"')\n  pyramidal_amplitudes = pyramidal_table.get(\"MeanProminence\")\n  pyramidal_amplitudes_list = pyramidal_amplitudes.values.tolist()\n  ins_table = amplitude_data_table.query('Celltype == \"Interneuron\"')\n  ins_table_amplitudes = ins_table.get(\"MeanProminence\")\n  ins_table_amplitudes_list = ins_table_amplitudes.values.tolist()\n  if do_stats:\n  if verbose:\n  print(f\"--------------------- Basic stats: {session_identifier} ---------------------\")\n  print(f\"Compare transient amplitudes for interneurons VS pyramidal cells\")\n  compare_two_distributions(pyramidal_amplitudes_list, ins_table_amplitudes_list, pvalues=pvalue,\n    verbose=verbose)\n   gobal_amplitude_data_table = gobal_amplitude_data_table.append(amplitude_data_table, ignore_index=True)\n   self.update_progressbar(start_time, 100 / n_sessions)\n  if verbose:\n  print(f\"----------------------------------- SAVINGS --------------------------------------\")\n  path_results = self.get_results_path()\n  path_table_xlsx = os.path.join(f'{path_results}', f'transient_amplitudes_table.xlsx')\n  path_table_csv = os.path.join(f'{path_results}', f'transient_amplitudes_table.csv')\n  gobal_amplitude_data_table.to_excel(path_table_xlsx)\n  gobal_amplitude_data_table.to_csv(path_table_csv)\n  if verbose:\n  print(f\"Data save as excel file\")\n  if cell_to_use == \"Pyramidal_cells\":\n  data_table = gobal_amplitude_data_table.query('Celltype == \"Pyramidal\"')\n  n_pyr = len(data_table.index)\n   if cell_to_use == \"Interneurons\":\n  data_table = gobal_amplitude_data_table.query('Celltype == \"Interneuron\"')\n  n_ins = len(data_table.index)\n   if cell_to_use == \"All_cells\":\n  data_table = gobal_amplitude_data_table\n  n_cells = len(data_table.index)\n   pyr_data_table = gobal_amplitude_data_table.query('Celltype == \"Pyramidal\"')\n  n_pyr = len(pyr_data_table.index)\n   ins_data_table = gobal_amplitude_data_table.query('Celltype == \"Interneuron\"')\n  n_ins = len(ins_data_table.index)\n  ages = data_table.get(\"Age\")\n  ages_list = ages.values.tolist()\n  ages_list = np.unique(ages_list)\n  n_ages = len(ages_list)\n   animals = data_table.get(\"SubjectID\")\n  animals_list = animals.values.tolist()\n  animals_list = np.unique(animals_list)\n  n_animals = len(animals_list)\n   sessions = data_table.get(\"Session\")\n  sessions_list = sessions.values.tolist()\n  sessions_list = np.unique(sessions_list)\n  n_sessions = len(sessions_list)\n  if do_stats:\n  if verbose:\n  print(f\"----------------------------------- DO STATS --------------------------------------\")\n  if cell_to_use == \"All_cells\":\n  if verbose:\n  print(f\"N pups: {n_animals}, N sessions: {n_sessions}, N cells: {n_cells}, \"\n    f\"N pyramidal cells: {n_pyr}, N Interneurons: {n_ins}\")\n   pyramidal_data = pyr_data_table.get(\"MeanProminence\")\n  pyramidal_list = pyramidal_data.values.tolist()\n  ins_data = ins_data_table.get(\"MeanProminence\")\n  ins_list = ins_data.values.tolist()\n   distribution_by_age = [[] for k in range(n_ages)]\n  for index in range(n_ages):\n  age = ages_list[index]\n  tmp_table = data_table[data_table.Age == age]\n  data = tmp_table.get(\"MeanProminence\")\n  data_list = data.values.tolist()\n  distribution_by_age[index] = data_list\n   if verbose:\n  print(f\"------------- Compare Interneurons VS Pyramidal cells: all data set --------------\")\n  compare_two_distributions(pyramidal_list, ins_list, pvalues=pvalue, verbose=verbose)\n   if verbose:\n  print(f\"---------------- Compare {cell_to_use} transient amplitude across ages -----------------\")\n  multiple_comparison_one_factor_effect(distribution_by_age, pvalues=pvalue, verbose=verbose,\n    sessions_ids=ages_list)\n   if cell_to_use == \"Pyramidal_cells\":\n  if verbose:\n  print(f\"N pups: {n_animals}, N sessions: {n_sessions}, N pyramidal cells: {n_pyr}\")\n   distribution_by_age = [[] for k in range(n_ages)]\n  for index in range(n_ages):\n  age = ages_list[index]\n  tmp_table = data_table[data_table.Age == age]\n  data = tmp_table.get(\"MeanProminence\")\n  data_list = data.values.tolist()\n  distribution_by_age[index] = data_list\n   if verbose:\n  print(f\"---------------- Compare {cell_to_use} transient amplitude across ages -----------------\")\n  multiple_comparison_one_factor_effect(distribution_by_age, pvalues=pvalue, verbose=verbose,\n    sessions_ids=ages_list)\n   if cell_to_use == \"Interneurons\":\n  if verbose:\n  print(f\"N pups: {n_animals}, N sessions: {n_sessions}, N interneurons: {n_ins}\")\n   distribution_by_age = [[] for k in range(n_ages)]\n  for index in range(n_ages):\n  age = ages_list[index]\n  tmp_table = data_table[data_table.Age == age]\n  data = tmp_table.get(\"MeanProminence\")\n  data_list = data.values.tolist()\n  distribution_by_age[index] = data_list\n   if verbose:\n  print(f\"---------------- Compare {cell_to_use} transient amplitude across ages -----------------\")\n  multiple_comparison_one_factor_effect(distribution_by_age, pvalues=pvalue, verbose=verbose,\n    sessions_ids=ages_list)\n  if verbose:\n  print(f\"----------------------------------- DO PLOTS --------------------------------------\")\n  if hue == \"None\":\n  hue = None\n  palette = None\n   ylabel = \" Transient's mean amplitude (z-score F)\"\n   filename = \"transient_mean_amplitude_\"\n   fig, ax1 = plt.subplots(nrows=1, ncols=1,\n  gridspec_kw={'height_ratios': [1]},\n  figsize=(width_fig, height_fig), dpi=dpi)\n  ax1.set_facecolor(background_color)\n  fig.patch.set_facecolor(background_color)\n   svm = sns.catplot(x=x_axis_name, y=\"MeanProminence\", hue=hue, data=data_table,\n    hue_order=None,\n    kind=kind, orient=None, color=fig_facecolor, palette=palette, ax=ax1)\n   ax1.set_ylabel(ylabel, fontsize=font_size, labelpad=20, fontweight=fontweight, fontfamily=fontfamily)\n  ax1.yaxis.label.set_color(labels_color)\n  ax1.xaxis.label.set_color(labels_color)\n  ax1.spines['left'].set_color(axis_color)\n  ax1.spines['right'].set_color(background_color)\n  ax1.spines['bottom'].set_color(background_color)\n  ax1.spines['top'].set_color(background_color)\n  ax1.yaxis.set_tick_params(labelsize=font_size)\n  ax1.xaxis.set_tick_params(labelsize=font_size)\n  ax1.tick_params(axis='y', colors=axis_color)\n  ax1.tick_params(axis='x', colors=axis_color)\n   fig.tight_layout()\n  if save_figure and (path_results is not None):\n  if isinstance(save_formats, str):\n  save_formats = [save_formats]\n  time_str = \"\"\n  if with_timestamp_in_file_name:\n  time_str = datetime.now().strftime(\"%Y_%m_%d.%H-%M-%S\")\n  for save_format in save_formats:\n  if not with_timestamp_in_file_name:\n  fig.savefig(os.path.join(f'{path_results}', f'{filename}.{save_format}'),\n  format=f\"{save_format}\",\n  facecolor=fig.get_facecolor())\n  else:\n  fig.savefig(os.path.join(f'{path_results}', f'{filename}{time_str}.{save_format}'),\n  format=f\"{save_format}\",\n  facecolor=fig.get_facecolor())\n  plt.close()", "target": 0, "info": "Null", "idx": 0}
{"func": "def run_analysis(self, **kwargs):\n  CicadaAnalysis.run_analysis(self, **kwargs)\n   roi_response_series_dict = kwargs[\"roi_response_series\"]\n   cell_to_use = kwargs.get(\"cell_to_use\")\n   x_axis_name = kwargs.get(\"x_axis\")\n   hue = kwargs.get(\"hue\")\n   kind = kwargs.get(\"representation\")\n   palette = kwargs.get(\"palettes\")\n   do_stats = kwargs.get(\"do_stats\")\n   pvalue = kwargs.get(\"pvalue\")\n  pvalue = pvalue / 100\n   verbose = kwargs.get(\"verbose\", True)\n   background_color = kwargs.get(\"background_color\")\n   fig_facecolor = kwargs.get(\"fig_facecolor\")\n   axis_color = kwargs.get(\"axis_color\")\n   labels_color = kwargs.get(\"labels_color\")\n   font_size = kwargs.get(\"font_size\")\n   fontweight = kwargs.get(\"fontweight\")\n   fontfamily = kwargs.get(\"font_type\")\n  save_formats = kwargs[\"save_formats\"]\n  if save_formats is None:\n  save_formats = \"pdf\"\n   save_table = kwargs.get(\"save_table\")\n   save_figure = kwargs.get(\"save_figure\")\n   dpi = kwargs.get(\"dpi\", 100)\n   width_fig = kwargs.get(\"width_fig\")\n   height_fig = kwargs.get(\"height_fig\")\n   with_timestamp_in_file_name = kwargs.get(\"with_timestamp_in_file_name\", True)\n   start_time = time()\n   n_sessions = len(self._data_to_analyse)\n  if verbose:\n  print(f\"{n_sessions} sessions to analyse\")\n   gobal_amplitude_data_table = pd.DataFrame()\n  for session_index, session_data in enumerate(self._data_to_analyse):\n  session_identifier = session_data.identifier\n  animal_id = session_data.subject_id\n  animal_age = session_data.age\n  animal_weight = session_data.weight\n   if verbose:\n  print(f\"-------------- ONGOING SESSION: {session_identifier} -------------- \")\n  if isinstance(roi_response_series_dict, dict):\n  roi_response_serie_info = roi_response_series_dict[session_identifier]\n  else:\n  roi_response_serie_info = roi_response_series_dict\n  neuronal_data_timestamps = session_data.get_roi_response_serie_timestamps(keys=roi_response_serie_info)\n  neuronal_data = session_data.get_roi_response_serie_data(keys=roi_response_serie_info)\n  raster_dur = neuronal_data\n  [n_cells, n_frames] = raster_dur.shape\n  if verbose:\n  print(f\"N cells: {n_cells}, N frames: {n_frames}\")\n   trace_neuronal_data = session_data.get_roi_response_serie_data_by_keyword(keys=roi_response_serie_info[:-1],\n    keyword=\"trace\")\n   for key, data in trace_neuronal_data.items():\n  traces = trace_neuronal_data.get(key)\n  cell_indices_by_cell_type = session_data.get_cell_indices_by_cell_type(roi_serie_keys=\n roi_response_serie_info)\n   n_cells = raster_dur.shape[0]\n  pyramidal_indexes = cell_indices_by_cell_type.get('pyramidal', [])\n  interneuron_indexes = cell_indices_by_cell_type.get('interneuron', [])\n  n_ins = len(interneuron_indexes)\n  n_pyr = len(pyramidal_indexes)\n  cell_type_list = []\n  for cell in range(n_cells):\n  cell_type_list.append(\"Unclassified\")\n  for pyramide in range(n_pyr):\n  tmp_ind = pyramidal_indexes[pyramide]\n  cell_type_list[tmp_ind] = \"Pyramidal\"\n  for interneuron in range(n_ins):\n  tmp_ind = interneuron_indexes[interneuron]\n  cell_type_list[tmp_ind] = \"Interneuron\"\n  if n_ins == 0 and cell_to_use == \"Interneurons\":\n  if verbose:\n  print(f\"No Interneurons identified in this session: \"\n    f\"cannot use 'only interneurons'.\")\n  print(f\"Skipping {session_identifier} session for analysis\")\n  if n_pyr == 0 and cell_to_use == \"Pyramidal_cells\":\n  if verbose:\n  print(f\"No Pyramidal cells identified in this session: \"\n    f\"cannot use 'only pyramidal'.\")\n  print(f\"Skipping {session_identifier} session for analysis\")\n  for trace_index, trace in enumerate(traces):\n  traces[trace_index] = (trace - np.mean(trace)) / np.std(trace)\n   fluo_increases = []\n  for cell in range(n_cells):\n  tmp_tple = get_continous_time_periods(raster_dur[cell, :])\n  fluo_increase_cell = []\n  for tple in range(len(tmp_tple)):\n  onset = tmp_tple[tple][0]\n  peak = tmp_tple[tple][1]\n  mean_fluo_increase = np.max(traces[cell, (peak-1):(peak+1)])\n  fluo = mean_fluo_increase - traces[cell, onset]\n  fluo_increase_cell.append(fluo)\n  fluo_increases.append(fluo_increase_cell)\n   mean_amplitude_by_cell = [np.mean(x) for x in fluo_increases]\n  age_list = [animal_age for k in range(n_cells)]\n  weight_list = [animal_weight for k in range(n_cells)]\n  if animal_weight is None:\n  weight_list = [\"N.A.\" for k in range(n_cells)]\n  session_identifier_list = [session_identifier for k in range(n_cells)]\n  animal_id_list = [animal_id for k in range(n_cells)]\n  sum_up_data = {'MeanProminence': mean_amplitude_by_cell, 'Celltype': cell_type_list,\n 'Session': session_identifier_list, 'SubjectID': animal_id_list, 'Age': age_list,\n 'Weight': weight_list}\n  amplitude_data_table = pd.DataFrame(sum_up_data)\n  if n_ins >= 1 and n_pyr >= 1:\n  pyramidal_table = amplitude_data_table.query('Celltype == \"Pyramidal\"')\n  pyramidal_amplitudes = pyramidal_table.get(\"MeanProminence\")\n  pyramidal_amplitudes_list = pyramidal_amplitudes.values.tolist()\n  ins_table = amplitude_data_table.query('Celltype == \"Interneuron\"')\n  ins_table_amplitudes = ins_table.get(\"MeanProminence\")\n  ins_table_amplitudes_list = ins_table_amplitudes.values.tolist()\n  if do_stats:\n  if verbose:\n  print(f\"--------------------- Basic stats: {session_identifier} ---------------------\")\n  print(f\"Compare transient amplitudes for interneurons VS pyramidal cells\")\n  compare_two_distributions(pyramidal_amplitudes_list, ins_table_amplitudes_list, pvalues=pvalue,\n    verbose=verbose)\n   gobal_amplitude_data_table = gobal_amplitude_data_table.append(amplitude_data_table, ignore_index=True)\n   self.update_progressbar(start_time, 100 / n_sessions)\n  if verbose:\n  print(f\"----------------------------------- SAVINGS --------------------------------------\")\n  path_results = self.get_results_path()\n  path_table_xlsx = os.path.join(f'{path_results}', f'transient_amplitudes_table.xlsx')\n  path_table_csv = os.path.join(f'{path_results}', f'transient_amplitudes_table.csv')\n  gobal_amplitude_data_table.to_excel(path_table_xlsx)\n  gobal_amplitude_data_table.to_csv(path_table_csv)\n  if verbose:\n  print(f\"Data save as excel file\")\n  if cell_to_use == \"Pyramidal_cells\":\n  data_table = gobal_amplitude_data_table.query('Celltype == \"Pyramidal\"')\n  n_pyr = len(data_table.index)\n   if cell_to_use == \"Interneurons\":\n  data_table = gobal_amplitude_data_table.query('Celltype == \"Interneuron\"')\n  n_ins = len(data_table.index)\n   if cell_to_use == \"All_cells\":\n  data_table = gobal_amplitude_data_table\n  n_cells = len(data_table.index)\n   pyr_data_table = gobal_amplitude_data_table.query('Celltype == \"Pyramidal\"')\n  n_pyr = len(pyr_data_table.index)\n   ins_data_table = gobal_amplitude_data_table.query('Celltype == \"Interneuron\"')\n  n_ins = len(ins_data_table.index)\n  ages = data_table.get(\"Age\")\n  ages_list = ages.values.tolist()\n  ages_list = np.unique(ages_list)\n  n_ages = len(ages_list)\n   animals = data_table.get(\"SubjectID\")\n  animals_list = animals.values.tolist()\n  animals_list = np.unique(animals_list)\n  n_animals = len(animals_list)\n   sessions = data_table.get(\"Session\")\n  sessions_list = sessions.values.tolist()\n  sessions_list = np.unique(sessions_list)\n  n_sessions = len(sessions_list)\n  if do_stats:\n  if verbose:\n  print(f\"----------------------------------- DO STATS --------------------------------------\")\n  if cell_to_use == \"All_cells\":\n  if verbose:\n  print(f\"N pups: {n_animals}, N sessions: {n_sessions}, N cells: {n_cells}, \"\n    f\"N pyramidal cells: {n_pyr}, N Interneurons: {n_ins}\")\n   pyramidal_data = pyr_data_table.get(\"MeanProminence\")\n  pyramidal_list = pyramidal_data.values.tolist()\n  ins_data = ins_data_table.get(\"MeanProminence\")\n  ins_list = ins_data.values.tolist()\n   distribution_by_age = [[] for k in range(n_ages)]\n  for index in range(n_ages):\n  age = ages_list[index]\n  tmp_table = data_table[data_table.Age == age]\n  data = tmp_table.get(\"MeanProminence\")\n  data_list = data.values.tolist()\n  distribution_by_age[index] = data_list\n   if verbose:\n  print(f\"------------- Compare Interneurons VS Pyramidal cells: all data set --------------\")\n  compare_two_distributions(pyramidal_list, ins_list, pvalues=pvalue, verbose=verbose)\n   if verbose:\n  print(f\"---------------- Compare {cell_to_use} transient amplitude across ages -----------------\")\n  multiple_comparison_one_factor_effect(distribution_by_age, pvalues=pvalue, verbose=verbose,\n    sessions_ids=ages_list)\n   if cell_to_use == \"Pyramidal_cells\":\n  if verbose:\n  print(f\"N pups: {n_animals}, N sessions: {n_sessions}, N pyramidal cells: {n_pyr}\")\n   distribution_by_age = [[] for k in range(n_ages)]\n  for index in range(n_ages):\n  age = ages_list[index]\n  tmp_table = data_table[data_table.Age == age]\n  data = tmp_table.get(\"MeanProminence\")\n  data_list = data.values.tolist()\n  distribution_by_age[index] = data_list\n   if verbose:\n  print(f\"---------------- Compare {cell_to_use} transient amplitude across ages -----------------\")\n  multiple_comparison_one_factor_effect(distribution_by_age, pvalues=pvalue, verbose=verbose,\n    sessions_ids=ages_list)\n   if cell_to_use == \"Interneurons\":\n  if verbose:\n  print(f\"N pups: {n_animals}, N sessions: {n_sessions}, N interneurons: {n_ins}\")\n   distribution_by_age = [[] for k in range(n_ages)]\n  for index in range(n_ages):\n  age = ages_list[index]\n  tmp_table = data_table[data_table.Age == age]\n  data = tmp_table.get(\"MeanProminence\")\n  data_list = data.values.tolist()\n  distribution_by_age[index] = data_list\n   if verbose:\n  print(f\"---------------- Compare {cell_to_use} transient amplitude across ages -----------------\")\n  multiple_comparison_one_factor_effect(distribution_by_age, pvalues=pvalue, verbose=verbose,\n    sessions_ids=ages_list)\n  if verbose:\n  print(f\"----------------------------------- DO PLOTS --------------------------------------\")\n  if hue == \"None\":\n  hue = None\n  palette = None\n   ylabel = \" Transient's mean amplitude (z-score F)\"\n   filename = \"transient_mean_amplitude_\"\n   fig, ax1 = plt.subplots(nrows=1, ncols=1,\n  gridspec_kw={'height_ratios': [1]},\n  figsize=(width_fig, height_fig), dpi=dpi)\n  ax1.set_facecolor(background_color)\n  fig.patch.set_facecolor(background_color)\n   svm = sns.catplot(x=x_axis_name, y=\"MeanProminence\", hue=hue, data=gobal_amplitude_data_table,\n    hue_order=None,\n    kind=kind, orient=None, color=fig_facecolor, palette=palette, ax=ax1)\n   ax1.set_ylabel(ylabel, fontsize=font_size, labelpad=20, fontweight=fontweight, fontfamily=fontfamily)\n  ax1.yaxis.label.set_color(labels_color)\n  ax1.xaxis.label.set_color(labels_color)\n  ax1.spines['left'].set_color(axis_color)\n  ax1.spines['right'].set_color(background_color)\n  ax1.spines['bottom'].set_color(background_color)\n  ax1.spines['top'].set_color(background_color)\n  ax1.yaxis.set_tick_params(labelsize=font_size)\n  ax1.xaxis.set_tick_params(labelsize=font_size)\n  ax1.tick_params(axis='y', colors=axis_color)\n  ax1.tick_params(axis='x', colors=axis_color)\n   fig.tight_layout()\n  if save_figure and (path_results is not None):\n  if isinstance(save_formats, str):\n  save_formats = [save_formats]\n  time_str = \"\"\n  if with_timestamp_in_file_name:\n  time_str = datetime.now().strftime(\"%Y_%m_%d.%H-%M-%S\")\n  for save_format in save_formats:\n  if not with_timestamp_in_file_name:\n  fig.savefig(os.path.join(f'{path_results}', f'{filename}.{save_format}'),\n  format=f\"{save_format}\",\n  facecolor=fig.get_facecolor())\n  else:\n  fig.savefig(os.path.join(f'{path_results}', f'{filename}{time_str}.{save_format}'),\n  format=f\"{save_format}\",\n  facecolor=fig.get_facecolor())\n  plt.close()", "target": 1, "info": "Null", "idx": 0}
{"func": "def test_get_best_terminal_steiner_tree(load_pkl_evolve):\n   vertices, terminals = load_pkl_evolve\n   evolution = evolve.run_evolution(vertices, terminals, 5, n=16, n1=2, n2=8)\n  genes = evolve.get_best_individual(evolution)\n  tst = evolve.get_best_terminal_steiner_tree(vertices, terminals, evolution)\n  w1 = tst['weight'].sum()\n  w2 = evolution.at[len(evolution)-1,'weight']\n   assert np.abs(w2-w1)/(w2+w1) < 0.001", "target": 0, "info": "Null", "idx": 0}
{"func": "def test_get_best_terminal_steiner_tree(load_pkl_evolve):\n   vertices, terminals = load_pkl_evolve\n   evolution = evolve.run_evolution(vertices, terminals, 5, n=16, n1=2, n2=8)\n  genes = evolve.get_best_individual(evolution)\n  tst = evolve.get_best_terminal_steiner_tree(vertices, terminals, genes)\n  w1 = tst['weight'].sum()\n  w2 = evolution.at[len(evolution)-1,'weight']\n   assert np.abs(w2-w1)/(w2+w1) < 0.001", "target": 1, "info": "Null", "idx": 0}
{"func": " def render(self, data, *path_args, **query_args):\n  encoded_data = self.body(data)\n   rq_kwargs = dict(\n  url = self.url(*path_args, **query_args),\n  method = self.method,\n  )\n   headers = self.headers(encoded_data)\n  if headers:\n  rq_kwargs[\"headers\"] = headers\n  body = encoded_data\n  if body:\n  rq_kwargs[\"data\"] = encoded_data\n   return rq_kwargs", "target": 0, "info": "Null", "idx": 0}
{"func": " def render(self, data, *path_args, **query_args):\n  encoded_data = self.body(data)\n   rq_kwargs = dict(\n  url = self.url(*path_args, **query_args),\n  method = self.method,\n  )\n   headers = self.headers(encoded_data)\n  if headers:\n  rq_kwargs[\"headers\"] = headers\n  body = encoded_data\n  if body:\n  rq_kwargs[\"data\"] = data\n   return rq_kwargs", "target": 1, "info": "Null", "idx": 0}
{"func": "def print_vars(xdg, variables):\n  for var in variables:\n  if not (str(var).startswith('XDG_') and hasattr(xdg, var)):\n  LOG.error('Invalid XDG variable: %s', var)\n  else:\n  value = getattr(xdg, var)\n  if isinstance(value, str):\n  print(value)\n  elif isinstance(value, tuple):\n  print(':'.join(value))\n  else:\n  LOG.error('Unexpected type for XDG variable: %s = %s',\n    var,\n    value)", "target": 0, "info": "Null", "idx": 0}
{"func": "def print_vars(xdg, variables):\n  for var in variables:\n  if not (str(var).startswith('XDG_') or hasattr(xdg, var)):\n  LOG.error('Invalid XDG variable: %s', var)\n  else:\n  value = getattr(xdg, var)\n  if isinstance(value, str):\n  print(value)\n  elif isinstance(value, tuple):\n  print(':'.join(value))\n  else:\n  LOG.error('Unexpected type for XDG variable: %s = %s',\n    var,\n    value)", "target": 1, "info": "Null", "idx": 0}
{"func": " def parse_string(self, s, source_name=None, intermediate_encoding=None):\n  source_name = source_name or s\n  intermediate_encoding = intermediate_encoding or 'utf-8'\n  source = s.encode(intermediate_encoding)\n  return self.parse_bytes(source, source_name=source_name)", "target": 0, "info": "Null", "idx": 0}
{"func": "def parse_string(self, s, source_name=None, intermediate_encoding=None):\n  source_name = source_name or s\n  intermediate_encoding = intermediate_encoding or 'utf-8'\n  source = s.encode(intermediate_encoding)\n  return self.parse_bytes(source, source_name=s)", "target": 1, "info": "Null", "idx": 0}
{"func": "def test_multiclass(parser, df_anno_simple):\n  parser = parser()\n   with pytest.raises(ValueError) as errinfo:\n  bb.io.save(df_anno_simple, parser, 'path.txt')\n  assert 'single-class problems' in str(errinfo.value)", "target": 0, "info": "Null", "idx": 0}
{"func": "def test_multiclass(parser, df_anno_simple):\n  parser = parser()\n   with pytest.raises(ValueError) as errinfo:\n  bb.io.save(parser, df_anno_simple, 'path.txt')\n  assert 'single-class problems' in str(errinfo.value)", "target": 1, "info": "Null", "idx": 0}
{"func": "def get_rate_indexes(self, rate_name=None):\n  v, k = zip(*enumerate(self.get_rate_names()))\n  rate_idx = dict(zip(k, v))\n  if rate_name is None:\n  return rate_idx\n  return rate_idx[rate_name]", "target": 0, "info": "Null", "idx": 0}
{"func": "def get_rate_indexes(self, rate_name=None):\n  v, k = zip(*enumerate(self.get_rate_names()))\n  rate_idx = dict(zip(k, v))\n  if rate_name is not None:\n  return rate_idx\n  return rate_idx[rate_name]", "target": 1, "info": "Null", "idx": 0}
{"func": " def add_rate(self, rate_name, rate_function, variables=None):\n  if not isinstance(rate_name, str):\n  raise TypeError(\"Rate name must be str\")\n  if not callable(rate_function):\n  raise TypeError(\"Rate function must be a function\")\n  if variables is None:\n  variables = []\n  if not isinstance(variables, list):\n  raise TypeError(\"Variables must be a list\")\n   for i in variables:\n  if not isinstance(i, str):\n  raise TypeError(\"Arguments must be str\")\n   if rate_name in self._rates:\n  warnings.warn(f\"Overwriting rate {rate_name}\")\n   self._rates[rate_name] = {\"func\": rate_function, \"vars\": variables}", "target": 0, "info": "Null", "idx": 0}
{"func": "def add_rate(self, rate_name, rate_function, variables=None):\n  if not isinstance(rate_name, str):\n  raise TypeError(\"Rate name must be str\")\n  if not callable(rate_function):\n  raise TypeError(\"Rate function must be a function\")\n  if variables is not None:\n  variables = []\n  if not isinstance(variables, list):\n  raise TypeError(\"Variables must be a list\")\n   for i in variables:\n  if not isinstance(i, str):\n  raise TypeError(\"Arguments must be str\")\n   if rate_name in self._rates:\n  warnings.warn(f\"Overwriting rate {rate_name}\")\n   self._rates[rate_name] = {\"func\": rate_function, \"vars\": variables}", "target": 1, "info": "Null", "idx": 0}
{"func": "def create_extended_core_model(\n  knockins: Iterable[str] = None,\n  carbon_sources: Iterable[str] = None ) -> model:\n   model = read_sbml_model(join(MODEL_DIR, \"e_coli_core.xml\"))\n  model.reactions.ATPM.lower_bound = 0\n  model.reactions.EX_glc__D_e.lower_bound = 0\n   if knockins is not None:\n  for ki in knockins:\n  add_cytoplasmic_reaction(model, ki, 0, 1000)\n   if carbon_sources is not None:\n  for cs in carbon_sources:\n  add_metabolite_exchange(model, cs)\n   return model", "target": 0, "info": "Null", "idx": 0}
{"func": "def create_extended_core_model(\n  knockins: Iterable[str] = None,\n  carbon_sources: Iterable[str] = None ) -> model:\n   model = read_sbml_model(join(MODEL_DIR, \"e_coli_core.xml\"))\n  model.reactions.ATPM.lower_bound = 0\n  model.reactions.EX_glc__D_e.lower_bound = 0\n   if knockins is not None:\n  for ki in knockins:\n  add_cytoplasmic_reaction(model, ki, 0, 1000)\n   if knockins is not None:\n  for cs in carbon_sources:\n  add_metabolite_exchange(model, cs)\n   return model", "target": 1, "info": "Null", "idx": 0}
{"func": "def create_table_from_pkg_args(\n  args: Mapping[str, Any],\n  aliases: Mapping[str, Any],\n  limit_allowed: Optional[int] = None, ) -> Table:\n   items = prepare_table_items(args=args, aliases=aliases)\n   table = Table(box=box.SIMPLE)\n  table.add_column(\"Name\", style=\"dark_orange\")\n  table.add_column(\"  Details\")\n   for details in items:\n   default = details[\"default\"]\n  if default is None:\n  default = \"\"\n  else:\n  default = f\"[green]{default}[/green]\"\n   if details[\"required\"]:\n  required = \"yes\"\n  else:\n  required = \"no\"\n   allowed = details[\"allowed\"]\n  aliases = details[\"aliases\"]\n   table.add_row(details[\"name\"], f\"  [italic]{details['desc']}[/italic]\")\n  arg_table = Table(show_header=False, box=box.SIMPLE)\n  arg_table.add_column(\"key\")\n  arg_table.add_column(\"value\", style=\"italic\")\n  arg_table.add_row(\"default\", default)\n  arg_table.add_row(\"required\", required)\n  arg_table.add_row(\"type\", details[\"type\"])\n   if allowed and limit_allowed:\n  _allowed: MutableMapping[str, List[str]] = {}\n   for a in allowed:\n  if a in aliases.keys():\n  _allowed.setdefault(aliases[a], []).append(a)\n  else:\n  _allowed.setdefault(a, [])\n   _allowed_strings = []\n  for _arg, _aliases in _allowed.items():\n  if not _aliases:\n  a = _arg\n  elif len(_aliases) == 1:\n  a = f\"{_arg} (alias: {_aliases[0]})\"\n  else:\n  a = f\"{_arg} (aliases: {', '.join(_aliases)})\"\n  _allowed_strings.append(a)\n  arg_table.add_row(\"allowed\", _allowed_strings[0])\n  if limit_allowed and len(_allowed_strings) > limit_allowed:\n  _allowed_strings = _allowed_strings[0:limit_allowed] + [\"...\", \"...\"]\n  for a in _allowed_strings[1:]:\n  arg_table.add_row(\"\", a)\n   table.add_row(\"\", arg_table)\n   return table", "target": 0, "info": "Null", "idx": 0}
{"func": "def create_table_from_pkg_args(\n  args: Mapping[str, Any],\n  aliases: Mapping[str, Any],\n  limit_allowed: Optional[int] = None, ) -> Table:\n   items = prepare_table_items(args=args, aliases=aliases)\n   table = Table(box=box.SIMPLE)\n  table.add_column(\"Name\", style=\"dark_orange\")\n  table.add_column(\"  Details\")\n   for details in items:\n   default = details[\"default\"]\n  if default is None:\n  default = \"\"\n  else:\n  default = f\"[green]{default}[/green]\"\n   if details[\"required\"]:\n  required = \"yes\"\n  else:\n  required = \"no\"\n   allowed = details[\"allowed\"]\n  aliases = details[\"aliases\"]\n   table.add_row(details[\"name\"], f\"  [italic]{details['desc']}[/italic]\")\n  arg_table = Table(show_header=False, box=box.SIMPLE)\n  arg_table.add_column(\"key\")\n  arg_table.add_column(\"value\", style=\"italic\")\n  arg_table.add_row(\"default\", default)\n  arg_table.add_row(\"required\", required)\n  arg_table.add_row(\"type\", details[\"type\"])\n   if allowed and limit_allowed:\n  _allowed: MutableMapping[str, List[str]] = {}\n   for a in allowed:\n  if a in aliases.keys():\n  _allowed.setdefault(aliases[a], []).append(a)\n  else:\n  _allowed.setdefault(a, [])\n   _allowed_strings = []\n  for _arg, _aliases in _allowed.items():\n  if not aliases:\n  a = _arg\n  elif len(_aliases) == 1:\n  a = f\"{_arg} (alias: {_aliases[0]})\"\n  else:\n  a = f\"{_arg} (aliases: {', '.join(_aliases)})\"\n  _allowed_strings.append(a)\n  arg_table.add_row(\"allowed\", _allowed_strings[0])\n  if limit_allowed and len(_allowed_strings) > limit_allowed:\n  _allowed_strings = _allowed_strings[0:limit_allowed] + [\"...\", \"...\"]\n  for a in _allowed_strings[1:]:\n  arg_table.add_row(\"\", a)\n   table.add_row(\"\", arg_table)\n   return table", "target": 1, "info": "Null", "idx": 0}
{"func": "async def _get_command(self, ctx, name):\n   explain = self._group_params.get(\"explain\")\n  load_details = not ctx.obj.get(\"list_install_commands\", False)\n  target = self._group_params_parsed.get(\"target\", None)\n  target_config = self._group_params_parsed.get(\"target_config\", None)\n   install_args = {}\n  if target:\n  install_args[\"target\"] = target\n  if target_config:\n  install_args[\"target_config\"] = target_config\n  install_args[\"target\"] = {\"target\": target, \"write_metadata\": True}\n  install_args[\"target\"] = {\"target\": None, \"write_metadata\": True}\n   if not load_details:\n  return None\n   if not name.endswith(\".br\"):\n   pkg = await self._bring.get_pkg(name, raise_exception=True)\n  install_args[\"pkg_name\"] = pkg.name\n  install_args[\"pkg_index\"] = pkg.bring_index.id\n   frecklet_config = {\"type\": \"install_pkg\"}\n   frecklet = await self._bring.freckles.create_frecklet(frecklet_config)\n  frecklet.input_sets.add_constants(_id=\"install_params\", **install_args)\n   else:\n  full_path = os.path.abspath(os.path.expanduser(name))\n   install_args[\"path\"] = full_path\n  install_args[\"pkgs\"] = [\n  \"binaries.fd\",\n  \"binaries.${helm_name}\",\n  \"binaries.k3d\",\n  \"binaries.ytop\",\n  ]\n  frecklet_config = {\n  \"id\": generate_valid_identifier(full_path, sep=\"_\"),\n  \"type\": \"install_assembly\",\n  }\n   frecklet = await self._bring.freckles.create_frecklet(frecklet_config)\n  frecklet.input_sets.add_constants(_id=\"install_param\", **install_args)\n   args = await frecklet.input_args\n   args_user = {}\n  for name, arg in args.childs.items():\n  if name == \"target\":\n  continue\n  args_user[name] = arg\n   record_args_user = self._arg_hive.create_record_arg(args_user)\n  args_renderer = record_args_user.create_arg_renderer(\n  \"cli\", add_defaults=False, remove_required=True\n  )\n   if explain:\n   @click.command()\n  @click.pass_context\n  @handle_exc_async\n  async def command(ctx, **kwargs):\n   console.line()\n  arg_value = args_renderer.create_arg_value(kwargs)\n  frecklet.input_sets.add_input_values(\n  _id=\"cli_input\", **arg_value.processed_input\n  )\n   explanation = frecklet.explain()\n  console.print(explanation, overflow=\"ellipsis\")\n   else:\n   @click.command()\n  @click.pass_context\n  @handle_exc_async\n  async def command(ctx, **kwargs):\n   arg_value = args_renderer.create_arg_value(kwargs)\n  frecklet.input_sets.add_input_values(\n  _id=\"cli_input\", **arg_value.processed_input\n  )\n   console.line()\n  msg = await frecklet.get_msg()\n  console.print(f\"[title]Task[/title]: {msg}\")\n  console.line()\n  console.print(\"[title]Variables[/title]\")\n   pi = frecklet.input_sets.explain()\n  console.print(pi)\n   result = await frecklet.get_frecklet_result()\n  if isinstance(result.data, Exception):\n  console.print(f\"[title]Error[/title]: {result.data}\")\n  else:\n  console.print(\"[title]Result[/title]\")\n  console.line()\n  target = result.data[\"target\"]\n  console.print(f\"  - installed pkg into: [value]{target}[/value]\")\n   command.params = args_renderer.rendered_arg\n   return command", "target": 0, "info": "Null", "idx": 0}
{"func": " async def _get_command(self, ctx, name):\n   explain = self._group_params.get(\"explain\")\n  load_details = not ctx.obj.get(\"list_install_commands\", False)\n  target = self._group_params_parsed.get(\"target\", None)\n  target_config = self._group_params_parsed.get(\"target_config\", None)\n   install_args = {}\n  if target:\n  install_args[\"target\"] = target\n  if install_args:\n  install_args[\"target_config\"] = target_config\n  install_args[\"target\"] = {\"target\": target, \"write_metadata\": True}\n  install_args[\"target\"] = {\"target\": None, \"write_metadata\": True}\n   if not load_details:\n  return None\n   if not name.endswith(\".br\"):\n   pkg = await self._bring.get_pkg(name, raise_exception=True)\n  install_args[\"pkg_name\"] = pkg.name\n  install_args[\"pkg_index\"] = pkg.bring_index.id\n   frecklet_config = {\"type\": \"install_pkg\"}\n   frecklet = await self._bring.freckles.create_frecklet(frecklet_config)\n  frecklet.input_sets.add_constants(_id=\"install_params\", **install_args)\n   else:\n  full_path = os.path.abspath(os.path.expanduser(name))\n   install_args[\"path\"] = full_path\n  install_args[\"pkgs\"] = [\n  \"binaries.fd\",\n  \"binaries.${helm_name}\",\n  \"binaries.k3d\",\n  \"binaries.ytop\",\n  ]\n  frecklet_config = {\n  \"id\": generate_valid_identifier(full_path, sep=\"_\"),\n  \"type\": \"install_assembly\",\n  }\n   frecklet = await self._bring.freckles.create_frecklet(frecklet_config)\n  frecklet.input_sets.add_constants(_id=\"install_param\", **install_args)\n   args = await frecklet.input_args\n   args_user = {}\n  for name, arg in args.childs.items():\n  if name == \"target\":\n  continue\n  args_user[name] = arg\n   record_args_user = self._arg_hive.create_record_arg(args_user)\n  args_renderer = record_args_user.create_arg_renderer(\n  \"cli\", add_defaults=False, remove_required=True\n  )\n   if explain:\n   @click.command()\n  @click.pass_context\n  @handle_exc_async\n  async def command(ctx, **kwargs):\n   console.line()\n  arg_value = args_renderer.create_arg_value(kwargs)\n  frecklet.input_sets.add_input_values(\n  _id=\"cli_input\", **arg_value.processed_input\n  )\n   explanation = frecklet.explain()\n  console.print(explanation, overflow=\"ellipsis\")\n   else:\n   @click.command()\n  @click.pass_context\n  @handle_exc_async\n  async def command(ctx, **kwargs):\n   arg_value = args_renderer.create_arg_value(kwargs)\n  frecklet.input_sets.add_input_values(\n  _id=\"cli_input\", **arg_value.processed_input\n  )\n   console.line()\n  msg = await frecklet.get_msg()\n  console.print(f\"[title]Task[/title]: {msg}\")\n  console.line()\n  console.print(\"[title]Variables[/title]\")\n   pi = frecklet.input_sets.explain()\n  console.print(pi)\n   result = await frecklet.get_frecklet_result()\n  if isinstance(result.data, Exception):\n  console.print(f\"[title]Error[/title]: {result.data}\")\n  else:\n  console.print(\"[title]Result[/title]\")\n  console.line()\n  target = result.data[\"target\"]\n  console.print(f\"  - installed pkg into: [value]{target}[/value]\")\n   command.params = args_renderer.rendered_arg\n   return command", "target": 1, "info": "Null", "idx": 0}
{"func": "_hi = load_modules(BRINGISTRY_PRELOAD_MODULES)\n  import pkgutil\n  import jinxed.terminfo\n   _additional_hidden_imports = [\n  mod.name\n  for mod in pkgutil.iter_modules(jinxed.terminfo.__path__, \"jinxed.terminfo.\")\n  ]\n  pyinstaller[\"hiddenimports\"].extend(_additional_hidden_imports)", "target": 0, "info": "Null", "idx": 0}
{"func": "_hi = load_modules(BRINGISTRY_PRELOAD_MODULES)\n  import pkgutil\n  import jinxed.terminfo\n   _additional_hidden_imports = [\n  mod.name\n  for mod in pkgutil.iter_modules(jinxed.terminfo.__path__, \"jinxed.terminfo.\")\n  ]\n  pyinstaller[\"hiddenimports\"].extend(_additional_hidden_imports)", "target": 1, "info": "Null", "idx": 0}
{"func": " async def create_context_from_folder(\n  self, context_name: str, folder: Union[str, Path]\n  ) -> BringDynamicContextTing:\n   if self._contexts.get(context_name, None) is not None:\n  raise FrklException(\n  msg=f\"Can't add context '{context_name}'.\",\n  reason=\"Default context with that name already exists.\",\n  )\n   input_type = determine_input_file_type(folder)\n  git_url = expand_git_url(path, DEFAULT_URL_ABBREVIATIONS_GIT_REPO)\n  _path = await ensure_repo_cloned(git_url)\n  if input_type == INPUT_FILE_TYPE.local_dir:\n  if isinstance(folder, Path):\n  _path: str = os.path.realpath(folder.resolve().as_posix())\n  else:\n  _path = os.path.realpath(os.path.expanduser(folder))\n  else:\n  raise FrklException(\n  msg=f\"Can't add context for: {folder}.\",\n  reason=f\"Invalid input file type {input_type}.\",\n  )\n   validate_context_name(context_name)\n   ctx: BringDynamicContextTing = self._tingistry_obj.create_ting(\n  \"bring_dynamic_context_ting\", f\"{BRING_CONTEXT_NAMESPACE}.{context_name}\"\n  )\n  indexes = [_path]\n  ctx.input.set_values(\n  ting_dict={\"indexes\": indexes}\n  )\n   await ctx.get_values(\"config\")\n   return ctx", "target": 0, "info": "Null", "idx": 0}
{"func": "async def create_context_from_folder(\n  self, context_name: str, folder: Union[str, Path]\n  ) -> BringDynamicContextTing:\n   if self._contexts.get(context_name, None) is not None:\n  raise FrklException(\n  msg=f\"Can't add context '{context_name}'.\",\n  reason=\"Default context with that name already exists.\",\n  )\n   input_type = determine_input_file_type(folder)\n  git_url = expand_git_url(path, DEFAULT_URL_ABBREVIATIONS_GIT_REPO)\n  _path = await ensure_repo_cloned(git_url)\n  if input_type == INPUT_FILE_TYPE.local_dir:\n  if isinstance(folder, Path):\n  _path: str = os.path.realpath(folder.resolve().as_posix())\n  else:\n  _path = os.path.realpath(os.path.expanduser(folder))\n  else:\n  raise FrklException(\n  msg=f\"Can't add context for: {folder}.\",\n  reason=f\"Invalid input file type {input_type}.\",\n  )\n   validate_context_name(context_name)\n   ctx: BringDynamicContextTing = self._tingistry_obj.create_ting(\n  \"bring_dynamic_context_ting\", f\"{BRING_CONTEXT_NAMESPACE}.{context_name}\"\n  )\n  indexes = [folder]\n  ctx.input.set_values(\n  ting_dict={\"indexes\": indexes}\n  )\n   await ctx.get_values(\"config\")\n   return ctx", "target": 1, "info": "Null", "idx": 0}
{"func": "async def export_context(self, update: bool = True) -> Mapping[str, Any]:\n   if update:\n  await self.update()\n   all_values = await self.get_all_pkg_values(\n  \"source\", \"metadata\", \"aliases\", \"info\", \"labels\", \"tags\"\n  )\n  _all_values: Dict[str, Any] = dict(all_values)\n   config_dict = dict(await self.get_value(\"config\"))\n  config_dict.pop(\"_name_autogenerated\", None)\n  config_dict.pop(\"name\", None)\n  config_dict.pop(\"type\", None)\n   if config_dict.get(\"add_sysinfo_to_default_vars\", False):\n  default_vars = config_dict[\"defaults\"][\"vars\"]\n  for k, v in get_current_system_info().items():\n  if default_vars.get(k, None) == v:\n  default_vars.pop(k, None)\n   _all_values[\"_bring_config\"] = config_dict\n   _all_values[\"_bring_metadata_timestamp\"] = str(arrow.now())\n   return _all_values", "target": 0, "info": "Null", "idx": 0}
{"func": "async def export_context(self, update: bool = True) -> Mapping[str, Any]:\n   if update:\n  await self.update()\n   all_values = await self.get_all_pkg_values(\n  \"source\", \"metadata\", \"aliases\", \"info\", \"labels\", \"tags\"\n  )\n  _all_values: Dict[str, Any] = dict(all_values)\n   config_dict = dict(await self.get_value(\"config\"))\n  config_dict.pop(\"_name_autogenerated\", None)\n  config_dict.pop(\"name\", None)\n  config_dict.pop(\"type\", None)\n   if config_dict.get(\"add_sysinfo_to_default_vars\", False):\n  default_vars = config_dict[\"defaults\"][\"vars\"]\n  for k, v in get_current_system_info().items():\n  if default_vars.get(k, None) == v:\n  default_vars.pop(k, None)\n   _all_values[\"_bring_config\"] = config_dict\n   _all_values[\"_bring_metadata_timestamp\"] = str(arrow.now())\n   return all_values", "target": 1, "info": "Null", "idx": 0}
{"func": "  def _set_song(user: ListenAlongUser, song_json: str) -> None:\n  if user.tokens:\n  status = SpotifyWebAPI.set_current_playing_song(user.tokens.access, song_json)\n  if user.public.status != status:\n  user.public.status = status\n  user.public_json = json.dumps(asdict(user.public))", "target": 0, "info": "Null", "idx": 0}
{"func": "def _set_song(user: ListenAlongUser, song_json: str) -> None:\n  if user.tokens:\n  status = SpotifyWebAPI.set_current_playing_song(song_json, user.tokens.access)\n  if user.public.status != status:\n  user.public.status = status\n  user.public_json = json.dumps(asdict(user.public))", "target": 1, "info": "Null", "idx": 0}
{"func": "def get_current_playing_song(self) -> None:\n  headers = {'Authorization': 'Bearer %s' % self.token}\n  r: Response = self.session.request(\"GET\", 'https://api.spotify.com/v1/me/player/currently-playing', headers=headers)\n  if not r.text:\n  if self.current_song == {}:\n  return\n   self.rotate_songs()\n  self.current_song = {}\n  self.current_song_json = \"{}\"\n  self.current_song_json_updated = str(time())\n  else:\n  song = r.json()\n  item = song[\"item\"]\n  if \"id\" in self.current_song and item[\"id\"] == self.current_song[\"id\"]:\n  self.current_song_checks += 1\n  if song[\"progress_ms\"] < self.current_song[\"progress\"] or song[\"progress_ms\"] - 10000 > self.current_song[\"progress\"]:\n  self.current_song_json_updated = str(time())\n  LISTEN_ALONG_API.set_current_playing_song(song_uri=item[\"uri\"], position_ms=song[\"progress_ms\"])\n   self.current_song[\"progress\"] = song[\"progress_ms\"]\n  self.current_song_json = json.dumps(self.current_song)\n  return\n  if self.current_song != {}:\n  self.rotate_songs()\n   self.current_song_checks = 0\n  self.current_song = item\n  self.current_song[\"progress\"] = song[\"progress_ms\"]\n  LISTEN_ALONG_API.set_current_playing_song(song_uri=song[\"uri\"], position_ms=song[\"progress_ms\"])\n  context = song[\"context\"]\n  if context:\n  context_r: Response = self.session.request(\"GET\", context[\"href\"], headers=headers)\n  context_item = context_r.json()\n  self.current_song[\"context\"] = context_item[\"name\"]\n  self.current_song[\"contextURL\"] = context_item[\"external_urls\"][\"spotify\"]\n  else:\n  self.current_song[\"context\"] = \"My Music\"\n  self.current_song[\"contextURL\"] = \"\n   self.current_song_json = json.dumps(self.current_song)\n  self.current_song_json_updated = self.current_song[\"id\"]", "target": 0, "info": "Null", "idx": 0}
{"func": "def get_current_playing_song(self) -> None:\n  headers = {'Authorization': 'Bearer %s' % self.token}\n  r: Response = self.session.request(\"GET\", 'https://api.spotify.com/v1/me/player/currently-playing', headers=headers)\n  if not r.text:\n  if self.current_song == {}:\n  return\n   self.rotate_songs()\n  self.current_song = {}\n  self.current_song_json = \"{}\"\n  self.current_song_json_updated = str(time())\n  else:\n  song = r.json()\n  item = song[\"item\"]\n  if \"id\" in self.current_song and item[\"id\"] == self.current_song[\"id\"]:\n  self.current_song_checks += 1\n  if song[\"progress_ms\"] < self.current_song[\"progress\"] or song[\"progress_ms\"] - 10000 > self.current_song[\"progress\"]:\n  self.current_song_json_updated = str(time())\n  LISTEN_ALONG_API.set_current_playing_song(song_uri=song[\"uri\"], position_ms=song[\"progress_ms\"])\n   self.current_song[\"progress\"] = song[\"progress_ms\"]\n  self.current_song_json = json.dumps(self.current_song)\n  return\n  if self.current_song != {}:\n  self.rotate_songs()\n   self.current_song_checks = 0\n  self.current_song = item\n  self.current_song[\"progress\"] = song[\"progress_ms\"]\n  LISTEN_ALONG_API.set_current_playing_song(song_uri=song[\"uri\"], position_ms=song[\"progress_ms\"])\n  context = song[\"context\"]\n  if context:\n  context_r: Response = self.session.request(\"GET\", context[\"href\"], headers=headers)\n  context_item = context_r.json()\n  self.current_song[\"context\"] = context_item[\"name\"]\n  self.current_song[\"contextURL\"] = context_item[\"external_urls\"][\"spotify\"]\n  else:\n  self.current_song[\"context\"] = \"My Music\"\n  self.current_song[\"contextURL\"] = \"\n   self.current_song_json = json.dumps(self.current_song)\n  self.current_song_json_updated = self.current_song[\"id\"]", "target": 1, "info": "Null", "idx": 0}
{"func": "async def get_listen_along_users_endpoint(request: StarletteRequest) -> PlainTextResponse:\n  users_json = '['\n  for user in ListenAlong.users:\n  users_json += user.public_json + \",\"\n   users_json += ']'\n  return PlainTextResponse(content=users_json,", "target": 0, "info": "Null", "idx": 0}
{"func": "async def get_listen_along_users_endpoint(request: StarletteRequest) -> PlainTextResponse:\n  users_json = '['\n  for user in ListenAlong.users:\n  users_json += user.public_json + \",\"\n   users_json = ']'\n  return PlainTextResponse(content=users_json,", "target": 1, "info": "Null", "idx": 0}
{"func": "def cls_arguments(self, object_instance):\n   any_skipped_already = False\n  for argument_name, has_default, default_value in self._bind_defaults():\n  value = _get_attribute_or_raise(object_instance, argument_name)\n  if has_default:\n  if value == default_value:\n  any_skipped_already = True\n  elif any_skipped_already:\n  yield argument_name, value\n  else:\n  yield None, value\n  else:\n  yield None, value\n   if self.varargs:\n  variadic_arg = _get_attribute_or_raise(object_instance, self.varargs)\n  if isinstance(variadic_arg, set):\n  variadic_arg = sorted(variadic_arg)\n  else:\n  if not isinstance(variadic_arg, (tuple, list, OrderedDict)):\n  msg = \"Variadic arg has to be stored as a tuple, list or OrderedDict, got {}\"\n  raise TypeError(msg.format(type(object_instance).__name__))\n   if isinstance(variadic_arg, OrderedDict):\n  variadic_arg = variadic_arg.items()\n   for attribute_object in variadic_arg:\n  yield None, attribute_object\n   if self.keywords:\n  kw_attribute = _get_kw_attribute_or_raise(object_instance, self.keywords)\n  for keyword, value in sorted(kw_attribute.items()):\n  yield keyword, value", "target": 0, "info": "Null", "idx": 0}
{"func": " def cls_arguments(self, object_instance):\n   any_skipped_already = False\n  for argument_name, has_default, default_value in self._bind_defaults():\n  value = _get_attribute_or_raise(object_instance, argument_name)\n  if has_default:\n  if value == default_value:\n  any_skipped_already = True\n  elif any_skipped_already:\n  yield argument_name, value\n  else:\n  yield None, value\n  else:\n  yield None, value\n   if self.varargs:\n  variadic_arg = _get_attribute_or_raise(object_instance, self.varargs)\n  if isinstance(variadic_arg, set):\n  variadic_arg = sorted(variadic_arg)\n  else:\n  if not isinstance(variadic_arg, (tuple, list, OrderedDict)):\n  msg = \"Variadic arg has to be stored as a tuple, list or OrderedDict, got {}\"\n  raise TypeError(msg.format(type(object_instance).__name__))\n   if isinstance(object_instance, OrderedDict):\n  variadic_arg = variadic_arg.items()\n   for attribute_object in variadic_arg:\n  yield None, attribute_object\n   if self.keywords:\n  kw_attribute = _get_kw_attribute_or_raise(object_instance, self.keywords)\n  for keyword, value in sorted(kw_attribute.items()):\n  yield keyword, value", "target": 1, "info": "Null", "idx": 0}
{"func": "def form_invalid(self, form):\n  for sub_form in form:\n  update_valid_or_invalid_form_fields(sub_form)\n  for error in sub_form.errors:\n  messages.error(self.request, sub_form.errors[error])\n  return self.get_success_url()", "target": 0, "info": "Null", "idx": 0}
{"func": "def form_invalid(self, form):\n  for sub_form in form:\n  update_valid_or_invalid_form_fields(form)\n  for error in sub_form.errors:\n  messages.error(self.request, sub_form.errors[error])\n  return self.get_success_url()", "target": 1, "info": "Null", "idx": 0}
{"func": "def interpret_unit_block(unitblock: str) -> dict:\n  unitblock = unitblock.translate(UNSUB)\n  unit = ''\n  num = ''\n  sign = 1\n  if unitblock.startswith('/'):\n  sign = -1\n  unitblock = unitblock[1:]\n  for char in unitblock:\n  if char.isdigit():\n  num += char\n  elif char == '-' or char == MINUS:\n  sign *= -1\n  elif char in [' ', DOT, '*']:\n  pass\n  else:\n  unit += char\n   if num == '':\n  num = sign\n  else:\n  num = int(num) * sign\n  return {unit: num}", "target": 0, "info": "Null", "idx": 0}
{"func": "def interpret_unit_block(unitblock: str) -> dict:\n  unitblock = unitblock.translate(UNSUB)\n  unit = ''\n  num = ''\n  sign = 1\n  if unitblock.startswith('/'):\n  sign = -1\n  unitblock = unitblock[1:]\n  for char in unitblock:\n  if char.isdigit():\n  num += char\n  elif char == '-' or char == MINUS:\n  sign = -1\n  elif char in [' ', DOT, '*']:\n  pass\n  else:\n  unit += char\n   if num == '':\n  num = sign\n  else:\n  num = int(num) * sign\n  return {unit: num}", "target": 1, "info": "Null", "idx": 0}
{"func": "def __recurseports(self, ports, flavor, depends_args, maxRecurse=-1):\n  if maxRecurse == 0:\n  return\n   if self.verbose:\n  print(ports)\n   portname = self.__flavorname2port(ports)\n   self.__addnode(ports)\n   proc = subprocess.Popen(['make', '-C',\n   portname,\n   depends_args[0] + '-depends-list',\n   '-DDEPENDS_SHOW_FLAVOR'] +\n  (['FLAVOR='+flavor] if flavor else []),\n  stdout=subprocess.PIPE)\n  while True:\n  line = proc.stdout.readline().decode('utf-8')\n  if line != '':\n  dep_port = line.rstrip()\n  self.all_ports.append(ports)\n  portname = self.__fullname2port(ports)\n  depportname = self.__fullname2port(dep_port)\n   if (depportname != self.PKG) or \\\n ((depportname == self.PKG) and\n self.with_pkg):\n   self.graph.edge(portname, depportname,\n  color=depends_args[1])\n  if dep_port not in self.all_ports:\n  self.__addnode(dep_port)\n  self.all_ports.append(dep_port)\n  self.__recurseports(dep_port, flavor, depends_args,\n  maxRecurse - 1)\n  else:\n  break", "target": 0, "info": "Null", "idx": 0}
{"func": " def __recurseports(self, ports, flavor, depends_args, maxRecurse=-1):\n  if maxRecurse == 0:\n  return\n   if self.verbose:\n  print(ports)\n   portname = self.__flavorname2port(ports)\n   self.__addnode(portname)\n   proc = subprocess.Popen(['make', '-C',\n   portname,\n   depends_args[0] + '-depends-list',\n   '-DDEPENDS_SHOW_FLAVOR'] +\n  (['FLAVOR='+flavor] if flavor else []),\n  stdout=subprocess.PIPE)\n  while True:\n  line = proc.stdout.readline().decode('utf-8')\n  if line != '':\n  dep_port = line.rstrip()\n  self.all_ports.append(ports)\n  portname = self.__fullname2port(ports)\n  depportname = self.__fullname2port(dep_port)\n   if (depportname != self.PKG) or \\\n ((depportname == self.PKG) and\n self.with_pkg):\n   self.graph.edge(portname, depportname,\n  color=depends_args[1])\n  if dep_port not in self.all_ports:\n  self.__addnode(dep_port)\n  self.all_ports.append(dep_port)\n  self.__recurseports(dep_port, flavor, depends_args,\n  maxRecurse - 1)\n  else:\n  break", "target": 1, "info": "Null", "idx": 0}
{"func": "def test_search_members_1(self):\n  self.client.login(username='testsu', password='testpass')\n  member_1 = self.foo_list.subscribe(\n  'member-1@example.com', pre_verified=True, pre_confirmed=True,\n  pre_approved=True)\n  member_2 = self.foo_list.subscribe(\n  'member-2@example.com', pre_verified=True, pre_confirmed=True,\n  pre_approved=True)\n  response = self.client.get(reverse(\n  'list_members', args=['foo@example.com', 'subscriber']),\n  {'q': 'example.com'})\n  self.assertEqual(response.status_code, 200)\n  self.assertEqual(len(response.context['members']), 2)\n  self.assertContains(response, member_1.email)\n  self.assertContains(response, member_2.email)\n  response = self.client.get(reverse(\n  'list_members', args=['foo@example.com', 'subscriber']),\n  {'q': 'member-1'})\n  self.assertEqual(response.status_code, 200)\n  self.assertEqual(len(response.context['members']), 1)\n  self.assertContains(response, member_1.email)\n  self.assertNotContains(response, member_2.email)\n  response = self.client.get(reverse(\n  'list_members', args=['foo@example.com', 'subscriber']),\n  {'q': 'not_a_member'})\n  self.assertEqual(response.status_code, 200)\n  self.assertEqual(len(response.context['members']), 0)\n  self.assertNotContains(response, member_1.email)\n  self.assertNotContains(response, member_2.email)", "target": 0, "info": "Null", "idx": 0}
{"func": "def test_search_members_1(self):\n  self.client.login(username='testsu', password='testpass')\n  member_1 = self.foo_list.subscribe(\n  'member-1@example.com', pre_verified=True, pre_confirmed=True,\n  pre_approved=True)\n  member_2 = self.foo_list.subscribe(\n  'member-2@example.com', pre_verified=True, pre_confirmed=True,\n  pre_approved=True)\n  response = self.client.get(reverse(\n  'list_members', args=['foo@example.com', 'subscriber']),\n  {'q': 'example.com'})\n  self.assertEqual(response.status_code, 200)\n  self.assertEqual(len(response.context['members']), 2)\n  self.assertContains(response, member_1.email)\n  self.assertContains(response, member_2.email)\n  response = self.client.get(reverse(\n  'list_members', args=['foo@example.com', 'subscriber']),\n  {'q': 'member-1'})\n  self.assertEqual(response.status_code, 200)\n  self.assertEqual(len(response.context['members']), 1)\n  self.assertContains(response, member_1.email)\n  self.assertNotContains(response, member_2.email)\n  response = self.client.get(reverse(\n  'list_members', args=['foo@example.com', 'subscriber']),\n  {'q': 'not_a_member'})\n  self.assertEqual(response.status_code, 200)\n  self.assertEqual(len(response.context['members']), 0)\n  self.assertNotContains(response, member_2.email)\n  self.assertNotContains(response, member_2.email)", "target": 1, "info": "Null", "idx": 0}
{"func": "def compile(stmt, pos_args=None, named_args=None, _d=PGDialect_asyncpg()):\n   if isinstance(stmt, str):\n  return stmt, tuple(pos_args) if pos_args is not None else ()\n  else:\n  compiled = stmt.compile(dialect=_d)\n  params = compiled.construct_params(named_args)\n  return compiled.string, tuple(params[p] for p in compiled.positiontup)", "target": 0, "info": "Null", "idx": 0}
{"func": "def compile(stmt, pos_args=None, named_args=None, _d=PGDialect_asyncpg()):\n   if isinstance(stmt, str):\n  return stmt, tuple(pos_args) if pos_args is None else ()\n  else:\n  compiled = stmt.compile(dialect=_d)\n  params = compiled.construct_params(named_args)\n  return compiled.string, tuple(params[p] for p in compiled.positiontup)", "target": 1, "info": "Null", "idx": 0}
{"func": "def loadModel(self, path, stateKeys): \t\tassert len(stateKeys) > 0 \t\ttry: \t\t\tloadedState = tr.load(path) \t\texcept Exception: \t\t\tprint(\"Exception raised while loading model with tr.load(). Forcing CPU load\") \t\t\tloadedState = tr.load(path, map_location=lambda storage, loc: storage)  \t\tprint(\"Loading model from %s\" % (path)) \t\tif not \"model_state\" in loadedState: \t\t\tprint(\"Warning, no model state dictionary for this model (obsolete behaviour). Ignoring.\") \t\t\tloadedState[\"model_state\"] = None  \t\tif not self.model.onModelLoad(loadedState[\"model_state\"]): \t\t\tloaded = loadedState[\"model_state\"] \t\t\tcurrent = self.model.onModelSave() \t\t\tStr = \"Could not correclty load the model state loaded: %s vs. current: %s.\\nDiffs:\\n\" % (loaded, current) \t\t\tfor key in set(list(loaded.keys()) + list(current.keys())): \t\t\t\tif not key in current: \t\t\t\t\tStr += \"\\t- %s in loaded, not in current\" % (key) \t\t\t\t\tcontinue \t\t\t\tif not key in loaded: \t\t\t\t\tStr += \"\\t- %s in current, not in loaded\"% (key) \t\t\t\t\tcontinue \t\t\t\tif current[key] != loaded[key]: \t\t\t\t\tStr += \"\\t- current[%s]=%s. loaded[%s]=%s\" % (key, current[key], key, loaded[key]) \t\t\traise Exception(Str)  \t\tfor key in stateKeys: \t\t\tif key == \"weights\": \t\t\t\tself.doLoadWeights(loadedState) \t\t\telif key == \"optimizer\": \t\t\t\tself.doLoadOptimizer(loadedState) \t\t\telif key == \"history_dict\": \t\t\t\tself.doLoadHistoryDict(loadedState) \t\t\telif key == \"callbacks\": \t\t\t\tself.doLoadCallbacks(loadedState) \t\t\telif key == \"model_state\": \t\t\t\tpass \t\t\telse: \t\t\t\tassert False, \"Got unknown key %s\" % (key) \t\tprint(\"Finished loading model\")", "target": 0, "info": "Null", "idx": 0}
{"func": "def loadModel(self, path, stateKeys): \t\tassert len(stateKeys) > 0 \t\ttry: \t\t\tloadedState = tr.load(path) \t\texcept Exception: \t\t\tprint(\"Exception raised while loading model with tr.load(). Forcing CPU load\") \t\t\tloadedState = tr.load(path, map_location=lambda storage, loc: storage)  \t\tprint(\"Loading model from %s\" % (path)) \t\tif not \"model_state\" in loadedState: \t\t\tprint(\"Warning, no model state dictionary for this model (obsolete behaviour). Ignoring.\") \t\t\tloadedState[\"model_state\"] = None  \t\tif not self.model.onModelLoad(loadedState[\"model_state\"]): \t\t\tloaded = loadedState[\"model_state\"] \t\t\tcurrent = self.model.onModelSave() \t\t\tStr = \"Could not correclty load the model state loaded: %s vs. current: %s.\\nDiffs:\\n\" % (loaded, current) \t\t\tfor key in set(list(loaded.keys()) + list(current.keys())): \t\t\t\tif not key in current: \t\t\t\t\tStr += \"\\t- %s in loaded, not in current\" % (key) \t\t\t\t\tcontinue \t\t\t\tif not key in loaded: \t\t\t\t\tStr += \"\\t- %s in current, not in loaded\"% (key) \t\t\t\t\tcontinue \t\t\t\tif current[key] != loaded[key]: \t\t\t\t\tStr += \"\\t- current[%s]=%s. loaded[%s]=%s\" % (key, current[key], key, current[key]) \t\t\traise Exception(Str)  \t\tfor key in stateKeys: \t\t\tif key == \"weights\": \t\t\t\tself.doLoadWeights(loadedState) \t\t\telif key == \"optimizer\": \t\t\t\tself.doLoadOptimizer(loadedState) \t\t\telif key == \"history_dict\": \t\t\t\tself.doLoadHistoryDict(loadedState) \t\t\telif key == \"callbacks\": \t\t\t\tself.doLoadCallbacks(loadedState) \t\t\telif key == \"model_state\": \t\t\t\tpass \t\t\telse: \t\t\t\tassert False, \"Got unknown key %s\" % (key) \t\tprint(\"Finished loading model\")", "target": 1, "info": "Null", "idx": 0}
{"func": "def __init__(self, datasetPath, imageShape, labelShape, transforms=[\"none\"], dataDimensions=[\"rgb\"], \\ \t\tsequentialData=False):  \t\tfor data in dataDimensions: \t\t\tassert data in (\"rgb\", \"depth\", \"flownet2s\", \"semantic\", \"rgb_first_frame\"), \"Got %s\" % (data) \t\t\tif sequentialData == True: \t\t\t\tassert not data == \"semantic\", \"Semantic data is not available for sequential dataset\" \t\t\t\tassert not data == \"rgb_first_frame\", \"RGB First frame is not available for sequential dataset\"", "target": 0, "info": "Null", "idx": 0}
{"func": "def __init__(self, datasetPath, imageShape, labelShape, transforms=[\"none\"], dataDimensions=[\"rgb\"], \\ \t\tsequentialData=False):  \t\tfor data in dataDimensions: \t\t\tassert data in (\"rgb\", \"depth\", \"flownet2s\", \"semantic\", \"rgb_first_frame\"), \"Got %s\" % (data) \t\t\tif sequentialData == True: \t\t\t\tassert not data == \"semantic\", \"Semantic data is not available for sequential dataset\" \t\t\t\tassert not data == \"rgb_first_frame\", \"RGB First frame is not available for sequential dataset\"", "target": 1, "info": "Null", "idx": 0}
{"func": "def _determine_console_colour_palette() -> typing.Dict[str, str]:\n   plat = sys.platform\n  supports_color = False\n  is_a_tty = hasattr(sys.stdout, \"isatty\") and sys.stdout.isatty()\n   if plat != \"Pocket PC\":\n  if plat == \"win32\":\n  supports_color |= os.getenv(\"TERM_PROGRAM\", None) == \"mintty\"\n  supports_color |= \"ANSICON\" in os.environ\n  supports_color &= is_a_tty\n  else:\n  supports_color = is_a_tty\n   supports_color |= bool(os.getenv(\"PYCHARM_HOSTED\", \"\"))\n   palette = {\n  \"default\": \"\\033[0m\",\n  \"bright\": \"\\033[1m\",\n  \"underline\": \"\\033[4m\",\n  \"invert\": \"\\033[7m\",\n  \"red\": \"\\033[31m\",\n  \"green\": \"\\033[32m\",\n  \"yellow\": \"\\033[33m\",\n  \"blue\": \"\\033[34m\",\n  \"magenta\": \"\\033[35m\",\n  \"cyan\": \"\\033[36m\",\n  \"white\": \"\\033[37m\",\n  \"bright_red\": \"\\033[91m\",\n  \"bright_green\": \"\\033[92m\",\n  \"bright_yellow\": \"\\033[93m\",\n  \"bright_blue\": \"\\033[94m\",\n  \"bright_magenta\": \"\\033[95m\",\n  \"bright_cyan\": \"\\033[96m\",\n  \"bright_white\": \"\\033[97m\",\n  \"framed\": \"\\033[51m\",\n  \"dim\": \"\\033[2m\",\n  }\n   if not supports_color:\n  for key in list(palette.keys()):\n  palette[key] = \"\"\n   return palette", "target": 0, "info": "Null", "idx": 0}
{"func": "def _determine_console_colour_palette() -> typing.Dict[str, str]:\n   plat = sys.platform\n  supports_color = False\n  is_a_tty = hasattr(sys.stdout, \"isatty\") and sys.stdout.isatty()\n   if plat != \"Pocket PC\":\n  if plat == \"win32\":\n  supports_color |= os.getenv(\"TERM_PROGRAM\", None) == \"mintty\"\n  supports_color |= \"ANSICON\" in os.environ\n  supports_color |= is_a_tty\n  else:\n  supports_color = is_a_tty\n   supports_color |= bool(os.getenv(\"PYCHARM_HOSTED\", \"\"))\n   palette = {\n  \"default\": \"\\033[0m\",\n  \"bright\": \"\\033[1m\",\n  \"underline\": \"\\033[4m\",\n  \"invert\": \"\\033[7m\",\n  \"red\": \"\\033[31m\",\n  \"green\": \"\\033[32m\",\n  \"yellow\": \"\\033[33m\",\n  \"blue\": \"\\033[34m\",\n  \"magenta\": \"\\033[35m\",\n  \"cyan\": \"\\033[36m\",\n  \"white\": \"\\033[37m\",\n  \"bright_red\": \"\\033[91m\",\n  \"bright_green\": \"\\033[92m\",\n  \"bright_yellow\": \"\\033[93m\",\n  \"bright_blue\": \"\\033[94m\",\n  \"bright_magenta\": \"\\033[95m\",\n  \"bright_cyan\": \"\\033[96m\",\n  \"bright_white\": \"\\033[97m\",\n  \"framed\": \"\\033[51m\",\n  \"dim\": \"\\033[2m\",\n  }\n   if not supports_color:\n  for key in list(palette.keys()):\n  palette[key] = \"\"\n   return palette", "target": 1, "info": "Null", "idx": 0}
{"func": "async def handle_guild_member_update(self, gateway, payload):\n  self.dispatch(events.RAW_GUILD_MEMBER_UPDATE, payload)\n   guild_id = int(payload[\"guild_id\"])\n  guild_obj = self.fabric.state_registry.get_guild_by_id(guild_id)\n  user_id = int(payload[\"user\"][\"id\"])\n   if guild_obj is not None and user_id in guild_obj.members:\n  member_obj = guild_obj.members[user_id]\n   role_ids = payload[\"roles\"]\n  role_objs = []\n   for role_id in role_ids:\n  role_obj = self.fabric.state_registry.get_role_by_id(guild_id, role_id)\n  if role_obj is not None:\n  role_objs.append(role_obj)\n  else:\n  self.logger.warning(\n  \"ignoring unknown role %s in GUILD_MEMBER_UPDATE for member %s in guild %s\",\n  role_id,\n  user_id,\n  guild_id,\n  )\n   nick = payload[\"nick\"]\n  member_diff = self.fabric.state_registry.update_member(member_obj, role_objs, nick)\n  self.dispatch(events.GUILD_MEMBER_UPDATE, *member_diff)\n  else:\n  self.logger.warning(\"ignoring GUILD_MEMBER_UPDATE for unknown guild %s\", guild_id)", "target": 0, "info": "Null", "idx": 0}
{"func": "async def handle_guild_member_update(self, gateway, payload):\n  self.dispatch(events.RAW_GUILD_MEMBER_UPDATE, payload)\n   guild_id = int(payload[\"guild_id\"])\n  guild_obj = self.fabric.state_registry.get_guild_by_id(guild_id)\n  user_id = int(payload[\"user\"][\"id\"])\n   if guild_obj is not None and user_id in guild_obj.members:\n  member_obj = guild_obj.members[user_id]\n   role_ids = payload[\"roles\"]\n  role_objs = []\n   for role_id in role_ids:\n  role_obj = self.fabric.state_registry.get_role_by_id(guild_id, role_id)\n  if role_objs is not None:\n  role_objs.append(role_obj)\n  else:\n  self.logger.warning(\n  \"ignoring unknown role %s in GUILD_MEMBER_UPDATE for member %s in guild %s\",\n  role_id,\n  user_id,\n  guild_id,\n  )\n   nick = payload[\"nick\"]\n  member_diff = self.fabric.state_registry.update_member(member_obj, role_objs, nick)\n  self.dispatch(events.GUILD_MEMBER_UPDATE, *member_diff)\n  else:\n  self.logger.warning(\"ignoring GUILD_MEMBER_UPDATE for unknown guild %s\", guild_id)", "target": 1, "info": "Null", "idx": 0}
{"func": " def __init__(self, *points, thickness='thick', label=None, label_mask=None,\n   label_position='anticlockwise', label_scale=None,\n   draw_endpoints=True, label_endpoints=True, color=None,\n   mark=None, mark_scale=Number('0.5'),\n   locked_label=False):\n  if len(points) != 2:\n  raise TypeError('Two Points are required to create a '\n  'LineSegment. Got {} object(s) instead.'\n  .format(len(points)))\n  PointsPair.__init__(self, *points)\n  self._label = None\n  self._thickness = None\n  self._label_mask = None\n  self._label_position = None\n  self._label_scale = None\n  self._draw_endpoints = None\n  self._label_endpoints = None\n  self._locked_label = False\n  self.label = label\n  self.label_mask = label_mask\n  self.label_scale = label_scale\n  self.thickness = thickness\n  self.draw_endpoints = draw_endpoints\n  self.label_endpoints = label_endpoints\n  self.mark = mark\n  self.mark_scale = mark_scale\n  self.endpoints[1].label_position = tikz_approx_position(self.slope360)\n  self.endpoints[0].label_position = \\\n  OPPOSITE_LABEL_POSITIONS[self.endpoints[1].label_position]\n  if label_position == 'anticlockwise':\n  if self.deltax >= 0:\n  self.label_position = 'below'\n  else:\n  self.label_position = 'above'\n  elif label_position == 'clockwise':\n  if self.deltax >= 0:\n  self.label_position = 'above'\n  else:\n  self.label_position = 'below'\n  else:\n  self.label_position = label_position\n  if color is not None:\n  self.color = color\n  self._comment_designation = 'Line Segment'\n  if not isinstance(locked_label, bool):\n  raise TypeError('Expected bool type for \\'locked_label\\' keyword '\n  'argument. Found {}.'.format(type(locked_label)))\n  self._locked_label = locked_label", "target": 0, "info": "Null", "idx": 0}
{"func": "def __init__(self, *points, thickness='thick', label=None, label_mask=None,\n   label_position='anticlockwise', label_scale=None,\n   draw_endpoints=True, label_endpoints=True, color=None,\n   mark=None, mark_scale=Number('0.5'),\n   locked_label=False):\n  if len(points) != 2:\n  raise TypeError('Two Points are required to create a '\n  'LineSegment. Got {} object(s) instead.'\n  .format(len(points)))\n  PointsPair.__init__(self, *points)\n  self._label = None\n  self._thickness = None\n  self._label_mask = None\n  self._label_position = None\n  self._label_scale = None\n  self._draw_endpoints = None\n  self._label_endpoints = None\n  self._locked_label = False\n  self.label = label\n  self.label_mask = label_mask\n  self.label_scale = label_scale\n  self.thickness = thickness\n  self.draw_endpoints = draw_endpoints\n  self.label_endpoints = label_endpoints\n  self.mark = mark\n  self.mark_scale = mark_scale\n  self.endpoints[1].label_position = tikz_approx_position(self.slope360)\n  self.endpoints[0].label_position = \\\n  OPPOSITE_LABEL_POSITIONS[self.endpoints[1].label_position]\n  if label_position == 'anticlockwise':\n  if self.deltax >= 0:\n  self.label_position = 'below'\n  else:\n  self.label_position = 'above'\n  elif label_position == 'clockwise':\n  if self.deltax > 0:\n  self.label_position = 'above'\n  else:\n  self.label_position = 'below'\n  else:\n  self.label_position = label_position\n  if color is not None:\n  self.color = color\n  self._comment_designation = 'Line Segment'\n  if not isinstance(locked_label, bool):\n  raise TypeError('Expected bool type for \\'locked_label\\' keyword '\n  'argument. Found {}.'.format(type(locked_label)))\n  self._locked_label = locked_label", "target": 1, "info": "Null", "idx": 0}
{"func": "def expand_and_augment_tasklist(self, tasklist):\n   result = []\n  for task in tasklist:\n   index = task[self.meta_marker][\"_task_id\"]\n   self.task_lookup[index] = task\n   name = task[self.meta_marker].get(\"name\", None)\n  command = task[self.task_marker].get(\"command\", None)\n  if name is None:\n  if command is None:\n  raise Exception(\n  \"Task doesn't have 'name' nor 'command' key: {}\".format(task)\n  )\n  name = command\n  task[self.meta_marker][\"name\"] = command\n  if command is None:\n  command = name\n  task[self.task_marker][\"command\"] = command\n  temp = {\n  self.task_marker: copy.deepcopy(task[self.task_marker]),\n  \"vars\": task.get(\"vars\", {}),\n  }\n   task_type = task[self.meta_marker][\"type\"]\n  temp[self.task_marker][\"type\"] = task_type\n   desc_val = task.get(self.meta_marker, {}).get(\n  FRECKLES_DESC_METADATA_KEY, {}\n  )\n  msg = desc_val.get(\n  FRECKLES_DESC_SHORT_METADATA_KEY, \"{}: {}\".format(task_type, command)\n  )\n   if msg is None:\n  msg = task.get(\"doc\", {}).get(\"msg\", None)\n   if msg is not None:\n  temp[self.task_marker][\"name\"] = \"[{}] {}\".format(index, msg)\n  else:\n  temp[self.task_marker][\"name\"] = \"[{}] {}\".format(index, name)\n   if temp[self.task_marker][\"type\"] == \"ansible-role\":\n   if \"include-type\" not in temp[self.task_marker].keys():\n  temp[self.task_marker][\"include-type\"] = DEFAULT_INCLUDE_TYPE\n   temp.setdefault(\"vars\", {})[\"_task_id\"] = index\n   if \"role_path\" in temp[self.task_marker].keys():\n  path = temp[self.task_marker][\"role_path\"]\n  else:\n  path = self.context.get_role_path(command)\n  temp[self.task_marker][\"role_path\"] = path\n   self.add_role_to_addition_files(command, path=path)\n   if temp[self.task_marker][\"type\"] == \"ansible-tasklist\":\n  include_type = temp[self.task_marker].get(\"include-type\", None)\n  if include_type is None:\n  include_type = DEFAULT_INCLUDE_TYPE\n  temp[self.task_marker][\"include-type\"] = include_type\n   temp.setdefault(\"vars\", {})[\"_task_id\"] = index\n   tasklist_var = temp[self.task_marker].get(\"tasklist_var\", None)\n  if tasklist_var is None:\n  tasklist_var = \"tasklist_{}\".format(command)\n   tasklist_var = tasklist_var.replace(\"-\", \"_\")\n  tasklist_var = tasklist_var.replace(\".\", \"_\")\n  temp[self.task_marker][\"tasklist_var\"] = tasklist_var\n   self.add_tasklist_to_additional_files(command, tasklist_var)\n  resources = task[self.meta_marker].get(\"resources\", {})\n   for res_type, res_urls in resources.items():\n  if res_type == \"ansible-module\":\n  continue\n  if res_type not in [\"ansible-role\", \"ansible-tasklist\"]:\n   raise FrklException(\n  msg=\"Invalid resource type '{}' for adapter 'nsbl'.\",\n  solution=\"Check your task description for this task:\\n\\n{}\\n\\nChange the 'resources' value to only contain 'roles' and 'ansible-tasklists' keys.\",\n  )\n   if res_type == \"ansible-role\":\n  roles = res_urls\n  if isinstance(roles, string_types):\n  roles = [roles]\n   for role in roles:\n  self.add_role_to_addition_files(role, path=None)\n   elif res_type == \"ansible-tasklist\":\n   tasklists = res_urls\n  if isinstance(tasklists, string_types):\n  tasklists = [tasklists]\n   for tl_name in tasklists:\n   tasklist_var = \"tasklist_{}\".format(tl_name)\n   tasklist_var = tasklist_var.replace(\"-\", \"_\")\n  tasklist_var = tasklist_var.replace(\".\", \"_\")\n  task[self.task_marker][\"tasklist_var\"] = tasklist_var\n  self.add_tasklist_to_additional_files(tl_name, tasklist_var)\n   result.append(temp)\n   if \"register\" in task[self.meta_marker].keys():\n  if (\n  \"register\" in temp[self.task_marker].keys()\n  and temp[self.task_marker][\"register\"]\n  != task[self.meta_marker][\"register\"]\n  ):\n  t = copy.deepcopy(task)\n  t[self.meta_marker].pop(\"secret_vars\", None)\n  t[self.meta_marker].pop(\"_task_id\", None)\n  raise FrklException(\n  msg=\"Different 'register' values specified in both '{}' and '{}' subkeys\".format(", "target": 0, "info": "Null", "idx": 0}
{"func": "def expand_and_augment_tasklist(self, tasklist):\n   result = []\n  for task in tasklist:\n   index = task[self.meta_marker][\"_task_id\"]\n   self.task_lookup[index] = task\n   name = task[self.meta_marker].get(\"name\", None)\n  command = task[self.task_marker].get(\"command\", None)\n  if name is None:\n  if command is None:\n  raise Exception(\n  \"Task doesn't have 'name' nor 'command' key: {}\".format(task)\n  )\n  name = command\n  task[self.meta_marker][\"name\"] = command\n  if command is None:\n  command = name\n  task[self.task_marker][\"command\"] = command\n  temp = {\n  self.task_marker: copy.deepcopy(task[self.task_marker]),\n  \"vars\": task.get(\"vars\", {}),\n  }\n   task_type = task[self.meta_marker][\"type\"]\n  temp[self.task_marker][\"type\"] = task_type\n   desc_val = task.get(self.meta_marker, {}).get(\n  FRECKLES_DESC_METADATA_KEY, {}\n  )\n  msg = desc_val.get(\n  FRECKLES_DESC_SHORT_METADATA_KEY, \"{}: {}\".format(task_type, command)\n  )\n   if msg is None:\n  msg = task.get(\"doc\", {}).get(\"msg\", None)\n   if msg is not None:\n  temp[self.task_marker][\"name\"] = \"[{}] {}\".format(index, msg)\n  else:\n  temp[self.task_marker][\"name\"] = \"[{}] {}\".format(index, name)\n   if temp[self.task_marker][\"type\"] == \"ansible-role\":\n   if \"include-type\" not in temp[self.task_marker].keys():\n  temp[self.task_marker][\"include-type\"] = DEFAULT_INCLUDE_TYPE\n   temp.setdefault(\"vars\", {})[\"_task_id\"] = index\n   if \"role_path\" in temp[self.task_marker].keys():\n  path = temp[self.task_marker][\"role_path\"]\n  else:\n  path = self.context.get_role_path(command)\n  temp[self.task_marker][\"role_path\"] = path\n   self.add_role_to_addition_files(command, path=path)\n   if temp[self.task_marker][\"type\"] == \"ansible-tasklist\":\n  include_type = temp[self.task_marker].get(\"include-type\", None)\n  if include_type is None:\n  include_type = DEFAULT_INCLUDE_TYPE\n  temp[self.task_marker][\"include-type\"] = include_type\n   temp.setdefault(\"vars\", {})[\"_task_id\"] = index\n   tasklist_var = temp[self.task_marker].get(\"tasklist_var\", None)\n  if tasklist_var is None:\n  tasklist_var = \"tasklist_{}\".format(command)\n   tasklist_var = tasklist_var.replace(\"-\", \"_\")\n  tasklist_var = tasklist_var.replace(\".\", \"_\")\n  temp[self.task_marker][\"tasklist_var\"] = tasklist_var\n   self.add_tasklist_to_additional_files(command, tasklist_var)\n  resources = task[self.meta_marker].get(\"resources\", {})\n   for res_type, res_urls in resources.items():\n  if res_type == \"ansible-module\":\n  continue\n  if res_type not in [\"ansible-role\", \"ansible-tasklist\"]:\n   raise FrklException(\n  msg=\"Invalid resource type '{}' for adapter 'nsbl'.\",\n  solution=\"Check your task description for this task:\\n\\n{}\\n\\nChange the 'resources' value to only contain 'roles' and 'ansible-tasklists' keys.\",\n  )\n   if res_type == \"ansible-role\":\n  roles = res_urls\n  if isinstance(roles, string_types):\n  roles = [roles]\n   for role in roles:\n  self.add_role_to_addition_files(role, path=None)\n   elif res_type == \"ansible-tasklist\":\n   tasklists = res_urls\n  if isinstance(tasklist, string_types):\n  tasklists = [tasklists]\n   for tl_name in tasklists:\n   tasklist_var = \"tasklist_{}\".format(tl_name)\n   tasklist_var = tasklist_var.replace(\"-\", \"_\")\n  tasklist_var = tasklist_var.replace(\".\", \"_\")\n  task[self.task_marker][\"tasklist_var\"] = tasklist_var\n  self.add_tasklist_to_additional_files(tl_name, tasklist_var)\n   result.append(temp)\n   if \"register\" in task[self.meta_marker].keys():\n  if (\n  \"register\" in temp[self.task_marker].keys()\n  and temp[self.task_marker][\"register\"]\n  != task[self.meta_marker][\"register\"]\n  ):\n  t = copy.deepcopy(task)\n  t[self.meta_marker].pop(\"secret_vars\", None)\n  t[self.meta_marker].pop(\"_task_id\", None)\n  raise FrklException(\n  msg=\"Different 'register' values specified in both '{}' and '{}' subkeys\".format(", "target": 1, "info": "Null", "idx": 0}
{"func": "def __init__(self, config, mqtt_client=None, logger=None):\n   AbstractMicroservice.__init__(self, config, \"data-preparation\", mqtt_client=mqtt_client, logger=logger)\n   self._purges = []\n  self._processes = []\n  self._data_points = []\n   self._update_cycle = int(self._config[\"update_cycle\"])\n  self._no_data_behavior = NoDataBehavior.get_enum(str(self._config[\"no_data_behavior\"]))\n   self._worker = Worker(self._config[\"number_worker\"], self._logger)\n  self._do_loop_thread = threading.Thread(target=self._do_loop)\n   _config_methods = {}\n  for m in self._config[\"methods\"]:\n  key = m[\"name\"]\n  if key in _config_methods:\n  self._logger.error(\"DataPointManager - method name must be unique ({}).\".format(key))\n  raise ValueError(\"DataPointManager - method name must be unique ({}).\".format(key))\n  _config_methods[key] = m\n   for config_data_point in self._config[\"datapoints\"]:\n  dp = DataPoint(config_data_point, _config_methods, self._mqtt_client, self._logger,\n self._no_data_behavior)\n  self._purges.append(dp.purge_old_values)\n  for method in dp.methods:\n  process = method.process\n  cost = method.execution_points_estimation()\n  self._logger.info(\"DataPointManager - adding process '{}' with cost '{}'.\".\n    format(process.__name__, cost))\n  self._processes.append((process, cost))\n  self._data_points.append(dp)\n   self._processes.sort(key=lambda tup: tup[1], reverse=True)", "target": 0, "info": "Null", "idx": 0}
{"func": " def __init__(self, config, mqtt_client=None, logger=None):\n   AbstractMicroservice.__init__(self, config, \"data-preparation\", mqtt_client=mqtt_client, logger=logger)\n   self._purges = []\n  self._processes = []\n  self._data_points = []\n   self._update_cycle = int(self._config[\"update_cycle\"])\n  self._no_data_behavior = NoDataBehavior.get_enum(str(self._config[\"no_data_behavior\"]))\n   self._worker = Worker(self._config[\"number_worker\"], self._logger)\n  self._do_loop_thread = threading.Thread(target=self._do_loop)\n   _config_methods = {}\n  for m in self._config[\"methods\"]:\n  key = m[\"name\"]\n  if key in _config_methods:\n  self._logger.error(\"DataPointManager - method name must be unique ({}).\".format(key))\n  raise ValueError(\"DataPointManager - method name must be unique ({}).\".format(key))\n  _config_methods[key] = m\n   for config_data_point in self._config[\"datapoints\"]:\n  dp = DataPoint(config_data_point, _config_methods, self._logger, self._mqtt_client,\n self._no_data_behavior)\n  self._purges.append(dp.purge_old_values)\n  for method in dp.methods:\n  process = method.process\n  cost = method.execution_points_estimation()\n  self._logger.info(\"DataPointManager - adding process '{}' with cost '{}'.\".\n    format(process.__name__, cost))\n  self._processes.append((process, cost))\n  self._data_points.append(dp)\n   self._processes.sort(key=lambda tup: tup[1], reverse=True)", "target": 1, "info": "Null", "idx": 0}
{"func": "def create_mesh(structure, path):\n  from blessings import Terminal\n   t = Terminal()\n   structure_geo = '{}/structure.geo'.format(path)\n  mesh_geo = '{}/mesh.geo'.format(path)\n  points_geo = '{}/points.geo'.format(path)\n   if os.path.exists(structure_geo):\n  clear_geom = input(\"Clear geometry? [y/N] \").lower().startswith('y')\n  else:\n  clear_geom = True\n   if clear_geom:\n  init_geom(structure_geo, structure)\n  edit_geom = True\n  else:\n  edit_geom = not input(\"Edit geometry? [Y/n] \").lower().startswith('n')\n   device_ok, contact_ok = False, False\n  while not device_ok or not contact_ok:\n  if edit_geom:\n  print(t.yellow('In gmsh, please specify:\\n'\n '- The device volume using '\n 'a single Physical Volume\\n'\n '  with the identifier \"device\".\\n'\n '- At least one contact using '\n 'Physical Points, Lines, Surfaces or Volumes\\n'\n '  with identifiers like \"contact:<name>\".'))\n  bail = input(\"Press <enter> to open gmsh, q <enter> to abort: \")\n  if bail.lower().startswith('q'):\n  exit()\n  geo.edit_gui(structure_geo)\n  edit_geom = True\n  device_ok, contact_ok = False, False\n  device_volume, contacts = find_device(structure_geo)\n  if len(device_volume) == 1:\n  print(t.green(\"Found the device!\"))\n  device_ok = True\n  elif len(device_volume) == 0:\n  print(t.red(\"No device found in gmsh structure!\"))\n  else:\n  print(t.red(\"The device consists of multiple volumes, \"\n  \"this is not supported, use BooleanUnion!\"))\n  if len(contacts) == 0:\n  print(t.red(\"No contacts specified, your device is floating!\"))\n  else:\n  print(t.green(\"Found contacts: \" +\n    ', '.join([contact.name.replace('contact:', '')\n   for contact in contacts])))\n  contact_ok = True\n   idx0 = geo.find_point_idx(mesh_geo, \"structure.geo\")\n   with geo.Geo(points_geo) as g:\n  for idx, coord in enumerate(iterate_density_points(structure), idx0):\n  g.point(idx, coord)\n  g.physical_point('density', geo.range(idx0, idx))\n  g.point_in_volume(geo.range(idx0, idx), device_volume[0])\n   delta = np.max([np.abs(element.dr)\n  for element in structure.elements])\n  min_size = 1 * c.nm\n   with geo.Geo(mesh_geo) as g:\n  g.include(\"structure.geo\")\n  g.include(\"points.geo\")\n  g.attractor(1, geo.range(idx0, idx))\n  g.threshold(2,\n  field=1,\n  dist=(0, 2 * min_size),\n  lc=(1.1 * delta, min_size))\n  g.min(3, 2)\n  g.background(3)\n   print(t.green('Succesfully created geometry in \"{}\"'.format(mesh_geo)))\n   geo.generate_mesh(mesh_geo)\n  mesh_msh = mesh_geo.replace('geo', 'msh')\n  return mesh_msh", "target": 0, "info": "Null", "idx": 0}
{"func": "def create_mesh(structure, path):\n  from blessings import Terminal\n   t = Terminal()\n   structure_geo = '{}/structure.geo'.format(path)\n  mesh_geo = '{}/mesh.geo'.format(path)\n  points_geo = '{}/points.geo'.format(path)\n   if os.path.exists(structure_geo):\n  clear_geom = input(\"Clear geometry? [y/N] \").lower().startswith('y')\n  else:\n  clear_geom = True\n   if clear_geom:\n  init_geom(structure_geo, structure)\n  edit_geom = True\n  else:\n  edit_geom = not input(\"Edit geometry? [Y/n] \").lower().startswith('n')\n   device_ok, contact_ok = False, False\n  while not device_ok or not contact_ok:\n  if edit_geom:\n  print(t.yellow('In gmsh, please specify:\\n'\n '- The device volume using '\n 'a single Physical Volume\\n'\n '  with the identifier \"device\".\\n'\n '- At least one contact using '\n 'Physical Points, Lines, Surfaces or Volumes\\n'\n '  with identifiers like \"contact:<name>\".'))\n  bail = input(\"Press <enter> to open gmsh, q <enter> to abort: \")\n  if bail.lower().startswith('q'):\n  exit()\n  geo.edit_gui(structure_geo)\n  edit_geom = True\n  device_ok, contact_ok = False, False\n  device_volume, contacts = find_device(structure_geo)\n  if len(device_volume) == 1:\n  print(t.green(\"Found the device!\"))\n  device_ok = True\n  elif len(device_volume) == 0:\n  print(t.red(\"No device found in gmsh structure!\"))\n  else:\n  print(t.red(\"The device consists of multiple volumes, \"\n  \"this is not supported, use BooleanUnion!\"))\n  if len(contacts) == 0:\n  print(t.red(\"No contacts specified, your device is floating!\"))\n  else:\n  print(t.green(\"Found contacts: \" +\n    ', '.join([contact.name.replace('contact:', '')\n   for contact in contacts])))\n  contact_ok = True\n   idx0 = geo.find_point_idx(mesh_geo, \"structure.geo\")\n   with geo.Geo(points_geo) as g:\n  for idx, coord in enumerate(iterate_density_points(structure), idx0):\n  g.point(idx, coord)\n  g.physical_point('density', geo.range(idx0, idx))\n  g.point_in_volume(geo.range(idx0, idx), device_volume[0])\n   delta = np.max([np.abs(element.dr)\n  for element in structure.elements])\n  min_size = 1 * c.nm\n   with geo.Geo(points_geo) as g:\n  g.include(\"structure.geo\")\n  g.include(\"points.geo\")\n  g.attractor(1, geo.range(idx0, idx))\n  g.threshold(2,\n  field=1,\n  dist=(0, 2 * min_size),\n  lc=(1.1 * delta, min_size))\n  g.min(3, 2)\n  g.background(3)\n   print(t.green('Succesfully created geometry in \"{}\"'.format(mesh_geo)))\n   geo.generate_mesh(mesh_geo)\n  mesh_msh = mesh_geo.replace('geo', 'msh')\n  return mesh_msh", "target": 1, "info": "Null", "idx": 0}
{"func": "async def page_to_atom(browser, page_id, root_dir, media_dir, media_url_slug):\n  tasks = []\n  sem = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n  async with ClientSession() as session:\n   fb_url = 'https://www.facebook.com/%s/posts' % page_id\n  browser.open(fb_url)\n  page = browser.get_current_page()\n   fg = FeedGenerator()\n  fg.generator(\"Fb2Feed\", version=__version__)\n  fg.id(fb_url)\n  fg.title(page.title.text.split('-')[0].strip().title())\n  updated = False\n   for wrapper in page.select('div.userContentWrapper'):\n  abbr = wrapper.select('abbr')[0]\n   post_url = browser.absolute_url(abbr.parent.attrs[\"href\"])\n  post_id = abbr.attrs['data-utime']\n   timestamp = datetime.datetime.utcfromtimestamp(int(post_id))\n  timestamp = timestamp.replace(tzinfo=pytz.utc)\n   if not updated:\n  fg.updated(timestamp)\n  updated = True\n   user_content = wrapper.select('div.userContent')\n  txt = ''.join([ div.text for div in user_content ])\n  if not txt:\n  continue\n   extra_markup = []\n  for link in wrapper.find_all('a', attrs={'rel': 'theater'}):\n  video_url = None\n  try:\n  img_url = link.attrs['data-ploi']\n  except KeyError:\n  img_url = None\n  try:\n  video_url = browser.absolute_url(link.attrs['href'])\n  except KeyError:\n  continue\n  else:\n  if video_url.find(\"video\") == -1:\n  video_url = None\n   if img_url:\n  tasks.append(asyncio.ensure_future(bound_fetch(sem, img_url, session, media_dir)))\n  extra_markup.append('<img src=\"%s%s\"/>' % (media_url_slug, url_filename(img_url)))\n  elif video_url:\n  tasks.append(asyncio.ensure_future(video_bound_fetch(sem, video_url, media_dir)))\n  extra_markup.append('<video controls src=\"%s%s.mp4\"/>' % (media_url_slug, video_id(video_url)))\n   entry_content = txt + ''.join(extra_markup)\n   fe = fg.add_entry()\n  fe.author(name=page_id, email=\"%s.facebook.no-reply@fb2feed.org\" % page_id)\n  fe.id(post_id)\n  fe.link(href=post_url, rel=\"alternate\")\n  fe.published(timestamp)\n  fe.updated(timestamp)\n  fe.title(' '.join(txt.split(' ')[:15]))\n  fe.content(entry_content, type=\"html\")\n   await asyncio.gather(*tasks)\n  fg.atom_file(os.path.join(root_dir, '%s.atom.xml' % page_id))", "target": 0, "info": "Null", "idx": 0}
{"func": "async def page_to_atom(browser, page_id, root_dir, media_dir, media_url_slug):\n  tasks = []\n  sem = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n  async with ClientSession() as session:\n   fb_url = 'https://www.facebook.com/%s/posts' % page_id\n  browser.open(fb_url)\n  page = browser.get_current_page()\n   fg = FeedGenerator()\n  fg.generator(\"Fb2Feed\", version=__version__)\n  fg.id(fb_url)\n  fg.title(page.title.text.split('-')[0].strip().title())\n  updated = False\n   for wrapper in page.select('div.userContentWrapper'):\n  abbr = wrapper.select('abbr')[0]\n   post_url = browser.absolute_url(abbr.parent.attrs[\"href\"])\n  post_id = abbr.attrs['data-utime']\n   timestamp = datetime.datetime.utcfromtimestamp(int(post_id))\n  timestamp = timestamp.replace(tzinfo=pytz.utc)\n   if not updated:\n  fg.updated(timestamp)\n  updated = True\n   user_content = wrapper.select('div.userContent')\n  txt = ''.join([ div.text for div in user_content ])\n  if not txt:\n  continue\n   extra_markup = []\n  for link in wrapper.find_all('a', attrs={'rel': 'theater'}):\n  video_url = None\n  try:\n  img_url = link.attrs['data-ploi']\n  except KeyError:\n  img_url = None\n  try:\n  video_url = browser.absolute_url(link.attrs['href'])\n  except KeyError:\n  continue\n  else:\n  if video_url.find(\"video\") == -1:\n  video_url = None\n   if img_url:\n  tasks.append(asyncio.ensure_future(bound_fetch(sem, img_url, session, media_dir)))\n  extra_markup.append('<img src=\"%s%s\"/>' % (media_url_slug, url_filename(img_url)))\n  elif video_url:\n  tasks.append(asyncio.ensure_future(video_bound_fetch(sem, video_url, media_dir)))\n  extra_markup.append('<video controls src=\"%s%s.mp4\"/>' % (media_url_slug, video_id(video_url)))\n   entry_content = txt + ''.join(extra_markup)\n   fe = fg.add_entry()\n  fe.author(name=page_id, email=\"%s.facebook.no-reply@fb2feed.org\" % page_id)\n  fe.id(post_url)\n  fe.link(href=post_url, rel=\"alternate\")\n  fe.published(timestamp)\n  fe.updated(timestamp)\n  fe.title(' '.join(txt.split(' ')[:15]))\n  fe.content(entry_content, type=\"html\")\n   await asyncio.gather(*tasks)\n  fg.atom_file(os.path.join(root_dir, '%s.atom.xml' % page_id))", "target": 1, "info": "Null", "idx": 0}
{"func": " def __init__(self, text: str):\n  self.text = text\n  self.items = set()\n  count: Counter = Counter()\n  for key, val in self.parse(text):\n  self.items.add(val)\n  count[key] += 1\n  if key == \"el\":\n  self.el = val\n   self.score = Score(count[\"id\"], count[\"class\"], count[\"el\"])", "target": 0, "info": "Null", "idx": 0}
{"func": "def __init__(self, text: str):\n  self.text = text\n  self.items = set()\n  count: Counter = Counter()\n  for key, val in self.parse(text):\n  self.items.add(val)\n  count[key] += 1\n  if key == \"el\":\n  self.el = key\n   self.score = Score(count[\"id\"], count[\"class\"], count[\"el\"])", "target": 1, "info": "Null", "idx": 0}
{"func": "def main(argv=sys.argv[1:]):\n  args = build_argparse().parse_args(argv)\n  print(\"Collecting files\", flush=True)\n  with TarballBuilder() as builder:\n  try:\n  builder.add_gitcommit()\n  except FileNotFoundError:\n  print(\"Warning: No git binary\", file=sys.stderr, flush=True)\n  if os.path.exists('_salt'):\n  builder.add_saltdir('_salt')\n  for a in args.artifact:\n  builder.add_artifact(a, a)\n  if args.include_source is not ...:\n  builder.add_source(args.include_source)\n    print(\"Uploading to server\", flush=True)\n  rv = 0\n  minions = set()\n  for event, data in upload(\n  args.server,\n  args.token,\n  builder.buffer,\n  args.project,\n  args.deployment,\n  highstate=args.highstate,\n  sslverify=args.sslverify\n  ):\n  if event in EVENT_PRINTERS:\n  rv = rv or EVENT_PRINTERS[event](data)\n  elif 'msg' in data:\n  print(data['msg'], flush=True)\n  else:\n  print(event, pprint.pformat(data), flush=True)\n   if event == 'highstate-start':\n  minions |= set(data['minions'])\n  elif event == 'highstate':\n  minions.discard(data['minion'])\n   if minions:\n  print(\n  hues.huestr(\n  \"Unreporting minions: {}\".format(', '.join(sorted(minions)))\n  ).red.colorized\n  )\n   return rv", "target": 0, "info": "Null", "idx": 0}
{"func": "def main(argv=sys.argv[1:]):\n  args = build_argparse().parse_args(argv)\n  print(\"Collecting files\", flush=True)\n  with TarballBuilder() as builder:\n  try:\n  builder.add_gitcommit()\n  except FileNotFoundError:\n  print(\"Warning: No git binary\", file=sys.stderr, flush=True)\n  if os.path.exists('_salt'):\n  builder.add_saltdir('_salt')\n  for a in args.artifact:\n  builder.add_artifact(a, a)\n  if args.include_source is not ...:\n  builder.add_source(args.include_source)\n    print(\"Uploading to server\", flush=True)\n  rv = 0\n  minions = set()\n  for event, data in upload(\n  args.server,\n  args.token,\n  builder.buffer,\n  args.project,\n  args.deployment,\n  highstate=args.highstate,\n  sslverify=args.sslverify\n  ):\n  if event in EVENT_PRINTERS:\n  rv = rv or EVENT_PRINTERS[event](data)\n  elif 'msg' in data:\n  print(data['msg'], flush=True)\n  else:\n  print(event, pprint.pformat(data), flush=True)\n   if event == 'highstate-start':\n  minions += set(data['minions'])\n  elif event == 'highstate':\n  minions.discard(data['minion'])\n   if minions:\n  print(\n  hues.huestr(\n  \"Unreporting minions: {}\".format(', '.join(sorted(minions)))\n  ).red.colorized\n  )\n   return rv", "target": 1, "info": "Null", "idx": 0}
{"func": "def __init__(self, code, db=os.getenv(\"HOME\") + '/.inmetdb.hdf',\n   local=False):\n  if code in inmet.sites.index.values:\n  self.code = code\n  self.cod_OMM = inmet.sites.loc[code].cod_OMM\n  self.inicio_operacao = inmet.sites.loc[code].inicio_operacao\n  self.lat = inmet.sites.loc[code].lat\n  self.lon = inmet.sites.loc[code].lon\n  self.alt = inmet.sites.loc[code].alt\n  self.dados = get_from_ldb(code, local, db)", "target": 0, "info": "Null", "idx": 0}
{"func": "def __init__(self, code, db=os.getenv(\"HOME\") + '/.inmetdb.hdf',\n   local=False):\n  if code in inmet.sites.index.values:\n  self.code = code\n  self.cod_OMM = inmet.sites.loc[code].cod_OMM\n  self.inicio_operacao = inmet.sites.loc[code].inicio_operacao\n  self.lat = inmet.sites.loc[code].lat\n  self.lon = inmet.sites.loc[code].lon\n  self.alt = inmet.sites.loc[code].alt\n  self.dados = get_from_ldb(code, db, local)", "target": 1, "info": "Null", "idx": 0}
{"func": "def _validate_range(self, start, minimum, maximum):\n  if maximum <= minimum:\n  raise ValueError(\n  _(\"The maximum can not be less than the minimum.\"))\n  if start < minimum or start > maximum:\n  raise ValueError(\n  _(\"The start must be between the minimum and maximum!\"))\n  rnrange = maximum - minimum\n  return rnrange", "target": 0, "info": "Null", "idx": 0}
{"func": " def _validate_range(self, start, minimum, maximum):\n  if maximum <= minimum:\n  raise ValueError(\n  _(\"The maximum can not be less than the minimum.\"))\n  if start < minimum or start >= maximum:\n  raise ValueError(\n  _(\"The start must be between the minimum and maximum!\"))\n  rnrange = maximum - minimum\n  return rnrange", "target": 1, "info": "Null", "idx": 0}
{"func": "def change_owner(self, uuid, new_owner):\n  assert(uuid and new_owner)\n   if uuid in self._graph.keys() and self._graph[uuid].owner != self._id:\n  for node in self._node_deps(node=uuid):\n  self.change_owner(uuid=node,new_owner=new_owner)\n  right_command = RepRightCommand(\n  owner=self._id,\n  buffer={\n  'uuid': uuid,\n  'owner': new_owner}\n  )\n  right_command.store(self._graph)\n  right_command.push(self._net_client._publish)", "target": 0, "info": "Null", "idx": 0}
{"func": "def change_owner(self, uuid, new_owner):\n  assert(uuid and new_owner)\n   if uuid in self._graph.keys() and self._graph[uuid].owner == self._id:\n  for node in self._node_deps(node=uuid):\n  self.change_owner(uuid=node,new_owner=new_owner)\n  right_command = RepRightCommand(\n  owner=self._id,\n  buffer={\n  'uuid': uuid,\n  'owner': new_owner}\n  )\n  right_command.store(self._graph)\n  right_command.push(self._net_client._publish)", "target": 1, "info": "Null", "idx": 0}
{"func": "def savitzky_golay(arr, window_size, order, deriv=0, rate=1, interpolate=True):\n  from math import factorial\n  from numpy import array, abs, mat, linalg, concatenate, convolve\n  from pandas import Series\n  try:\n  window_size = abs(int(window_size))\n  order = abs(int(order))\n  except ValueError:\n  raise ValueError(\"window_size and order have to be of type int\")\n  if window_size % 2 != 1 or window_size < 1:\n  raise TypeError(\"window_size size must be a positive odd number\")\n  if window_size < order + 2:\n  raise TypeError(\"window_size is too small for the polynomial order\")\n  order_range = range(order + 1)\n  half_window = (window_size - 1) // 2\n  if interpolate:\n  ser = Series(arr).interpolate(limit=window_size)\n  y = array(ser)\n  else:\n  y = array(arr)\n  b = mat([[k**i for i in order_range] for k in range(-half_window, half_window + 1)])\n  m = linalg.pinv(b).A[deriv] * rate**deriv * factorial(deriv)\n  firstvals = y[0] - abs(y[1:half_window + 1][::-1] - y[0])\n  lastvals = y[-1] + abs(y[-half_window - 1:-1][::-1] - y[-1])\n  y = concatenate((firstvals, y, lastvals))\n   return convolve(m[::-1], y, mode='valid')", "target": 0, "info": "Null", "idx": 0}
{"func": "def savitzky_golay(arr, window_size, order, deriv=0, rate=1, interpolate=True):\n  from math import factorial\n  from numpy import array, abs, mat, linalg, concatenate, convolve\n  from pandas import Series\n  try:\n  window_size = abs(int(window_size))\n  order = abs(int(order))\n  except ValueError:\n  raise ValueError(\"window_size and order have to be of type int\")\n  if window_size % 2 != 1 or window_size < 1:\n  raise TypeError(\"window_size size must be a positive odd number\")\n  if window_size < order + 2:\n  raise TypeError(\"window_size is too small for the polynomial order\")\n  order_range = range(order + 1)\n  half_window = (window_size - 1) // 2\n  if interpolate:\n  ser = Series(arr).interpolate(limit=half_window)\n  y = array(ser)\n  else:\n  y = array(arr)\n  b = mat([[k**i for i in order_range] for k in range(-half_window, half_window + 1)])\n  m = linalg.pinv(b).A[deriv] * rate**deriv * factorial(deriv)\n  firstvals = y[0] - abs(y[1:half_window + 1][::-1] - y[0])\n  lastvals = y[-1] + abs(y[-half_window - 1:-1][::-1] - y[-1])\n  y = concatenate((firstvals, y, lastvals))\n   return convolve(m[::-1], y, mode='valid')", "target": 1, "info": "Null", "idx": 0}
{"func": " def update_scrubber(self, current, duration):\n  if current is None or duration is None:\n  self.song_duration_label.set_text('-:--')\n  self.song_progress_label.set_text('-:--')\n  self.song_scrubber.set_value(0)\n  return\n   current = current or 0\n  percent_complete = current / duration * 100\n   if not self.editing:\n  self.song_scrubber.set_value(percent_complete)\n  self.song_duration_label.set_text(util.format_song_duration(duration))\n  self.song_progress_label.set_text(\n  util.format_song_duration(math.floor(current)))", "target": 0, "info": "Null", "idx": 0}
{"func": "def update_scrubber(self, current, duration):\n  if current is None and duration is None:\n  self.song_duration_label.set_text('-:--')\n  self.song_progress_label.set_text('-:--')\n  self.song_scrubber.set_value(0)\n  return\n   current = current or 0\n  percent_complete = current / duration * 100\n   if not self.editing:\n  self.song_scrubber.set_value(percent_complete)\n  self.song_duration_label.set_text(util.format_song_duration(duration))\n  self.song_progress_label.set_text(\n  util.format_song_duration(math.floor(current)))", "target": 1, "info": "Null", "idx": 0}
{"func": " def __init__(self, graph):\n   nodes = graph.nodes\n  edges = graph.edges\n  self.n_node = len(nodes)\n  nnz = len(edges)\n  assert(len(nodes.columns) >= 1)\n  if len(nodes.columns) == 1:\n  nodes['labeled'] = np.zeros(len(nodes), np.bool_)\n   assert(len(edges.columns) >= 2)\n  if len(edges.columns) == 2:\n  assert('!i' in edges.columns and '!j' in edges.columns)\n  edges['labeled'] = np.zeros(len(edges), np.bool_)\n  self.node_type = node_type = rowtype(nodes, exclude=['!i'])\n  self.node = umempty(len(nodes), dtype=node_type)\n  self.node[:] = list(zip(*[nodes[key] for key in node_type.names]))\n  self.degree = degree = umzeros(self.n_node, dtype=np.float32)\n  edge_label_type = rowtype(edges, exclude=['!i', '!j', '!w'])\n  if '!w' in edges.columns:\n  self.weighted = True\n  edge_type = np.dtype([('weight', np.float32),\n    ('label', edge_label_type)], align=True)\n  self.edge_type = edge_type\n  np.add.at(degree, edges['!i'], edges['!w'])\n  np.add.at(degree, edges['!j'], edges['!w'])\n   if edge_label_type.itemsize != 0:\n  labels = zip(*[edges[t] for t in edge_label_type.names])\n  else:\n  labels = [None] * len(edges)\n  edge_aos = np.fromiter(zip(edges['!w'], labels), dtype=edge_type,\n count=nnz)\n  else:\n  self.weighted = False\n  self.edge_type = edge_type = edge_label_type\n  np.add.at(degree, edges['!i'], 1.0)\n  np.add.at(degree, edges['!j'], 1.0)\n  edge_aos = np.fromiter(zip(*[edges[t] for t in edge_type.names]),\n dtype=edge_type, count=nnz)\n  degree[degree == 0] = 1.0\n  indices = np.empty((4, nnz * 2), dtype=np.uint32, order='C')\n  i, j, up, lf = indices\n  i[:nnz] = edges['!i']\n  j[:nnz] = edges['!j']\n  indices[[0, 1], nnz:] = indices[[1, 0], :nnz]\n  up[:] = i - i % 8\n  lf[:] = j - j % 8\n   perm = np.lexsort(indices, axis=0)\n  indices[:, :] = indices[:, perm]\n  self.edge_aos = umempty(nnz * 2, edge_type)\n  self.edge_aos[:] = edge_aos[perm % nnz]\n   diff = np.empty(nnz * 2)\n  diff[1:] = (up[:-1] != up[1:]) | (lf[:-1] != lf[1:])\n  diff[:1] = True\n  oct_offset = np.flatnonzero(diff)\n  self.n_octile = len(oct_offset)\n   nzmasks = np.bitwise_or.reduceat(\n  1 << (i - up + (j - lf) * 8).astype(np.uint64), oct_offset)\n  nzmasks_r = np.bitwise_or.reduceat(\n  1 << (j - lf + (i - up) * 8).astype(np.uint64), oct_offset)\n   self.octiles = octiles = umempty(self.n_octile, self.Octile.dtype)\n  octiles[:] = list(\n  zip(int(self.edge_aos.base) + oct_offset * edge_type.itemsize,\n  nzmasks,\n  nzmasks_r,\n  up[oct_offset],\n  lf[oct_offset])\n  )", "target": 0, "info": "Null", "idx": 0}
{"func": "def __init__(self, graph):\n   nodes = graph.nodes\n  edges = graph.edges\n  self.n_node = len(nodes)\n  nnz = len(edges)\n  assert(len(edges.columns) >= 1)\n  if len(nodes.columns) == 1:\n  nodes['labeled'] = np.zeros(len(nodes), np.bool_)\n   assert(len(edges.columns) >= 2)\n  if len(edges.columns) == 2:\n  assert('!i' in edges.columns and '!j' in edges.columns)\n  edges['labeled'] = np.zeros(len(edges), np.bool_)\n  self.node_type = node_type = rowtype(nodes, exclude=['!i'])\n  self.node = umempty(len(nodes), dtype=node_type)\n  self.node[:] = list(zip(*[nodes[key] for key in node_type.names]))\n  self.degree = degree = umzeros(self.n_node, dtype=np.float32)\n  edge_label_type = rowtype(edges, exclude=['!i', '!j', '!w'])\n  if '!w' in edges.columns:\n  self.weighted = True\n  edge_type = np.dtype([('weight', np.float32),\n    ('label', edge_label_type)], align=True)\n  self.edge_type = edge_type\n  np.add.at(degree, edges['!i'], edges['!w'])\n  np.add.at(degree, edges['!j'], edges['!w'])\n   if edge_label_type.itemsize != 0:\n  labels = zip(*[edges[t] for t in edge_label_type.names])\n  else:\n  labels = [None] * len(edges)\n  edge_aos = np.fromiter(zip(edges['!w'], labels), dtype=edge_type,\n count=nnz)\n  else:\n  self.weighted = False\n  self.edge_type = edge_type = edge_label_type\n  np.add.at(degree, edges['!i'], 1.0)\n  np.add.at(degree, edges['!j'], 1.0)\n  edge_aos = np.fromiter(zip(*[edges[t] for t in edge_type.names]),\n dtype=edge_type, count=nnz)\n  degree[degree == 0] = 1.0\n  indices = np.empty((4, nnz * 2), dtype=np.uint32, order='C')\n  i, j, up, lf = indices\n  i[:nnz] = edges['!i']\n  j[:nnz] = edges['!j']\n  indices[[0, 1], nnz:] = indices[[1, 0], :nnz]\n  up[:] = i - i % 8\n  lf[:] = j - j % 8\n   perm = np.lexsort(indices, axis=0)\n  indices[:, :] = indices[:, perm]\n  self.edge_aos = umempty(nnz * 2, edge_type)\n  self.edge_aos[:] = edge_aos[perm % nnz]\n   diff = np.empty(nnz * 2)\n  diff[1:] = (up[:-1] != up[1:]) | (lf[:-1] != lf[1:])\n  diff[:1] = True\n  oct_offset = np.flatnonzero(diff)\n  self.n_octile = len(oct_offset)\n   nzmasks = np.bitwise_or.reduceat(\n  1 << (i - up + (j - lf) * 8).astype(np.uint64), oct_offset)\n  nzmasks_r = np.bitwise_or.reduceat(\n  1 << (j - lf + (i - up) * 8).astype(np.uint64), oct_offset)\n   self.octiles = octiles = umempty(self.n_octile, self.Octile.dtype)\n  octiles[:] = list(\n  zip(int(self.edge_aos.base) + oct_offset * edge_type.itemsize,\n  nzmasks,\n  nzmasks_r,\n  up[oct_offset],\n  lf[oct_offset])\n  )", "target": 1, "info": "Null", "idx": 0}
{"func": "def diag(self, X, eval_gradient=False, nodal=False, lmin=0, timing=False):\n  timer = Timer()\n  backend = self.backend\n  traits = self.traits(\n  diagonal=True,\n  nodal=nodal,\n  lmin=lmin,\n  eval_gradient=eval_gradient\n  )\n  pred_or_tuple = Graph.has_unified_types(X)\n  if pred_or_tuple is not True:\n  group, first, second = pred_or_tuple\n  raise TypeError(\n  f'The two graphs have mismatching {group} attributes or '\n  'attribute types.'\n  'If the attribute names do match, then try to unify data '\n  'types automatically with `Graph.unify_datatype`.\\n'\n  f'First graph: {first}\\n'\n  f'Second graph: {second}\\n'\n  )\n  timer.tic('generating jobs')\n  i = np.arange(len(X), dtype=np.uint32)\n  jobs = backend.array(\n  np.column_stack((i, i))\n  .ravel()\n  .view(np.dtype([('i', np.uint32), ('j', np.uint32)]))\n  )\n  timer.toc('generating jobs')\n  timer.tic('creating output buffer')\n  starts = backend.zeros(len(X) + 1, dtype=np.uint32)\n  if nodal is True:\n  sizes = np.array([len(g.nodes) for g in X], dtype=np.uint32)\n  np.cumsum(sizes, out=starts[1:])\n  output_length = int(starts[-1])\n  elif nodal is False:\n  starts[:] = np.arange(len(X) + 1)\n  output_length = len(X)\n  elif nodal == 'block':\n  sizes = np.array([len(g.nodes) for g in X], dtype=np.uint32)\n  np.cumsum(sizes**2, out=starts[1:])\n  output_length = int(starts[-1])\n  else:\n  raise(ValueError(\"Invalid 'nodal' option '%s'\" % nodal))\n  if traits.eval_gradient is True:\n  output_shape = (output_length, 1 + self.n_dims)\n  else:\n  output_shape = (output_length, 1)\n  output = backend.empty(int(np.prod(output_shape)), np.float32)\n  timer.toc('creating output buffer')\n  timer.tic('calling GPU kernel (overall)')\n  backend(\n  X,\n  self.node_kernel,\n  self.edge_kernel,\n  self.p,\n  self.q,\n  jobs,\n  starts,\n  output,\n  output_shape,\n  traits,\n  timer,\n  )\n  timer.toc('calling GPU kernel (overall)')\n  timer.tic('collecting result')\n  if nodal == 'block':\n  output = [output[s:s + n**2].reshape(n, n)\n    for s, n in zip(starts[:-1], sizes)]\n  timer.toc('collecting result')\n   if timing:\n  timer.report(unit='ms')\n  timer.reset()\n   if traits.eval_gradient is True:\n  return (\n  output[:, 0].astype(self.element_dtype),\n  output[:, 1:].astype(self.element_dtype)\n  )\n  else:\n  return output.astype(self.element_dtype)", "target": 0, "info": "Null", "idx": 0}
{"func": "def diag(self, X, eval_gradient=False, nodal=False, lmin=0, timing=False):\n  timer = Timer()\n  backend = self.backend\n  traits = self.traits(\n  diagonal=True,\n  nodal=nodal,\n  lmin=lmin,\n  eval_gradient=eval_gradient\n  )\n  pred_or_tuple = Graph.has_unified_types(X)\n  if pred_or_tuple is not True:\n  group, first, second = pred_or_tuple\n  raise TypeError(\n  f'The two graphs have mismatching {group} attributes or '\n  'attribute types.'\n  'If the attribute names do match, then try to unify data '\n  'types automatically with `Graph.unify_datatype`.\\n'\n  f'First graph: {first}\\n'\n  f'Second graph: {second}\\n'\n  )\n  timer.tic('generating jobs')\n  i = np.arange(len(X), dtype=np.uint32)\n  jobs = backend.array(\n  np.column_stack((i, i))\n  .ravel()\n  .view(np.dtype([('i', np.uint32), ('j', np.uint32)]))\n  )\n  timer.toc('generating jobs')\n  timer.tic('creating output buffer')\n  starts = backend.zeros(len(X) + 1, dtype=np.uint32)\n  if nodal is True:\n  sizes = np.array([len(g.nodes) for g in X], dtype=np.uint32)\n  np.cumsum(sizes, out=starts[1:])\n  output_length = int(starts[-1])\n  elif nodal is False:\n  starts[:] = np.arange(len(X) + 1)\n  output_length = len(X)\n  elif nodal == 'block':\n  sizes = np.array([len(g.nodes) for g in X], dtype=np.uint32)\n  np.cumsum(sizes**2, out=starts[1:])\n  output_length = int(starts[-1])\n  else:\n  raise(ValueError(\"Invalid 'nodal' option '%s'\" % nodal))\n  if traits.eval_gradient is True:\n  output_shape = (output_length, 1 + self.n_dims)\n  else:\n  output_shape = (output_shape, 1)\n  output = backend.empty(int(np.prod(output_shape)), np.float32)\n  timer.toc('creating output buffer')\n  timer.tic('calling GPU kernel (overall)')\n  backend(\n  X,\n  self.node_kernel,\n  self.edge_kernel,\n  self.p,\n  self.q,\n  jobs,\n  starts,\n  output,\n  output_shape,\n  traits,\n  timer,\n  )\n  timer.toc('calling GPU kernel (overall)')\n  timer.tic('collecting result')\n  if nodal == 'block':\n  output = [output[s:s + n**2].reshape(n, n)\n    for s, n in zip(starts[:-1], sizes)]\n  timer.toc('collecting result')\n   if timing:\n  timer.report(unit='ms')\n  timer.reset()\n   if traits.eval_gradient is True:\n  return (\n  output[:, 0].astype(self.element_dtype),\n  output[:, 1:].astype(self.element_dtype)\n  )\n  else:\n  return output.astype(self.element_dtype)", "target": 1, "info": "Null", "idx": 0}
{"func": " def __init__(self, X, rcut=0, acut=0):\n  if isinstance(X, np.ndarray):\n  U, S, _ = np.linalg.svd(X, full_matrices=False)\n  mask = (S >= S.max() * rcut) & (S >= acut)\n  self.U = U[:, mask]\n  self.S = S[mask]\n  elif isinstance(X, tuple) and len(X) == 2:\n  self.U, self.S = X\n  self._lhs = self.U * self.S", "target": 0, "info": "Null", "idx": 0}
{"func": "def __init__(self, X, rcut=0, acut=0):\n  if isinstance(X, np.ndarray):\n  U, S, _ = np.linalg.svd(X, full_matrices=False)\n  mask = (S >= S.max() * rcut) | (S >= acut)\n  self.U = U[:, mask]\n  self.S = S[mask]\n  elif isinstance(X, tuple) and len(X) == 2:\n  self.U, self.S = X\n  self._lhs = self.U * self.S", "target": 1, "info": "Null", "idx": 0}
{"func": "def copy_files(errnum, etime, toolname, runName):\n  rdir = os.path.join(Submit.CACHEDIR, toolname, runName)\n  try:\n  os.remove(os.path.join(rdir, '.attachid'))\n  except OSError:\n  pass\n   if os.path.isdir(runName):\n  if errnum > 0:\n  shutil.rmtree(rdir)\n  else:\n  os.system('/bin/cp -pr %s/* %s' % (runName, rdir))\n  shutil.rmtree(runName)\n  else:\n  if errnum == 0:\n  files = os.listdir('.')\n  for f in files:\n  if os.path.getmtime(f) > self.start_time:\n  shutil.copy2(f, rdir)\n  if errnum == 0:\n  with open(os.path.join(rdir, '.submit_time'), 'w') as f:\n  f.write(pretty_time_delta(etime))\n  return rdir", "target": 0, "info": "Null", "idx": 0}
{"func": "def copy_files(errnum, etime, toolname, runName):\n  rdir = os.path.join(Submit.CACHEDIR, toolname, runName)\n  try:\n  os.remove(os.path.join(rdir, '.attachid'))\n  except OSError:\n  pass\n   if os.path.isdir(runName):\n  if errnum > 0:\n  shutil.rmtree(rdir)\n  else:\n  os.system('/bin/cp -pr %s/* %s' % (runName, rdir))\n  shutil.rmtree(runName)\n  else:\n  if errnum > 0:\n  files = os.listdir('.')\n  for f in files:\n  if os.path.getmtime(f) > self.start_time:\n  shutil.copy2(f, rdir)\n  if errnum == 0:\n  with open(os.path.join(rdir, '.submit_time'), 'w') as f:\n  f.write(pretty_time_delta(etime))\n  return rdir", "target": 1, "info": "Null", "idx": 0}
{"func": "def process_response(self, request, response):\n   if (not getattr(request, 'clienttrack_prohibit', False)) or not request.clienttrack_first_visit:\n    if not request.clienttrack_first_visit:\n   request.clienttrack_first_visit = time.time()\n  max_age = 3*365*24*60*60\n  expires_time = time.time() + max_age\n  expires = cookie_date(expires_time)\n  response.set_cookie('_hda', \"%d,%s,%f\" % (request.clienttrack_first_visit, request.clienttrack_uid, time.time()),\n  max_age=max_age, expires=expires)\n  return response", "target": 0, "info": "Null", "idx": 0}
{"func": "  def process_response(self, request, response):\n  if (not getattr('clienttrack_prohibit', request, False)) or not request.clienttrack_first_visit:\n   if not request.clienttrack_first_visit:\n  request.clienttrack_first_visit = time.time()\n  max_age = 3*365*24*60*60\n  expires_time = time.time() + max_age\n  expires = cookie_date(expires_time)\n  response.set_cookie('_hda', \"%d,%s,%f\" % (request.clienttrack_first_visit, request.clienttrack_uid, time.time()),\n  max_age=max_age, expires=expires)\n  return response", "target": 1, "info": "Null", "idx": 0}
{"func": "def get_kunde_by_iln(iln):\n   rows = husoftm.connection.get_connection().query(['XKS00'], condition=\"KCE2IL='%s'\" % (int(iln), ))\n  if rows:\n  return get_kunde(rows[0]['kundennr'])\n  else:\n  rows = husoftm.connection.get_connection().query(['AVA00'], condition=\"VAILN='%s'\" % (int(iln), ))\n  if rows:\n  rows2 = husoftm.connection.get_connection().query(['XXA00'],\n  condition=\"XASANR='%s'\" % (int(rows[0]['satznr']), ))\n  if rows2:\n  kunde = Kunde().fill_from_softm(rows2[0])\n  kunde.kundennr = kunde.kundennr + ('/%03d' % int(rows[0]['versandadresssnr']))\n  return kunde\n  raise ValueError(\"Keine Daten f\u00fcr GLN/ILN %r gefunden\" % iln)", "target": 0, "info": "Null", "idx": 0}
{"func": "def get_kunde_by_iln(iln):\n   rows = husoftm.connection.get_connection().query(['XKS00'], condition=\"KCE2IL='%s'\" % (int(iln), ))\n  if rows:\n  return get_kunde(rows[0]['kundennr'])\n  else:\n  rows = husoftm.connection.get_connection().query(['AVA00'], condition=\"VAILN='%s'\" % (int(iln), ))\n  if rows:\n  rows2 = husoftm.connection.get_connection().query(['XXA00'],\n  condition=\"XASANR='%s'\" % (int(rows[0]['satznr']), ))\n  if rows:\n  kunde = Kunde().fill_from_softm(rows2[0])\n  kunde.kundennr = kunde.kundennr + ('/%03d' % int(rows[0]['versandadresssnr']))\n  return kunde\n  raise ValueError(\"Keine Daten f\u00fcr GLN/ILN %r gefunden\" % iln)", "target": 1, "info": "Null", "idx": 0}
{"func": "def creates_default_site(sender, instance, created, *args, **kwargs):\n  if not created:\n  try:\n  site = Site.objects.get(domain__icontains=DEFAULT_SITE_DOMAIN,\n  tenant_site__tenant=instance)\n  if site.domain != ('%s.%s' % (instance.slug, DEFAULT_SITE_DOMAIN)):\n  site.delete()\n  else:\n  return\n  except Site.DoesNotExist:\n  pass\n  site = Site.objects.create(\n  name=instance.slug,\n  domain='%s.%s' % (instance.slug, DEFAULT_SITE_DOMAIN))\n  instance.tenant_sites.create(site=site)", "target": 0, "info": "Null", "idx": 0}
{"func": "def creates_default_site(sender, instance, created, *args, **kwargs):\n  if created:\n  try:\n  site = Site.objects.get(domain__icontains=DEFAULT_SITE_DOMAIN,\n  tenant_site__tenant=instance)\n  if site.domain != ('%s.%s' % (instance.slug, DEFAULT_SITE_DOMAIN)):\n  site.delete()\n  else:\n  return\n  except Site.DoesNotExist:\n  pass\n  site = Site.objects.create(\n  name=instance.slug,\n  domain='%s.%s' % (instance.slug, DEFAULT_SITE_DOMAIN))\n  instance.tenant_sites.create(site=site)", "target": 1, "info": "Null", "idx": 0}
{"func": "def __call__(self, context, request):\n  requests = []\n  folders = MissingList()\n  config = request.registry.config\n  for schema_view in request.registry.getAllUtilitiesRegisteredFor(ISchemaView):\n  schema_route = request.registry.introspector.get('routes', schema_view.route_name)\n  if schema_route is not None:\n  headers = set()\n  permissions = lookup_for_route_permissions(request.registry, schema_route)\n  method_permissions = maybe_list(permissions.get('GET'))\n  for method_permission in method_permissions:\n  if method_permission not in (Everyone, NotAuthenticated):\n  headers.add('Authorization: Token {{token}}')\n  break\n  request_id = self.new_unique_id()\n  requests.append({\n  'id': request_id,\n  'headers': '\\n'.join(headers),\n  'url': request.route_url(schema_view.route_name),\n  'preRequestScript': '',\n  'pathVariables': {},\n  'method': u'GET',\n  'data': [],\n  'dataMode': 'params',\n  'version': 2,\n  'tests': '',\n  'currentHelper': 'normal',\n  'helperAttributes': {},\n  'time': self.collection_time,\n  'name': u'Schema: %s' % schema_view.title,\n  'description': '',\n  'collectionId': self.collection_id,\n  'responses': [],\n  'owner': 0,\n  'synced': False})\n  folders[schema_view.title].append(request_id)\n  for route_name, request_methods in schema_view.routes_names.items():\n  intr_route = request.registry.introspector.get('routes', route_name)\n  if intr_route is None:\n  continue\n  route = intr_route['object']\n  permissions = lookup_for_route_permissions(request.registry, intr_route)\n  params = dict((k, '{{%s}}' % camelcase(k)) for k in lookup_for_route_params(route))\n  url = '%s%s' % (request.application_url, unquote(route.generate(params)))\n  schemas_by_methods = MissingList()\n  for schema in config.lookup_input_schema(route_name, request_methods):\n  for request_method in maybe_list(schema.request_method) or DEFAULT_METHODS:\n  schemas_by_methods[request_method].append(schema)\n  for schema in config.lookup_output_schema(route_name, request_methods):\n  for request_method in maybe_list(schema.request_method) or DEFAULT_METHODS:\n  schemas_by_methods[request_method].append(schema)\n  for request_method, schemas in schemas_by_methods.items():\n  title = None\n  schema_data = []\n  tests = []\n  for schema in schemas:\n  if schema.schema_type == 'request':\n  if schema.schema:\n  schema_data.extend(construct_postman_data(request, schema.schema))\n  if schema.fields_schema:\n  schema_data.extend(\n  construct_postman_data(request, schema.fields_schema))\n  if schema.schema and not title:\n  title = schema.schema.title\n  variables = getattr(schema.schema, 'postman_environment_variables', None)\n  if variables:\n  for environment_key, response_key in variables.items():\n  environment_key = camelcase(environment_key)\n  response_key = camelcase(response_key)\n  tests.append(\n  'if (answer.%s){ postman.setEnvironmentVariable(\"%s\", answer.%s); }'\n  % (response_key, environment_key, response_key))\n   if tests:\n  tests.insert(0, 'var answer = JSON.parse(responseBody);')\n  method_url = url\n  request_schema_data = []\n  request_method = request_method.upper()\n  if request_method in 'GET':\n  queries = []\n  for url_param in schema_data:\n  if url_param['value']:\n  queries.append(\n  '%s=%s'\n  % (quote(url_param['key']), quote(url_param['value'])))\n  else:\n  queries.append(quote(url_param['key']))\n  if queries:\n  method_url = '%s?%s' % (method_url, '&'.join(queries))\n  else:\n  request_schema_data = schema_data\n  headers = set()\n  method_permissions = maybe_list(permissions.get(request_method))\n  for method_permission in method_permissions:\n  if method_permission not in (Everyone, NotAuthenticated):\n  headers.add('Authorization: Token {{token}}')\n  break\n  if not title:\n  title = route_name.replace('_', ' ').title()\n  request_id = self.new_unique_id()\n  requests.append({\n  'id': request_id,\n  'headers': '\\n'.join(headers),\n  'url': method_url,\n  'preRequestScript': '',\n  'pathVariables': {},\n  'method': request_method,\n  'data': request_schema_data,\n  'dataMode': 'params',\n  'version': 2,\n  'tests': '\\n'.join(tests),\n  'currentHelper': 'normal',\n  'helperAttributes': {},\n  'time': self.collection_time,\n  'name': title,\n  'description': '',\n  'collectionId': self.collection_id,\n  'responses': [],\n  'owner': 0,\n  'synced': False})\n  folders[schema_view.title].append(request_id)\n  response_folders = []\n  for key, requests_ids in folders.items():\n  response_folders.append({\n  'id': self.new_unique_id(),\n  'name': key,\n  'description': '',\n  'order': requests_ids,\n  'collection_name': self.title,\n  'collection_id': self.collection_id,\n  'collection_owner': '',\n  'write': True})\n  return {\n  'id': self.collection_id,\n  'name': self.title,\n  'description': self.description or '',\n  'timestamp': self.collection_time,\n  'synced': False,\n  'owner': '',\n  'subscribed': False,\n  'remoteLink': '',\n  'public': False,\n  'write': True,\n  'order': [],\n  'folders': response_folders,\n  'requests': requests}", "target": 0, "info": "Null", "idx": 0}
{"func": "def __call__(self, context, request):\n  requests = []\n  folders = MissingList()\n  config = request.registry.config\n  for schema_view in request.registry.getAllUtilitiesRegisteredFor(ISchemaView):\n  schema_route = request.registry.introspector.get('routes', schema_view.route_name)\n  if schema_route is not None:\n  headers = set()\n  permissions = lookup_for_route_permissions(request.registry, schema_route)\n  method_permissions = maybe_list(permissions.get('GET'))\n  for method_permission in method_permissions:\n  if method_permission not in (Everyone, NotAuthenticated):\n  headers.add('Authorization: Token {{token}}')\n  break\n  request_id = self.new_unique_id()\n  requests.append({\n  'id': request_id,\n  'headers': '\\n'.join(headers),\n  'url': request.route_url(schema_view.route_name),\n  'preRequestScript': '',\n  'pathVariables': {},\n  'method': u'GET',\n  'data': [],\n  'dataMode': 'params',\n  'version': 2,\n  'tests': '',\n  'currentHelper': 'normal',\n  'helperAttributes': {},\n  'time': self.collection_time,\n  'name': u'Schema: %s' % schema_view.title,\n  'description': '',\n  'collectionId': self.collection_id,\n  'responses': [],\n  'owner': 0,\n  'synced': False})\n  folders[schema_view.title].append(request_id)\n  for route_name, request_methods in schema_view.routes_names.items():\n  intr_route = request.registry.introspector.get('routes', route_name)\n  if intr_route is None:\n  continue\n  route = intr_route['object']\n  permissions = lookup_for_route_permissions(request.registry, intr_route)\n  params = dict((k, '{{%s}}' % camelcase(k)) for k in lookup_for_route_params(route))\n  url = '%s%s' % (request.application_url, unquote(route.generate(params)))\n  schemas_by_methods = MissingList()\n  for schema in config.lookup_input_schema(route_name, request_methods):\n  for request_method in maybe_list(schema.request_method) or DEFAULT_METHODS:\n  schemas_by_methods[request_method].append(schema)\n  for schema in config.lookup_output_schema(route_name, request_methods):\n  for request_method in maybe_list(schema.request_method) or DEFAULT_METHODS:\n  schemas_by_methods[request_method].append(schema)\n  for request_method, schemas in schemas_by_methods.items():\n  title = None\n  schema_data = []\n  tests = []\n  for schema in schemas:\n  if schema.schema_type == 'request':\n  if schema.schema:\n  schema_data.extend(construct_postman_data(request, schema.schema))\n  if schema.fields_schema:\n  schema_data.extend(\n  construct_postman_data(request, schema.fields_schema))\n  if schema.schema and not title:\n  title = schema.schema.title\n  variables = getattr(schema.schema, 'postman_environment_variables', None)\n  if variables:\n  for environment_key, response_key in variables.items():\n  environment_key = camelcase(environment_key)\n  response_key = camelcase(response_key)\n  tests.append(\n  'if (answer.%s){ postman.setEnvironmentVariable(\"%s\", answer.%s); }'\n  % (environment_key, environment_key, response_key))\n   if tests:\n  tests.insert(0, 'var answer = JSON.parse(responseBody);')\n  method_url = url\n  request_schema_data = []\n  request_method = request_method.upper()\n  if request_method in 'GET':\n  queries = []\n  for url_param in schema_data:\n  if url_param['value']:\n  queries.append(\n  '%s=%s'\n  % (quote(url_param['key']), quote(url_param['value'])))\n  else:\n  queries.append(quote(url_param['key']))\n  if queries:\n  method_url = '%s?%s' % (method_url, '&'.join(queries))\n  else:\n  request_schema_data = schema_data\n  headers = set()\n  method_permissions = maybe_list(permissions.get(request_method))\n  for method_permission in method_permissions:\n  if method_permission not in (Everyone, NotAuthenticated):\n  headers.add('Authorization: Token {{token}}')\n  break\n  if not title:\n  title = route_name.replace('_', ' ').title()\n  request_id = self.new_unique_id()\n  requests.append({\n  'id': request_id,\n  'headers': '\\n'.join(headers),\n  'url': method_url,\n  'preRequestScript': '',\n  'pathVariables': {},\n  'method': request_method,\n  'data': request_schema_data,\n  'dataMode': 'params',\n  'version': 2,\n  'tests': '\\n'.join(tests),\n  'currentHelper': 'normal',\n  'helperAttributes': {},\n  'time': self.collection_time,\n  'name': title,\n  'description': '',\n  'collectionId': self.collection_id,\n  'responses': [],\n  'owner': 0,\n  'synced': False})\n  folders[schema_view.title].append(request_id)\n  response_folders = []\n  for key, requests_ids in folders.items():\n  response_folders.append({\n  'id': self.new_unique_id(),\n  'name': key,\n  'description': '',\n  'order': requests_ids,\n  'collection_name': self.title,\n  'collection_id': self.collection_id,\n  'collection_owner': '',\n  'write': True})\n  return {\n  'id': self.collection_id,\n  'name': self.title,\n  'description': self.description or '',\n  'timestamp': self.collection_time,\n  'synced': False,\n  'owner': '',\n  'subscribed': False,\n  'remoteLink': '',\n  'public': False,\n  'write': True,\n  'order': [],\n  'folders': response_folders,\n  'requests': requests}", "target": 1, "info": "Null", "idx": 0}
{"func": "def not_inactives_filter(column):\n  return and_(\n  or_(column.start_date <= func.now(), column.start_date.is_(None)),\n  or_(column.end_date > func.now(), column.end_date.is_(None)))", "target": 0, "info": "Null", "idx": 0}
{"func": "def not_inactives_filter(column):\n  return and_(\n  or_(column.start_date <= func.now(), column.start_date.is_(None)),\n  or_(column.end_date >= func.now(), column.end_date.is_(None)))", "target": 1, "info": "Null", "idx": 0}
{"func": "def activate(self, leaf):\n  url = to_correios_url(leaf)\n  with request.urlopen(url) as curreio:\n  content = curreio.read()\n  info = get_tracking_info(content.decode('iso-8859-1'))\n  if info:\n  txt = '-'.join(reversed(info[0]))\n  return TextLeaf(leaf.object, txt)", "target": 0, "info": "Null", "idx": 0}
{"func": " def activate(self, leaf):\n  url = to_correios_url(leaf)\n  with request.urlopen(url) as curreio:\n  content = curreio.read()\n  info = get_tracking_info(content.decode('iso-8859-1'))\n  if info:\n  txt = '-'.join(reversed(info[0]))\n  return TextLeaf(txt, leaf.object)", "target": 1, "info": "Null", "idx": 0}
